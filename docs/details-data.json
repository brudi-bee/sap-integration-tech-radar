{
  "generatedAt": "2026-02-22",
  "items": [
    {
      "label": "SAP Cloud Integration (CI)",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/integration-suite"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/cloud-integration",
        "https://api.sap.com/package/SAPIntegrationSuiteContent",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/operate-and-monitor-integrations",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/iflow-design-guidelines"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Cloud Integration (CPI) ist die zentrale Middleware-Laufzeit der SAP Integration Suite auf BTP. Sie unterstützt synchrone (HTTP/REST, OData, SOAP) und asynchrone (JMS, AMQP) Protokolle, einen grafischen IFlow-Designer, Groovy/XSLT-Scripting, vorkonfigurierte Content-Pakete aus dem SAP Business Accelerator Hub sowie integriertes Message-Monitoring. CPI ersetzt SAP PI/PO als strategische SAP-Middleware und ist Dreh- und Angelpunkt für alle neuen Integrationsvorhaben.",
          "whyRing": "Produktionsreif, breite Kundenbasis, aktive SAP-Weiterentwicklung. Für alle Neuvorhaben im SAP-Umfeld ist CPI die Standardwahl ohne Alternative auf BTP. ADOPT ohne Einschränkung.",
          "risks": [
            "Vendor-Lock-in durch proprietäre Groovy-Skripte und SAP-spezifische Adapter. Fehlende IFlow-Modularisierung erzeugt schwer wartbare Monolith-Flows. Ohne strukturierten Transport (CTMS + Content Agent) entstehen Drift und unkontrollierte Änderungen in Produktion. Monitoring ohne korrelierte Trace-IDs bleibt auf Log-Ebene."
          ],
          "do": [
            "Flows modular halten: 1 IFlow = 1 fachliche Verantwortung, max. ~200 Steps pro Flow. CTMS + Content Agent für DEV→QA→PROD-Transport verbindlich nutzen. Fehlerbehandlung mit expliziten Fehlerklassen, Alerting-Regeln und strukturierten Fehlermeldungen implementieren. Correlation-ID (z. B. SAP-Message-ID) End-to-End durchschleifen. SAP-Standardpakete vom Business Accelerator Hub als Ausgangsbasis nutzen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Nicht als primäre Laufzeit wenn Latenz <50 ms zwingend (CPI ist nicht für Echtzeit ausgelegt). Nicht wenn Eigenhosting aus regulatorischen Gründen zwingend und BTP-Datenregionen unzureichend sind → Edge Integration Cell evaluieren."
          ]
        },
        "en": {
          "intro": "SAP Cloud Integration (CPI) is the core middleware runtime of the SAP Integration Suite on BTP. It supports synchronous (HTTP/REST, OData, SOAP) and asynchronous (JMS, AMQP) protocols, a graphical IFlow designer, Groovy/XSLT scripting, pre-built content packages from the SAP Business Accelerator Hub, and integrated message monitoring. CPI replaces SAP PI/PO as the strategic SAP middleware and is the hub for all new integration initiatives.",
          "whyRing": "Production-ready, broad customer base, active SAP development. For all new SAP integration projects, CPI is the default choice with no BTP-native alternative. ADOPT without restriction.",
          "risks": [
            "Vendor lock-in through proprietary Groovy scripts and SAP-specific adapters. Missing IFlow modularization creates hard-to-maintain monolith flows. Without structured transport (CTMS + Content Agent), landscape drift and uncontrolled production changes occur. Monitoring without correlated trace IDs stays at log level only."
          ],
          "do": [
            "Keep flows modular: 1 IFlow = 1 business responsibility, max ~200 steps per flow. Use CTMS + Content Agent for mandatory DEV→QA→PROD transport. Implement error handling with explicit error classes, alerting rules, and structured messages. Thread Correlation-ID (e.g. SAP-Message-ID) end-to-end. Use SAP standard packages from Business Accelerator Hub as baseline."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "Not as primary runtime when latency <50 ms is mandatory (CPI is not designed for real-time). Not when on-premise hosting is mandated by regulation and BTP data regions are insufficient → evaluate Edge Integration Cell."
          ]
        }
      }
    },
    {
      "label": "SAP API Management",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/integration-suite"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management-policies",
        "https://api.sap.com",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/developer-portal"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP API Management (APIM) auf BTP ist ein vollständiges API-Gateway mit Policy-Engine, Developer Portal, Analytics und API-Produktkatalog. Es erzwingt AuthN/AuthZ (OAuth2, JWT, API-Key), Rate Limiting, Quotas und Threat Protection für alle publizierten APIs. APIM ist der zentrale Enforcement-Punkt zwischen API-Producern (CPI, S/4HANA, Kyma) und internen/externen Konsumenten.",
          "whyRing": "Strategische SAP-Komponente für API-Governance. Native Integration mit CPI, SAP IAS und dem SAP Business Accelerator Hub. Reifes Produkt mit breiter Produktivnutzung. ADOPT.",
          "risks": [
            "Ohne Policy-Templates entstehen inkonsistente Security-Profile zwischen API-Produkten. Developer-Portal-Akzeptanz gering wenn OpenAPI-Spezifikationen nicht gepflegt. Non-SAP-Backend-Szenarien schwächer abgedeckt als bei Kong/Apigee. Kein nativer Support für AsyncAPI / Event-APIs."
          ],
          "do": [
            "API-Policies als versionierte Templates standardisieren (AuthN, Rate Limit, Logging als Pflicht-Layer). OpenAPI 3.x als Single Source of Truth im Developer Portal. URI-Versionierungsstrategie (/v1/, /v2/) vorab verbindlich festlegen. API Analytics für SLA-Monitoring und Capacity Planning nutzen. Alle externen APIs hinter APIM führen – kein direktes Backend-Exposure."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Wenn primärer Stack Non-SAP ist und ein reifes Gateway (Kong, Apigee) bereits produktiv läuft – Koexistenzstrategie oder Ablösung prüfen statt parallele APIM-Instanz. Nicht für rein intern Microservice-zu-Microservice-Kommunikation (→ Service Mesh)."
          ]
        },
        "en": {
          "intro": "SAP API Management (APIM) on BTP is a full API gateway with policy engine, developer portal, analytics, and API product catalog. It enforces AuthN/AuthZ (OAuth2, JWT, API-Key), rate limiting, quotas, and threat protection for all published APIs. APIM is the central enforcement point between API producers (CPI, S/4HANA, Kyma) and internal/external consumers.",
          "whyRing": "Strategic SAP component for API governance. Native integration with CPI, SAP IAS, and the SAP Business Accelerator Hub. Mature product with broad production adoption. ADOPT.",
          "risks": [
            "Without policy templates, inconsistent security profiles emerge across API products. Developer portal adoption is low if OpenAPI specs are unmaintained. Non-SAP backend scenarios weaker than Kong/Apigee. No native AsyncAPI/event API support."
          ],
          "do": [
            "Standardize API policies as versioned templates (AuthN, rate limit, logging as mandatory layers). OpenAPI 3.x as single source of truth in the developer portal. Define URI versioning strategy (/v1/, /v2/) upfront as a binding standard. Use API analytics for SLA monitoring and capacity planning. Route all external APIs through APIM – no direct backend exposure."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "When the primary stack is non-SAP and a mature gateway (Kong, Apigee) is already in production – evaluate coexistence or replacement rather than a parallel APIM instance. Not for purely internal microservice-to-microservice communication (→ Service Mesh)."
          ]
        }
      }
    },
    {
      "label": "SAP Event Mesh",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/event-mesh"
      ],
      "references": [
        "https://help.sap.com/docs/event-mesh",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/event-driven-architecture",
        "https://help.sap.com/docs/s4hana-cloud/sap-s-4hana-cloud/business-events",
        "https://cloudevents.io/"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "S/4HANA Business Events sind in Cloud-Releases stabil und produktionsreif. EDA ist strategische Architekturrichtung. Empfehle ADOPT für Standard-Event-Mesh-Szenarien; AEM bleibt TRIAL für erweiterte Topologien. Confidence: High.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Event Mesh ist ein Managed-Message-Broker auf BTP, der AMQP 1.0, MQTT und REST-basiertes Publish/Subscribe unterstützt. Er empfängt S/4HANA Business Events (z. B. BusinessPartner.Changed, SalesOrder.Created) und leitet sie an registrierte Consumer weiter. Event Mesh ist die Entry-Level-Eventing-Komponente der Integration Suite; für komplexere Mesh-Topologien ist SAP Advanced Event Mesh (AEM) die Erweiterung.",
          "whyRing": "Technisch stabil, in aktiver Produktivnutzung für B2B- und interne EDA-Szenarien. TRIAL reflektiert, dass EDA-Governance-Reife (Schema-Evolution, Idempotenz, DLQ-Strategie) organisatorisch aufgebaut werden muss – nicht einen technischen Vorbehalt.",
          "risks": [
            "Fehlende Schema-Registry führt zu inkompatiblen Event-Formaten zwischen Producern und Consumern. Basis-Event-Mesh hat keine native Dead Letter Queue – DLQ muss über CPI-Retry kompensiert werden. At-least-once-Semantik erfordert idempotente Consumer. Queue-Kapazitäts- und TTL-Limits bei Spitzenlast unterschätzt."
          ],
          "do": [
            "Für jeden Event-Topic ein versioniertes Schema (AsyncAPI oder JSON Schema) definieren und zentral registrieren. DLQ-Strategie und Reprocessing-Runbook vor Go-live festlegen. Retry-Backoff im CPI-Adapter konfigurieren (exponentiell, max. 3–5 Versuche). S/4HANA Business Events über BTP Event-Enablement konfigurieren, nicht über RFC-Polling."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Wenn Latenz <100 ms oder Streaming-Volumen sehr hoch. Wenn umfangreiche Mesh-Topologien, Replay oder komplexes Topic-Routing benötigt → AEM. Wenn Kafka bereits strategische Streaming-Plattform."
          ]
        },
        "en": {
          "intro": "SAP Event Mesh is a managed message broker on BTP supporting AMQP 1.0, MQTT, and REST-based publish/subscribe. It receives S/4HANA Business Events (e.g., BusinessPartner.Changed, SalesOrder.Created) and forwards them to registered consumers. Event Mesh is the entry-level eventing component of Integration Suite; SAP Advanced Event Mesh (AEM) extends it for complex mesh topologies.",
          "whyRing": "Technically stable, in active production use for B2B and internal EDA scenarios. TRIAL reflects that EDA governance maturity (schema evolution, idempotency, DLQ strategy) must be built organizationally – not a technical reservation.",
          "risks": [
            "Lack of schema registry leads to incompatible event formats between producers and consumers. Base Event Mesh has no native dead letter queue – DLQ must be compensated via CPI retry. At-least-once semantics require idempotent consumers. Queue capacity and TTL limits underestimated under peak load."
          ],
          "do": [
            "Define a versioned schema (AsyncAPI or JSON Schema) per event topic and register it centrally. Define DLQ strategy and reprocessing runbook before go-live. Configure retry backoff in the CPI adapter (exponential, max 3–5 attempts). Configure S/4HANA Business Events via BTP Event-Enablement, not RFC polling."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "When latency <100 ms or streaming volume is very high. When extensive mesh topologies, replay, or complex topic routing are needed → AEM. When Kafka is already the strategic streaming platform."
          ]
        }
      }
    },
    {
      "label": "SAP Advanced Event Mesh (AEM)",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "sources": [
        "https://help.pubsub.em.services.cloud.sap/"
      ],
      "references": [
        "https://help.pubsub.em.services.cloud.sap/",
        "https://help.sap.com/docs/sap-advanced-event-mesh",
        "https://solace.com/products/event-broker/cloud/",
        "https://help.sap.com/docs/sap-advanced-event-mesh/sap-advanced-event-mesh/event-portal"
      ],
      "ringChangeSuggestion": "ADOPT",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Advanced Event Mesh (AEM) basiert auf dem Solace PubSub+-Broker-Stack und bietet Multi-Protocol-Support (AMQP 1.0, MQTT 3.1/5.0, REST, WebSocket, SMF), Dynamic Message Routing (DMR) über mehrere Regionen und Clouds, Topic-basiertes Wildcard-Routing, Event-Replay sowie ein Event-Portal für Schema- und Topic-Taxonomie-Management. AEM ist die leistungsfähige Erweiterung von SAP Event Mesh für komplexe Enterprise-EDA-Szenarien.",
          "whyRing": "Technisch führend für Enterprise-EDA mit Multi-Cloud/Multi-Region-Anforderungen. TRIAL korrekt: Betriebskomplexität durch DMR-Topologien und Event-Portal-Governance ist erheblich; Lernkurve hoch; Kundenbasis noch begrenzt. Pilot-first zwingend.",
          "risks": [
            "Betriebskomplexität durch Multi-Broker-DMR-Topologien erfordert spezialisiertes Know-how. Abhängigkeit vom Solace-Proprietär-Stack (SMF-Protokoll) schränkt Portabilität ein. Event-Portal-Governance ohne dedizierten Owner degeneriert schnell. Kostenmodell ist Connection-basiert – falsches Sizing teuer."
          ],
          "do": [
            "Event-Portal-Taxonomie (Topic-Struktur, Schema-Versionen) vor erstem Event definieren. DMR nur aktivieren wenn Multi-Region/Multi-Cloud nachweisbar notwendig. Replay für Consumer-Onboarding und Fehlerdiagnose von Beginn einplanen. Pilot mit dediziertem Ops-Team und klarem Abnahmekriterium starten."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Wenn einfaches Point-to-Point-Eventing ausreicht → Event Mesh Basis. Wenn kein dediziertes EDA-Betriebsteam aufgebaut werden kann. Wenn Connection-basiertes Pricing nicht ins Budget passt."
          ]
        },
        "en": {
          "intro": "SAP Advanced Event Mesh (AEM) is built on the Solace PubSub+ broker stack and provides multi-protocol support (AMQP 1.0, MQTT 3.1/5.0, REST, WebSocket, SMF), Dynamic Message Routing (DMR) across regions and clouds, topic-based wildcard routing, event replay, and an Event Portal for schema and topic taxonomy management. AEM is the powerful extension of SAP Event Mesh for complex enterprise EDA scenarios.",
          "whyRing": "Technically leading for enterprise EDA with multi-cloud/multi-region requirements. TRIAL is correct: operational complexity through DMR topologies and Event Portal governance is significant; steep learning curve; customer base still limited. Pilot-first is mandatory.",
          "risks": [
            "Operational complexity of multi-broker DMR topologies requires specialized expertise. Dependency on Solace proprietary stack (SMF protocol) limits portability. Event Portal governance without a dedicated owner degrades quickly. Connection-based cost model – incorrect sizing is expensive."
          ],
          "do": [
            "Define Event Portal taxonomy (topic structure, schema versions) before the first event. Enable DMR only when multi-region/multi-cloud is demonstrably necessary. Plan replay for consumer onboarding and failure diagnosis from day one. Start pilot with a dedicated ops team and clear acceptance criteria."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "When simple point-to-point eventing suffices → Event Mesh base. When no dedicated EDA operations team can be established. When connection-based pricing does not fit the budget."
          ]
        }
      }
    },
    {
      "label": "SAP Cloud Connector",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/connectivity"
      ],
      "references": [
        "https://help.sap.com/docs/connectivity/sap-btp-connectivity-cf/cloud-connector",
        "https://help.sap.com/docs/connectivity/sap-btp-connectivity-cf/high-availability-setup",
        "https://help.sap.com/docs/connectivity/sap-btp-connectivity-cf/cloud-connector-monitoring"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Der SAP Cloud Connector (SCC) ist ein On-Premise-Reverse-Proxy-Agent, der über ausgehende TLS-Tunnels sichere Konnektivität zwischen BTP-Cloud-Services und On-Premise-Systemen (S/4HANA On-Premise, ECC, Non-SAP) herstellt. Er benötigt keine eingehenden Firewall-Ports und unterstützt RFC, HTTP, LDAP und SQL über den BTP Connectivity Service. SCC ist der Standard-Konnektivitätsweg in hybriden SAP-Landschaften.",
          "whyRing": "Unverzichtbarer Bestandteil jeder hybriden BTP-Landschaft. Kein alternatives SAP-natives Produkt deckt On-Premise-Konnektivität für BTP ohne Netzwerk-Exposierung ab. ADOPT ohne Einschränkung.",
          "risks": [
            "Single-Point-of-Failure wenn SCC nicht als Master/Shadow-HA-Paar betrieben wird. Ablaufende Zertifikate führen zu unerwartetem Konnektivitätsausfall. Übermäßige Systemzuordnungen ohne Inventur erhöhen Angriffsfläche. SCC-Versionsinkompatibilität mit BTP Connectivity Service nach ungepflegtem Update-Freeze."
          ],
          "do": [
            "SCC immer als Master/Shadow-Paar für HA betreiben. Systemzuordnungen inventarisieren und ungenutzte Verbindungen quartalsweise entfernen. Zertifikatsablauf-Monitoring in SAP Cloud ALM konfigurieren (mind. 30 Tage Vorlauf). SCC-Audit-Log aktivieren und in SIEM integrieren. SCC-Versionsupdates nach SAP-Patchplan regelmäßig einspielen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Bei Cloud-only-Landschaft ohne On-Premise-Systeme. Wenn SAP Private Link Service (hyperscaler-native Netzwerkkonnektivität) verfügbar und wirtschaftlich – dann direktere Option bevorzugen."
          ]
        },
        "en": {
          "intro": "The SAP Cloud Connector (SCC) is an on-premise reverse proxy agent that creates secure outbound TLS tunnels between BTP cloud services and on-premise systems (S/4HANA on-premise, ECC, non-SAP). It requires no inbound firewall ports and supports RFC, HTTP, LDAP, and SQL via the BTP Connectivity Service. SCC is the standard connectivity path in hybrid SAP landscapes.",
          "whyRing": "Essential component of every hybrid BTP landscape. No alternative SAP-native product covers on-premise connectivity for BTP without network exposure. ADOPT without restriction.",
          "risks": [
            "Single point of failure if SCC is not operated as Master/Shadow HA pair. Expiring certificates cause unexpected connectivity outages. Excessive system mappings without audit increase attack surface. SCC version incompatibility with BTP Connectivity Service after neglected update freeze."
          ],
          "do": [
            "Always operate SCC as Master/Shadow pair for HA. Inventory system mappings and remove unused connections quarterly. Configure certificate expiry monitoring in SAP Cloud ALM (minimum 30 days advance warning). Enable SCC audit log and integrate into SIEM. Apply SCC version updates regularly per SAP patch plan."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "In cloud-only landscapes without on-premise systems. When SAP Private Link Service (hyperscaler-native network connectivity) is available and economical – prefer the more direct option."
          ]
        }
      }
    },
    {
      "label": "SAP Integration Advisor",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/integration-suite"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/integration-advisor",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/message-implementation-guidelines",
        "https://api.sap.com/package/SAPIntegrationAdvisor"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "Für Organisationen mit aktivem B2B-EDI-Betrieb ist IA produktionsreif und wertbringend. Empfehle ADOPT mit der Bedingung, dass MIG-Governance und Owner vorab definiert sind. Confidence: High.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Integration Advisor (IA) ist ein ML-gestütztes B2B/EDI-Mapping-Werkzeug innerhalb der SAP Integration Suite. Es nutzt ML-basierte Mapping-Vorschläge und eine Community-Library aus Message Implementation Guidelines (MIGs) für Standards wie EDIFACT, X12, SAP IDoc und UN/CEFACT. IA generiert deploybare XSLT-Mappings und reduziert den manuellen Mapping-Aufwand in EDI-Projekten nachweislich.",
          "whyRing": "Für Organisationen mit B2B/EDI-Volumen (>10 Handelspartner, standardisierte Formate) ist IA ein erheblicher Effizienzgewinn. Der TRIAL-Status reflektiert, dass initialer Invest in MIG-Design und Community-Library-Governance notwendig ist – nicht mangelnde Reife.",
          "risks": [
            "MIG-Pflege ohne dedizierten Owner führt zu veralteten Mapping-Templates. Vorschläge müssen fachlich validiert werden – kein automatisches Ergebnis. Generiertes XSLT kann für Non-Standard-Felder manuelles Nacharbeiten erfordern. Vendor-Abhängigkeit: MIGs sind BTP-spezifisch und nicht portabel."
          ],
          "do": [
            "MIGs pro Handelspartner-Typ standardisieren und semantisch versionieren. Community-Library-Beiträge aktiv nutzen und eigene MIGs zurückbeitragen. Generierte XSLT-Mappings in automatisierte Integrationstests einbinden. EDI-Fachspezialist muss IA-Output vor Produktivgang abnehmen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn ausschließlich REST/JSON-Integrationen ohne EDI-Standardformate bestehen. Wenn Handelspartneranzahl <5 und manuelle Mappings ausreichend."
          ]
        },
        "en": {
          "intro": "SAP Integration Advisor (IA) is an AI-assisted B2B/EDI mapping tool within the SAP Integration Suite. It uses ML-based mapping suggestions and a community library of Message Implementation Guidelines (MIGs) for standards such as EDIFACT, X12, SAP IDoc, and UN/CEFACT. IA generates deployable XSLT mappings and measurably reduces manual mapping effort in EDI projects.",
          "whyRing": "For organizations with B2B/EDI volume (>10 trading partners, standardized formats), IA offers significant efficiency gains. TRIAL status reflects that initial investment in MIG design and community library governance is required – not a lack of maturity.",
          "risks": [
            "MIG maintenance without a dedicated owner leads to outdated mapping templates. AI suggestions must be validated by domain experts – no automatic result. Generated XSLT may require manual rework for non-standard fields. Vendor dependency: MIGs are BTP-specific and not portable."
          ],
          "do": [
            "Standardize and semantically version MIGs per trading partner type. Actively use community library contributions and contribute own MIGs back. Embed generated XSLT mappings in automated integration tests. EDI domain specialist must sign off IA output before go-live."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When exclusively running REST/JSON integrations without EDI standard formats. When trading partner count is <5 and manual mappings are sufficient."
          ]
        }
      }
    },
    {
      "label": "SAP Trading Partner Management",
      "ring": "TRIAL",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/integration-suite"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/trading-partner-management",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/b2b-integration"
      ],
      "confidence": "medium",
      "i18n": {
        "de": {
          "intro": "SAP Trading Partner Management (TPM) ist eine BTP-Komponente zur zentralen Verwaltung von B2B-Handelspartnerkonfigurationen, Kommunikationskanälen (AS2, SFTP, REST) und Agreement-Management. TPM integriert mit SAP Integration Advisor (MIGs) und CPI für durchgängige B2B-Prozesse ohne manuelle Einzelkonfigurationen pro Partner.",
          "whyRing": "Relevant ab ~20 Handelspartnern mit heterogenen Kommunikationsprotokollen. ASSESS korrekt: Plattform ist noch nicht in breiter Produktivnutzung; Integration mit IA und CPI reift kontinuierlich; Community-Erfahrung noch begrenzt. Gezielte Pilotevaluation empfohlen.",
          "risks": [
            "Geringe Marktdurchdringung bedeutet wenig Erfahrungsberichte und Community-Wissen. Integration mit bestehenden B2B-Gateways (Seeburger, OpenText) erfordert individuelle Prüfung. Migrationspfad von bestehenden Partnerkonfigurationen ist aufwändig und riskant."
          ],
          "do": [
            "Nur evaluieren wenn B2B-Volumen und AS2/EDIFACT-Szenarien vorhanden. Pilot mit 2–3 realen Partnern und vollständigem Dokumentenfluss durchführen. Integration mit SAP Integration Advisor (MIGs) von Beginn einplanen. Evaluationskriterien klar definieren (Time-to-Onboard, Fehlertransparenz, Audit-Trail)."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenige (<10) Handelspartner ohne standardisierte EDI-Anforderungen. Wenn bestehende B2B-Plattform (Seeburger, OpenText) ausreichend und langfristig gewartet."
          ]
        },
        "en": {
          "intro": "SAP Trading Partner Management (TPM) is a BTP component for centrally managing B2B trading partner configurations, communication channels (AS2, SFTP, REST), and agreement management. TPM integrates with SAP Integration Advisor (MIGs) and CPI for end-to-end B2B processes without manual per-partner configuration.",
          "whyRing": "Relevant for ~20+ trading partners with heterogeneous communication protocols. ASSESS is correct: platform is not yet in broad production use; integration with IA and CPI is continuously maturing; community experience still limited. Targeted pilot evaluation recommended.",
          "risks": [
            "Low market penetration means limited field reports and community knowledge. Integration with existing B2B gateways (Seeburger, OpenText) requires individual assessment. Migration path from existing partner configurations is effort-intensive and risky."
          ],
          "do": [
            "Evaluate only if B2B volume and AS2/EDIFACT scenarios are present. Run pilot with 2–3 real partners and full document flow. Plan SAP Integration Advisor (MIGs) integration from the start. Define clear evaluation criteria (time-to-onboard, error transparency, audit trail)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "Few (<10) trading partners without standardized EDI requirements. When existing B2B platform (Seeburger, OpenText) is adequate and long-term supported."
          ]
        }
      }
    },
    {
      "label": "Edge Integration Cell",
      "ring": "ASSESS",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/integration-suite"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/edge-integration-cell",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/operating-edge-integration-cell",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/edge-integration-cell-prerequisites"
      ],
      "confidence": "medium",
      "i18n": {
        "de": {
          "intro": "Die Edge Integration Cell (EIC) ist eine SAP-verwaltete Kubernetes-basierte Laufzeit für CPI-IFlows, die auf kundeneigener Infrastruktur (On-Premise oder Private Cloud) deployed wird. Datenverarbeitung findet lokal statt ohne Cloud-Transit. Richtet sich an Szenarien mit Datensouveränitätsanforderungen (DSGVO, Branchenregulatorik), latenzempfindlichen Integrationen oder Offline-fähigen Edge-Szenarien.",
          "whyRing": "Strategisch wichtig für Branchen mit strikten Datenlokalisierungsanforderungen (Healthcare, Public Sector, regulierte Finanzbranche). TRIAL korrekt: Kubernetes-Betriebskompetenz zwingend, Feature-Parität mit Cloud-CPI noch nicht vollständig, Kundenbasis noch begrenzt.",
          "risks": [
            "Kubernetes-Betrieb (Cluster-Management, Upgrades, Zertifikatspflege) erheblicher Zusatzaufwand. SAP EIC-Update-Zyklen müssen aktiv verfolgt und zeitnah angewendet werden. Feature-Gaps gegenüber Cloud-CPI (nicht alle Adapter verfügbar). Monitoring und Alerting erfordern separate Konfiguration vs. Cloud-CPI."
          ],
          "do": [
            "Nur einführen wenn konkrete, nachgewiesene Datensouveränitäts- oder Latenzanforderungen vorliegen. Kubernetes-Expertise vor Rollout sicherstellen (intern oder via Partner). Monitoring-Integration mit SAP Cloud ALM ab Tag 1 einrichten. Feature-Parität mit Cloud-CPI vor Projekstart prüfen (welche Adapter fehlen?)."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn keine zwingenden Datenlokalisierungs- oder Latenzanforderungen bestehen. Wenn kein Kubernetes-Betrieb intern realisierbar oder gewünscht."
          ]
        },
        "en": {
          "intro": "The Edge Integration Cell (EIC) is an SAP-managed Kubernetes-based runtime for CPI IFlows deployed on customer-owned infrastructure (on-premise or private cloud). Data processing occurs locally without cloud transit. Targets scenarios with data sovereignty requirements (GDPR, industry regulation), latency-sensitive integrations, or offline-capable edge scenarios.",
          "whyRing": "Strategically important for industries with strict data localization requirements (healthcare, public sector, regulated finance). TRIAL is correct: Kubernetes operational competency is mandatory, feature parity with cloud CPI not yet complete, customer base still limited.",
          "risks": [
            "Kubernetes operations (cluster management, upgrades, certificate maintenance) add significant overhead. SAP EIC update cycles must be actively tracked and applied promptly. Feature gaps vs. cloud CPI (not all adapters available). Monitoring and alerting require separate configuration vs. cloud CPI."
          ],
          "do": [
            "Only introduce when concrete, proven data sovereignty or latency requirements exist. Ensure Kubernetes expertise before rollout (internal or via partner). Set up monitoring integration with SAP Cloud ALM from day 1. Check feature parity with cloud CPI before project start (which adapters are missing?)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When no mandatory data localization or latency requirements exist. When internal Kubernetes operations are not feasible or desired."
          ]
        }
      }
    },
    {
      "label": "Kyma Runtime (for integration microservices)",
      "ring": "TRIAL",
      "quadrant": "Platforms",
      "sources": [
        "https://kyma-project.io/docs/"
      ],
      "references": [
        "https://help.sap.com/docs/btp/sap-business-technology-platform/kyma-environment",
        "https://kyma-project.io/docs/",
        "https://help.sap.com/docs/btp/sap-business-technology-platform/extending-sap-solutions-using-automated-configurations"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Kyma Runtime ist ein SAP-verwaltetes Kubernetes-/Knative-basiertes Laufzeitsystem auf BTP für Custom-Microservices, Event-Handler und Erweiterungskomponenten. Im Integrationskontext eignet es sich für Custom-Adapter, Event-Preprocessing, Daten-Enrichment und technische Funktionen, die im CPI-IFlow-Modell nicht sauber abbildbar sind (z. B. stateful Services, Container-basierte Fremdbibliotheken).",
          "whyRing": "Technisch reif; die SAP-strategische Laufzeit für Cloud-native BTP-Erweiterungen. Im Integrationskontext ist der Mehrwert gegenüber CPI nur bei Custom-Code, Stateful-Services oder Container-basierten Komponenten gegeben. TRIAL korrekt.",
          "risks": [
            "Container-Management- und Kubernetes-Lifecycle-Kompetenz erforderlich. Entwicklungskompetenz (Node.js, Go, Python) notwendig – abweichend vom CPI-Kompetenzprofil. Ressourcenkosten können bei nicht optimierten Deployments hoch ausfallen. Serviceabhängigkeiten zwischen CPI und Kyma erhöhen Debugging-Komplexität."
          ],
          "do": [
            "Kyma für Custom-Adapter, Event-Enricher oder komplexe Transformation einsetzen, die in CPI nicht sauber lösbar sind. Shared-Kyma-Cluster für mehrere Integrations-Microservices. Observability via OpenTelemetry von Beginn einplanen. CI/CD-Pipeline (GitHub Actions, Argo CD) für Kyma-Deployments etablieren."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn CPI-Groovy-Skripte oder External-Libraries das Problem lösen. Wenn kein Container-Betriebswissen vorhanden."
          ]
        },
        "en": {
          "intro": "Kyma Runtime is an SAP-managed Kubernetes/Knative-based runtime on BTP for custom microservices, event handlers, and extension components. In the integration context, it is suited for custom adapters, event preprocessing, data enrichment, and technical functions that cannot be cleanly modeled in the CPI IFlow model (e.g., stateful services, container-based third-party libraries).",
          "whyRing": "Technically mature; SAP's strategic runtime for cloud-native BTP extensions. In the integration context, the value over CPI exists only for custom code, stateful services, or container-based components. TRIAL is correct.",
          "risks": [
            "Container management and Kubernetes lifecycle competency required. Development expertise (Node.js, Go, Python) needed – different from CPI skill profile. Resource costs can be high with unoptimized deployments. Service dependencies between CPI and Kyma increase debugging complexity."
          ],
          "do": [
            "Use Kyma for custom adapters, event enrichers, or complex transformations that cannot be cleanly solved in CPI. Use shared Kyma cluster for multiple integration microservices. Plan observability via OpenTelemetry from the start. Establish CI/CD pipeline (GitHub Actions, Argo CD) for Kyma deployments."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When CPI Groovy scripts or external libraries solve the problem. When no container operations knowledge is available."
          ]
        }
      }
    },
    {
      "label": "SAP Build Process Automation",
      "ring": "ASSESS",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/build-process-automation"
      ],
      "references": [
        "https://help.sap.com/docs/build-process-automation",
        "https://help.sap.com/docs/build-process-automation/sap-build-process-automation/what-is-sap-build-process-automation"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "SAP Build Process Automation (SBPA) ist eine Low-Code/No-Code-Plattform auf BTP für Workflow-Automatisierung, RPA (Robotic Process Automation) und Prozessdesign durch Citizen Developer und IT. Sie integriert mit S/4HANA, SuccessFactors und anderen SAP-Anwendungen über vorgefertigte Aktionsbibliotheken und ermöglicht automatisierte Genehmigungsprozesse, Formulare und Bot-gestützte Datenverarbeitung.",
          "whyRing": "ASSESS korrekt: SBPA adressiert einen realen Automatisierungsbedarf, birgt aber ohne Governance-Rahmen das Risiko von Schatten-IT auf Integrationsebene. Evaluieren für Workflows mit menschlichen Aufgaben und Genehmigungsschritten; nicht als Ersatz für strukturierte Systemintegration via CPI.",
          "risks": [
            "Ohne klare Guardrails entstehen unkontrollierte Prozessautomationen ohne IT-Oversight. RPA-Bots brechen bei UI-Änderungen in Quellsystemen – hohe Wartungskosten. Citizen-Developer-Prozesse können produktionskritische Abhängigkeiten ohne Sicherheitsnetz schaffen. Fehlende Auditierbarkeit und Versionierung von Low-Code-Artefakten."
          ],
          "do": [
            "Center-of-Excellence (CoE)-Modell definieren: welche Automatisierungen darf Citizen Developer bauen, welche erfordern IT-Architektur-Review. Versionierung und Transport von SBPA-Artefakten via CTMS sicherstellen. Monitoring und Exception-Handling für alle produktiven Prozesse einfordern. RPA nur für Szenarien ohne API-Alternative einsetzen."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Wenn strukturierte API- oder CPI-Integration möglich und nachhaltiger. Wenn Quellsysteme häufige UI-Änderungen erfahren (RPA-Stabilität gering)."
          ]
        },
        "en": {
          "intro": "SAP Build Process Automation (SBPA) is a low-code/no-code platform on BTP for workflow automation, RPA (Robotic Process Automation), and process design by citizen developers and IT. It integrates with S/4HANA, SuccessFactors, and other SAP applications via pre-built action libraries and enables automated approval processes, forms, and bot-assisted data processing.",
          "whyRing": "ASSESS is correct: SBPA addresses a real automation need but risks shadow IT at the integration layer without a governance framework. Evaluate for workflows with human tasks and approval steps; not as a replacement for structured system integration via CPI.",
          "risks": [
            "Without clear guardrails, uncontrolled process automations emerge without IT oversight. RPA bots break when UI changes in source systems – high maintenance cost. Citizen developer processes can create production-critical dependencies without a safety net. Missing auditability and versioning of low-code artifacts."
          ],
          "do": [
            "Define a Center-of-Excellence (CoE) model: which automations may citizen developers build, which require IT architecture review. Ensure versioning and transport of SBPA artifacts via CTMS. Mandate monitoring and exception handling for all production processes. Use RPA only for scenarios without an API alternative."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "When structured API or CPI integration is possible and more sustainable. When source systems undergo frequent UI changes (RPA stability is low)."
          ]
        }
      }
    },
    {
      "label": "SAP Open Connectors",
      "ring": "HOLD",
      "quadrant": "Platforms",
      "sources": [
        "https://help.sap.com/docs/open-connectors"
      ],
      "references": [
        "https://help.openconnectors.ext.hana.ondemand.com/home",
        "https://help.sap.com/docs/open-connectors"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Open Connectors (ehemals Cloud Elements) ist eine Konnektivitätsschicht auf BTP mit ~170 vorkonfigurierten Konnektoren zu Drittanbieter-SaaS-Systemen (Salesforce, ServiceNow, HubSpot etc.) über ein harmonisiertes REST-API-Modell. Die Plattform soll Non-SAP-SaaS-Anbindung vereinfachen, hat aber kaum Weiterentwicklung erfahren.",
          "whyRing": "HOLD korrekt: SAP investiert nicht aktiv in Open Connectors. Technologie stagniert, Konnektoren-Qualität heterogen, kaum SAP-Roadmap-Erwähnungen. Keine Neuimplementierungen starten. Für Non-SAP-SaaS-Integration CPI-Adapter oder dedizierte iPaaS-Lösung bevorzugen.",
          "risks": [
            "Fehlende aktive Weiterentwicklung erhöht Ausfallrisiko bei API-Änderungen der Drittanbieter. Support-Qualität und Reaktionszeiten sinken bei stagnierenden Produkten. Kein klarer SAP-Migrationspfad kommuniziert."
          ],
          "do": [
            "Bestehende Open-Connectors-Integrationen inventarisieren und Migrationsplan erstellen. Alternative Konnektivität via CPI-Adapter oder direktem REST/OAuth2 bewerten."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Generell: für alle Neuvorhaben. Bestehende Integrationen aktiv migrieren."
          ]
        },
        "en": {
          "intro": "SAP Open Connectors (formerly Cloud Elements) is a connectivity layer on BTP with ~170 pre-configured connectors to third-party SaaS systems (Salesforce, ServiceNow, HubSpot, etc.) via a harmonized REST API model. The platform aims to simplify non-SAP SaaS connectivity but has seen minimal active development.",
          "whyRing": "HOLD is correct: SAP is not actively investing in Open Connectors. Technology is stagnating, connector quality is heterogeneous, minimal SAP roadmap mention. Do not start new implementations. Prefer CPI adapters or dedicated iPaaS solutions for non-SAP SaaS integration.",
          "risks": [
            "Lack of active development increases failure risk when third-party APIs change. Support quality and response times decline for stagnating products. No clear SAP migration path communicated."
          ],
          "do": [
            "Inventory existing Open Connectors integrations and create a migration plan. Evaluate alternative connectivity via CPI adapters or direct REST/OAuth2."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "Generally: for all new projects. Actively migrate existing integrations."
          ]
        }
      }
    },
    {
      "label": "SAP PI/PO",
      "ring": "HOLD",
      "quadrant": "Platforms",
      "sources": [
        "https://pages.community.sap.com/topics/process-orchestration"
      ],
      "references": [
        "https://pages.community.sap.com/topics/process-orchestration",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-from-sap-process-integration-and-process-orchestration",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-assessment"
      ],
      "ringChangeSuggestion": "HOLD",
      "changeRationale": "End-of-Life (2027/2030). Jede neue Investition erzeugt technische Schulden.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Process Integration (PI) und Process Orchestration (PO) sind die On-Premise-Middleware-Plattformen der Vorgängergeneration. Sie unterstützen IDoc, RFC, SOAP, REST und BPM-Prozessorchestrierung. PI/PO ist in vielen Bestandslandschaften noch breit im Einsatz, wird aber von SAP nicht mehr strategisch weiterentwickelt.",
          "whyRing": "HOLD ohne Ausnahme. Ende der Extended Maintenance: PI 7.5 Ende 2030, PO 7.5 Ende 2027 (Extended). Keine Neuimplementierungen; laufende Systeme migrieren. Strategischer Nachfolger ist SAP Cloud Integration (CPI) auf BTP.",
          "risks": [
            "Ende des regulären SAP-Supports erzwingt Migration unter Zeitdruck. Fehlende neue Adapter und Konnektoren für moderne Protokolle. Sinkende Verfügbarkeit von PI/PO-Spezialisten am Markt. Java-Stack-Abhängigkeiten (Netweaver AS Java) erzeugen zusätzliche Wartungsrisiken."
          ],
          "do": [
            "Alle PI/PO-Interfaces inventarisieren (Volumen, Kritikalität, Protokoll, Eigentümer). Migrationspfad zu CPI nach Strangler-Fig-Pattern aufsetzen – kein Big Bang. SAP-Migrationstool 'Migration Assessment' nutzen um Interfaces zu klassifizieren. Priorisierung nach Business Value und technischem Risiko (nicht nach Interface-Anzahl)."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Grundsätzlich für alle Neuvorhaben. Sofort auf Migrationspfad setzen."
          ]
        },
        "en": {
          "intro": "SAP Process Integration (PI) and Process Orchestration (PO) are the previous-generation on-premise middleware platforms. They support IDoc, RFC, SOAP, REST, and BPM process orchestration. PI/PO is still widely in use in legacy landscapes but is no longer strategically developed by SAP.",
          "whyRing": "HOLD without exception. End of extended maintenance: PI 7.5 end of 2030, PO 7.5 end of 2027. No new implementations; migrate existing systems. Strategic successor is SAP Cloud Integration (CPI) on BTP.",
          "risks": [
            "End of SAP support forces migration under time pressure. No new adapters or connectors for modern protocols. Declining availability of PI/PO specialists in the market. Java stack dependencies (NetWeaver AS Java) create additional maintenance risks."
          ],
          "do": [
            "Inventory all PI/PO interfaces (volume, criticality, protocol, owner). Set up migration path to CPI using Strangler Fig pattern – no big bang. Use SAP Migration Assessment tool to classify interfaces. Prioritize by business value and technical risk (not interface count)."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Fundamentally for all new projects. Immediately put on migration path."
          ]
        }
      }
    },
    {
      "label": "Central logging + correlation IDs",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://opentelemetry.io/docs/"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/message-monitoring",
        "https://help.sap.com/docs/cloud-alm/applicationhelp/integration-and-exception-monitoring",
        "https://opentelemetry.io/docs/concepts/observability-primer/"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Zentrales Logging mit durchgängigen Korrelations-IDs (Trace-ID, Message-ID) ist die Grundvoraussetzung für Observability in verteilten Integrationslandschaften. Jeder IFlow, jeder API-Call und jede Event-Verarbeitung muss eine nachverfolgbare Korrelations-ID erzeugen und weiterleiten, die über Systemgrenzen hinweg erhalten bleibt. In BTP-Landschaften kann SAP Cloud ALM oder ein externer Log-Aggregator (ELK, Splunk) genutzt werden.",
          "whyRing": "ADOPT ohne Einschränkung. Ohne korrelierte Logs ist Fehlerdiagnose in verteilten Integrationen nicht skalierbar. Grundlegende Hygiene-Maßnahme.",
          "risks": [
            "Fehlende Korrelations-IDs machen End-to-End-Tracing unmöglich – Diagnose kostet Stunden statt Minuten. Log-Daten ohne Klassifizierung (Level, Fehlertyp) schwer auswertbar. Personenbezogene Daten in Logs → DSGVO-Risiko wenn Logs unverschlüsselt oder zu lang aufbewahrt."
          ],
          "do": [
            "SAP-Message-ID oder eigene UUID als Korrelations-ID in jedem IFlow in den Header schreiben und an alle nachgelagerten Calls weitergeben. Strukturierte Logs (JSON) mit Pflichtfeldern (Timestamp, Severity, CorrelationID, SystemID). Log-Retention-Policy definieren (DSGVO: Personenbezug prüfen, max. Aufbewahrung festlegen). Zentrales Log-Aggregations-Dashboard mit Alert-Regeln für kritische Fehlerklassen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Keine Ausnahme – Pflicht für alle produktiven Integrationen."
          ]
        },
        "en": {
          "intro": "Centralized logging with end-to-end correlation IDs (trace ID, message ID) is the prerequisite for observability in distributed integration landscapes. Every IFlow, every API call, and every event processing step must generate and propagate a traceable correlation ID preserved across system boundaries. BTP landscapes can use SAP Cloud ALM or an external log aggregator (ELK, Splunk).",
          "whyRing": "ADOPT without restriction. Without correlated logs, error diagnosis in distributed integrations does not scale. This is a fundamental hygiene measure.",
          "risks": [
            "Missing correlation IDs make end-to-end tracing impossible – diagnosis takes hours instead of minutes. Log data without classification (level, error type) is hard to analyze. Personal data in logs → GDPR risk if logs are unencrypted or retained too long."
          ],
          "do": [
            "Write SAP-Message-ID or custom UUID as correlation ID in every IFlow header and propagate to all downstream calls. Structured logs (JSON) with mandatory fields (timestamp, severity, correlationID, systemID). Define log retention policy (GDPR: check for personal data, set max retention). Central log aggregation dashboard with alert rules for critical error classes."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "No exception – mandatory for all production integrations."
          ]
        }
      }
    },
    {
      "label": "CI/CD for integration artifacts",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/automation-pilot"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-reference",
        "https://help.sap.com/docs/automation-pilot",
        "https://github.com/SAP-samples/cloud-integration-flow"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "Deployment-Automatisierung für CPI-Artefakte ist mittlerweile gut toolgestützt (SAP CPI-API, Automation Pilot). Teams mit mehreren parallelen Integrationslieferungen benötigen CI/CD als Standard. Empfehle ADOPT mit der Bedingung: automatisierte Test-Stage ist Pflicht. Confidence: Medium.",
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "CI/CD für Integrationsartefakte umfasst automatisierte Build-, Test- und Deployment-Pipelines für CPI-IFlows, API-Proxies und zugehörige Konfigurationsdateien. Über die CPI-API und SAP Automation Pilot können IFlows per Git-gesteuerten Pipelines (GitHub Actions, Azure DevOps, Jenkins) deployed, getestet und promoviert werden. Ziel: reproduzierbare, auditierbare Deployments ohne manuelle Eingriffe.",
          "whyRing": "TRIAL korrekt: Tooling ist verfügbar (SAP CPI API, Terraform für BTP, Automation Pilot), aber Setups sind noch nicht standardisiert. Wer früh investiert, gewinnt erheblich an Deployment-Sicherheit und Auditierbarkeit. Mittelfristig ADOPT-Kandidat.",
          "risks": [
            "CPI-API-Versionsänderungen können Pipeline-Skripte brechen. Geheimnisse (API-Keys, Credentials) in Pipelines ohne Vault-Integration führen zu Security-Risiken. Ohne automatisierte Integrationstests in der Pipeline führt Continuous Deployment zu ungeprüften Releases. Fehlende Rollback-Mechanismen bei fehlgeschlagenen Deployments."
          ],
          "do": [
            "Git als Single Source of Truth für alle IFlow-Konfigurationen (MTAR-Packages). Pipeline: Lint → Unit Test → Deploy DEV → Integrationstest → Promote QA → Promote PROD. Secrets in Vault (HashiCorp Vault, Azure Key Vault) – nie in Pipeline-Variablen im Klartext. Deployment-Logs und Audit-Trail automatisch in SAP Cloud ALM oder SIEM schreiben."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Nicht für einzelne Proof-of-Concept-IFlows ohne Produktivrelevanz. Nicht wenn CTMS bereits als Transport-Mechanismus ausreicht und kein Automatisierungsbedarf besteht."
          ]
        },
        "en": {
          "intro": "CI/CD for integration artifacts encompasses automated build, test, and deployment pipelines for CPI IFlows, API proxies, and related configuration files. Via the CPI API and SAP Automation Pilot, IFlows can be deployed, tested, and promoted via Git-driven pipelines (GitHub Actions, Azure DevOps, Jenkins). Goal: reproducible, auditable deployments without manual intervention.",
          "whyRing": "TRIAL is correct: tooling is available (SAP CPI API, Terraform for BTP, Automation Pilot) but setups are not yet standardized. Early adopters gain significant deployment safety and auditability. Medium-term ADOPT candidate.",
          "risks": [
            "CPI API version changes can break pipeline scripts. Secrets (API keys, credentials) in pipelines without Vault integration create security risks. Without automated integration tests in the pipeline, continuous deployment leads to unvalidated releases. Missing rollback mechanisms for failed deployments."
          ],
          "do": [
            "Git as single source of truth for all IFlow configurations (MTAR packages). Pipeline: Lint → Unit Test → Deploy DEV → Integration Test → Promote QA → Promote PROD. Secrets in Vault (HashiCorp Vault, Azure Key Vault) – never as plaintext pipeline variables. Automatically write deployment logs and audit trail to SAP Cloud ALM or SIEM."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Not for single proof-of-concept IFlows without production relevance. Not when CTMS already suffices as a transport mechanism and no automation need exists."
          ]
        }
      }
    },
    {
      "label": "Secret handling via vault/KMS",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/credential-store"
      ],
      "references": [
        "https://help.sap.com/docs/credential-store",
        "https://developer.hashicorp.com/vault/docs",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/managing-security-material"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Strukturiertes Secret-Management für Integrations-Credentials (API-Keys, OAuth2-Secrets, Zertifikate, DB-Passwörter) über einen zentralen Secrets Store: HashiCorp Vault, Azure Key Vault, AWS KMS oder SAP Credential Store (BTP). Verhindert Klartext-Credentials in IFlow-Konfigurationen, Pipeline-Variablen oder Git-Repositories. Ermöglicht Rotation ohne Code-Änderungen.",
          "whyRing": "ADOPT ohne Ausnahme. Klartext-Credentials in Integrationsartefakten sind ein kritisches Sicherheitsrisiko und verstoßen gegen jede gängige Security-Policy. SAP BTP bietet einen nativen Credential Store; für unternehmensweites Secret-Management ist ein zentraler Vault empfohlen.",
          "risks": [
            "Credentials in Git-History oder IFlow-Konfigurationen sind dauerhaft exponiert – auch nach Rotation. Fehlende Rotation-Automatisierung führt zu abgelaufenen Credentials und Integrationsausfällen. Kein Audit-Trail über Credential-Zugriffe."
          ],
          "do": [
            "SAP BTP Credential Store für CPI-interne Secrets nutzen. Für unternehmensweites Secret-Management: HashiCorp Vault oder Cloud-KMS integrieren. Rotation-Mechanismus und Ablauf-Alerting für alle Credentials definieren. Audit-Log für alle Secret-Zugriffe aktivieren. Least-Privilege-Prinzip: jeder IFlow nur Zugriff auf die notwendigen Credentials."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Keine Ausnahme – Pflicht für alle produktiven Integrationen."
          ]
        },
        "en": {
          "intro": "Structured secret management for integration credentials (API keys, OAuth2 secrets, certificates, DB passwords) via a central secrets store: HashiCorp Vault, Azure Key Vault, AWS KMS, or SAP Credential Store (BTP). Prevents plaintext credentials in IFlow configurations, pipeline variables, or Git repositories. Enables rotation without code changes.",
          "whyRing": "ADOPT without exception. Plaintext credentials in integration artifacts are a critical security risk and violate any standard security policy. SAP BTP provides a native Credential Store; a central Vault is recommended for enterprise-wide secret management.",
          "risks": [
            "Credentials in Git history or IFlow configurations are permanently exposed – even after rotation. Missing rotation automation leads to expired credentials and integration outages. No audit trail for credential access."
          ],
          "do": [
            "Use SAP BTP Credential Store for CPI-internal secrets. For enterprise-wide secret management: integrate HashiCorp Vault or cloud KMS. Define rotation mechanism and expiry alerting for all credentials. Enable audit log for all secret access. Least-privilege: each IFlow only has access to its required credentials."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "No exception – mandatory for all production integrations."
          ]
        }
      }
    },
    {
      "label": "OpenTelemetry",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://opentelemetry.io/docs/"
      ],
      "references": [
        "https://opentelemetry.io/docs/",
        "https://opentelemetry.io/docs/specs/otel/",
        "https://github.com/open-telemetry/opentelemetry-collector",
        "https://www.cncf.io/projects/opentelemetry/"
      ],
      "confidence": "Medium",
      "ringChangeAppliedFrom": "review-round-3",
      "i18n": {
        "de": {
          "intro": "OpenTelemetry (OTel) ist ein CNCF-Standard für Observability mit drei Signaltypen: Traces, Metrics und Logs. Es definiert ein SDK, Collector-Architektur und Wire-Protokoll (OTLP). Im SAP-Integrationskontext ermöglicht OTel End-to-End-Tracing über CPI, Kyma-Services und Non-SAP-Systeme hinweg, unabhängig vom Observability-Backend (Grafana, Datadog, Dynatrace).",
          "whyRing": "TRIAL korrekt: OTel ist industriell etabliert (CNCF Graduated Project), aber die native CPI-OTel-Integration ist noch nicht vollständig. Instrumentierung über CPI-Groovy und OTLP-Exporter ist möglich und empfohlen für Teams, die über Log-Level hinaus wollen.",
          "risks": [
            "CPI-native OTel-Unterstützung noch begrenzt – manuelle Instrumentierung über Groovy-Skripte nötig. Sampling-Strategie ohne Sorgfalt führt zu hohen Datenmengern oder blinden Flecken. Collector-Konfiguration und -Betrieb erfordert Expertise."
          ],
          "do": [
            "OTLP-fähigen Collector (SAP-managed oder self-hosted) als zentralen Observability-Hub einrichten. Trace-Context (W3C TraceContext) in CPI-Headern propagieren. Strukturierte Logs als Log-Signal in OTel einbinden. Alerting auf Basis von OTel-Metrics (Fehlerrate, Latenz-Percentile) definieren."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn SAP Cloud ALM bereits ausreichende Observability-Abdeckung liefert und kein Cross-System-Tracing erforderlich."
          ]
        },
        "en": {
          "intro": "OpenTelemetry (OTel) is a CNCF standard for observability with three signal types: traces, metrics, and logs. It defines an SDK, collector architecture, and wire protocol (OTLP). In the SAP integration context, OTel enables end-to-end tracing across CPI, Kyma services, and non-SAP systems, independent of the observability backend (Grafana, Datadog, Dynatrace).",
          "whyRing": "TRIAL is correct: OTel is industrially established (CNCF Graduated Project), but native CPI OTel integration is not yet complete. Instrumentation via CPI Groovy and OTLP exporter is possible and recommended for teams wanting to go beyond log level.",
          "risks": [
            "CPI native OTel support is still limited – manual instrumentation via Groovy scripts required. Without careful sampling strategy, data volumes become large or blind spots emerge. Collector configuration and operations require expertise."
          ],
          "do": [
            "Set up an OTLP-capable collector (SAP-managed or self-hosted) as the central observability hub. Propagate trace context (W3C TraceContext) in CPI headers. Include structured logs as log signals in OTel. Define alerting based on OTel metrics (error rate, latency percentiles)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When SAP Cloud ALM already provides sufficient observability coverage and no cross-system tracing is required."
          ]
        }
      }
    },
    {
      "label": "Contract testing for integrations",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://docs.pact.io/"
      ],
      "references": [
        "https://docs.pact.io/",
        "https://spec.openapis.org/oas/latest.html",
        "https://stoplight.io/open-source/spectral"
      ],
      "confidence": "Medium",
      "ringChangeAppliedFrom": "review-round-3",
      "i18n": {
        "de": {
          "intro": "Contract Testing verifiziert, dass API-Producer und -Consumer kompatibel bleiben, ohne End-to-End-Integrationstests zu benötigen. Im SAP-Kontext: Pact-Frameworks oder OpenAPI-Schema-Validierung sichern, dass S/4HANA OData-APIs oder CPI-IFlow-Outputs den vereinbarten Vertrag (Schema, HTTP-Status-Codes, Pflichtfelder) einhalten. Consumer-Driven Contract Tests geben dem Consumer-Team Sicherheit bei Producer-Änderungen.",
          "whyRing": "TRIAL korrekt: Contract Testing ist für moderne API-First-Landschaften essenziell, aber im SAP-Ökosystem noch wenig verbreitet. Tooling (Pact, Schemathesis) ist reif; Adoption erfordert kulturellen Wandel (Teams müssen Kontrakte austauschen).",
          "risks": [
            "Ohne Contract Tests werden Breaking Changes erst in Integration- oder Produktivtests entdeckt. Pact-Broker-Betrieb als zusätzliche Infrastruktur notwendig. S/4HANA OData-APIs können nicht immer als Producer-Mock aufgesetzt werden."
          ],
          "do": [
            "Consumer-Teams definieren Vertragserwartungen als Pact-Spezifikation oder OpenAPI-Schemata. Provider-seitige Vertragsvalidierung in CI/CD-Pipeline einbinden. Pact Broker oder Spectral für Schema-Validierung als zentrale Registry nutzen. Mit 2–3 APIs als Pilot starten, bevor breit ausgerollt wird."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn APIs stabil und ohne regelmäßige Änderungen (Legacy-Systeme ohne Weiterentwicklung). Wenn keine CI/CD-Pipeline vorhanden, in die Tests integriert werden könnten."
          ]
        },
        "en": {
          "intro": "Contract Testing verifies that API producers and consumers remain compatible without requiring end-to-end integration tests. In the SAP context: Pact frameworks or OpenAPI schema validation ensure that S/4HANA OData APIs or CPI IFlow outputs comply with the agreed contract (schema, HTTP status codes, required fields). Consumer-driven contract tests give consumer teams confidence during producer changes.",
          "whyRing": "TRIAL is correct: contract testing is essential for modern API-first landscapes but still uncommon in the SAP ecosystem. Tooling (Pact, Schemathesis) is mature; adoption requires cultural change (teams must exchange contracts).",
          "risks": [
            "Without contract tests, breaking changes are only discovered in integration or production tests. Pact Broker operation requires additional infrastructure. S/4HANA OData APIs cannot always be set up as producer mocks."
          ],
          "do": [
            "Consumer teams define contract expectations as Pact specs or OpenAPI schemas. Include provider-side contract validation in CI/CD pipeline. Use Pact Broker or Spectral for schema validation as central registry. Start with 2–3 APIs as pilot before broad rollout."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When APIs are stable and without regular changes (legacy systems without further development). When no CI/CD pipeline exists to integrate tests into."
          ]
        }
      }
    },
    {
      "label": "Terraform for SAP BTP",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://registry.terraform.io/providers/SAP/btp/latest"
      ],
      "references": [
        "https://registry.terraform.io/providers/SAP/btp/latest",
        "https://github.com/SAP/terraform-provider-btp",
        "https://help.sap.com/docs/btp/sap-business-technology-platform/account-administration"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "IaC für BTP ist mittlerweile durch den offiziellen SAP-Provider gut unterstützt. Für Teams mit mehreren Subaccounts und CI/CD-Betrieb ist Terraform Best Practice. Empfehle ADOPT für Teams mit entsprechendem Reifegrad. Confidence: Medium.",
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Der offizielle SAP BTP Terraform Provider ermöglicht Infrastructure-as-Code für BTP-Ressourcen: Subaccounts, Entitlements, Service-Instanzen, Role Collections und Destinations. Damit werden BTP-Landschaften reproduzierbar, versioniert und automatisierbar. Pipelines können BTP-Umgebungen konsistent über DEV/QA/PROD erstellen ohne manuelles Cockpit-Klicken.",
          "whyRing": "TRIAL korrekt: Offizieller SAP-Provider existiert und ist in aktiver Entwicklung. Nicht alle BTP-Ressourcen sind bereits abgedeckt. Teams, die mehrere BTP-Subaccounts verwalten, profitieren bereits heute deutlich.",
          "risks": [
            "Provider deckt noch nicht alle BTP-Ressourcentypen ab – manuelle Reste bleiben. Terraform-State-Management (Remote State, Locking) muss sicher eingerichtet werden. Fehlerhafter Terraform-Apply kann BTP-Konfigurationen zerstören – Dry-Run-Pflicht. IaC-Kompetenz (Terraform/HCL) muss im Team aufgebaut werden."
          ],
          "do": [
            "Remote State Backend (Terraform Cloud, S3, Azure Blob) mit State-Locking von Beginn. Terraform Plan immer vor Apply prüfen – kein blindes Apply. BTP-Subaccount-Struktur und Entitlements als IaC-Grundlage für alle neuen Umgebungen. Module für wiederverwendbare BTP-Muster (Integration Suite, Event Mesh) erstellen."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Wenn nur 1–2 BTP-Subaccounts ohne Automation-Bedarf. Wenn kein Terraform-Know-how im Team und keine Ressourcen zum Aufbau."
          ]
        },
        "en": {
          "intro": "The official SAP BTP Terraform provider enables Infrastructure-as-Code for BTP resources: subaccounts, entitlements, service instances, role collections, and destinations. This makes BTP landscapes reproducible, versioned, and automatable. Pipelines can create BTP environments consistently across DEV/QA/PROD without manual cockpit interaction.",
          "whyRing": "TRIAL is correct: official SAP provider exists and is in active development. Not all BTP resources are covered yet. Teams managing multiple BTP subaccounts already benefit significantly today.",
          "risks": [
            "Provider does not yet cover all BTP resource types – manual residuals remain. Terraform state management (remote state, locking) must be securely set up. Incorrect terraform apply can destroy BTP configurations – dry-run is mandatory. IaC competency (Terraform/HCL) must be built in the team."
          ],
          "do": [
            "Remote state backend (Terraform Cloud, S3, Azure Blob) with state locking from day one. Always review Terraform Plan before Apply – no blind apply. BTP subaccount structure and entitlements as IaC baseline for all new environments. Create modules for reusable BTP patterns (Integration Suite, Event Mesh)."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "When only 1–2 BTP subaccounts with no automation need. When no Terraform knowledge in the team and no resources to build it."
          ]
        }
      }
    },
    {
      "label": "Cloud Integration Automation Tool",
      "ring": "ASSESS",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/cloud-integration-automation"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration-automation",
        "https://help.sap.com/docs/cloud-integration-automation/user-guide/what-is-sap-cloud-integration-automation-service"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Das SAP Cloud Integration Automation Tool (CIAT) ist ein BTP-Dienst, der geführte Integrations-Konfigurationsabläufe für SAP-Szenarien bereitstellt (z. B. S/4HANA → SAP Ariba, SAP SuccessFactors). Es automatisiert repetitive Konfigurationsschritte und reduziert Implementierungszeit für Standard-SAP-Integrationsszenarien.",
          "whyRing": "ASSESS korrekt: CIAT deckt einen nützlichen, aber schmalen Anwendungsfall ab (SAP-zu-SAP-Standardszenarien). Außerhalb dieser Szenarien kein Mehrwert. Evaluieren für Projekte mit vielen SAP-Standardintegrationen.",
          "risks": [
            "Sehr begrenzte Abdeckung – nur SAP-zu-SAP-Szenarien mit vordefiniertem Scope. Geringe Flexibilität für kundenspezifische Erweiterungen. Abhängigkeit von SAP-seitiger Szenariopflege."
          ],
          "do": [
            "Vor Pilotstart prüfen, ob das eigene Szenario im CIAT-Katalog abgedeckt ist. CIAT-Output als Startpunkt, nicht als finales Ergebnis behandeln."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Wenn Integrationsszenarien nicht im CIAT-Katalog abgedeckt sind. Für alle Non-SAP-zu-SAP-Szenarien."
          ]
        },
        "en": {
          "intro": "SAP Cloud Integration Automation Tool (CIAT) is a BTP service providing guided integration configuration workflows for SAP scenarios (e.g., S/4HANA → SAP Ariba, SAP SuccessFactors). It automates repetitive configuration steps and reduces implementation time for standard SAP integration scenarios.",
          "whyRing": "ASSESS is correct: CIAT covers a useful but narrow use case (SAP-to-SAP standard scenarios). No value outside these scenarios. Evaluate for projects with many SAP standard integrations.",
          "risks": [
            "Very limited coverage – only SAP-to-SAP scenarios with predefined scope. Low flexibility for customer-specific extensions. Dependency on SAP-side scenario maintenance."
          ],
          "do": [
            "Before piloting, verify that your scenario is covered in the CIAT catalog. Treat CIAT output as a starting point, not a final result."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "When integration scenarios are not covered in the CIAT catalog. For all non-SAP-to-SAP scenarios."
          ]
        }
      }
    },
    {
      "label": "CPI DevOps suites (Figaf/Int4)",
      "ring": "ASSESS",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://figaf.com/integration-suite/"
      ],
      "references": [
        "https://figaf.com/integration-suite/",
        "https://www.int4.com/",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-reference"
      ],
      "confidence": "Low",
      "i18n": {
        "de": {
          "intro": "Kommerzielle DevOps-Werkzeuge (Figaf Integration Suite, Int4 iFlowTester u. a.) ergänzen den SAP-nativen CPI-Toolstack um automatisiertes Testen, Regressionstest-Frameworks, Impact-Analyse und erweiterte Deployment-Pipelines für CPI-IFlows. Sie adressieren Lücken, die mit reinen SAP-Bordmitteln schwer zu schließen sind.",
          "whyRing": "ASSESS korrekt: Nische, aber legitim. Für Teams mit hohem Testaufwand und vielen IFlows kann ROI nachgewiesen werden. Evaluieren wenn native CPI-API-basierte CI/CD-Pipelines nicht ausreichen.",
          "risks": [
            "Zusätzlicher Vendor-Lock-in neben SAP. Kosten-Nutzen-Verhältnis muss klar gemessen werden. Tool-Versions-Kompatibilität mit CPI-API-Updates muss aktiv verfolgt werden."
          ],
          "do": [
            "Proof of Value mit realem Testfall vor Lizenzkauf durchführen. Total Cost of Ownership (Lizenz + Integration + Pflege) gegen Build-Eigenentwicklung abwägen. Langfristige Vendor-Stabilität prüfen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn native CPI-API + GitHub Actions bereits ausreichende Automatisierung liefert. Wenn Budget für Drittanbieter-Lizenzen nicht vorhanden."
          ]
        },
        "en": {
          "intro": "Commercial DevOps tools (Figaf Integration Suite, Int4 iFlowTester, etc.) extend the SAP-native CPI toolstack with automated testing, regression test frameworks, impact analysis, and advanced deployment pipelines for CPI IFlows. They address gaps that are hard to close with SAP tools alone.",
          "whyRing": "ASSESS is correct: niche but legitimate. For teams with high test overhead and many IFlows, ROI can be demonstrated. Evaluate when native CPI-API-based CI/CD pipelines are insufficient.",
          "risks": [
            "Additional vendor lock-in beyond SAP. Cost-benefit ratio must be clearly measured. Tool version compatibility with CPI API updates must be actively tracked."
          ],
          "do": [
            "Run proof of value with a real test case before purchasing a license. Compare total cost of ownership (license + integration + maintenance) against building in-house. Assess long-term vendor stability."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When native CPI API + GitHub Actions already provides sufficient automation. When budget for third-party licenses is not available."
          ]
        }
      }
    },
    {
      "label": "SAP Cloud Transport Management (CTMS) + Content Agent",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/transport-management-service"
      ],
      "references": [
        "https://help.sap.com/docs/transport-management-service",
        "https://help.sap.com/docs/content-agent-service",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/content-transport"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Cloud Transport Management Service (CTMS) in Kombination mit dem Content Agent Service ermöglicht strukturierten, auditierbaren Transport von CPI-Integrationsartefakten und API-Proxy-Konfigurationen über Entwicklungs-, Test- und Produktivlandschaften (DEV→QA→PROD). CTMS ersetzt manuelle Export/Import-Prozesse und bildet die Grundlage für kontrolliertes Change Management auf BTP.",
          "whyRing": "ADOPT ohne Einschränkung. Jede Organisation, die CPI produktiv betreibt, benötigt CTMS. Manuelles Deployment in Produktion ist ein Audit- und Stabilitätsrisiko.",
          "risks": [
            "Initiale Einrichtung komplex (Content Agent, Transport-Nodes, Subaccount-Verbindungen). Ohne Transportrouten-Definition ist CTMS nicht funktionsfähig. Notfall-Deployments ohne CTMS-Bypass-Prozess und Dokumentation führen zu Audit-Lücken."
          ],
          "do": [
            "CTMS-Transportrouten DEV→QA→PROD vor Beginn des ersten Projekts einrichten. Content Agent für alle CPI- und APIM-Artefakte konfigurieren. CTMS-Transportaufträge mit Jira/ServiceNow-Tickets verknüpfen (Audit-Trail). Notfall-Deployment-Prozess mit Genehmigungsworkflow dokumentieren."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Keine Ausnahme – Pflicht für alle produktiven CPI/APIM-Landschaften."
          ]
        },
        "en": {
          "intro": "SAP Cloud Transport Management Service (CTMS) combined with the Content Agent Service enables structured, auditable transport of CPI integration artifacts and API proxy configurations across development, test, and production landscapes (DEV→QA→PROD). CTMS replaces manual export/import processes and forms the foundation for controlled change management on BTP.",
          "whyRing": "ADOPT without restriction. Every organization running CPI in production needs CTMS. Manual production deployment is an audit and stability risk.",
          "risks": [
            "Initial setup is complex (Content Agent, transport nodes, subaccount connections). Without transport route definitions, CTMS is non-functional. Emergency deployments without a CTMS bypass process and documentation create audit gaps."
          ],
          "do": [
            "Set up CTMS transport routes DEV→QA→PROD before the first project begins. Configure Content Agent for all CPI and APIM artifacts. Link CTMS transport orders to Jira/ServiceNow tickets (audit trail). Document emergency deployment process with approval workflow."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "No exception – mandatory for all production CPI/APIM landscapes."
          ]
        }
      }
    },
    {
      "label": "Content transport via CTS+ (transition)",
      "ring": "HOLD",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/transport-management-service"
      ],
      "references": [
        "https://help.sap.com/docs/transport-management-service",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/content-transport"
      ],
      "ringChangeSuggestion": "HOLD",
      "changeRationale": "CTS+ ist für Cloud-Artefakt-Transport architektonisch nicht der strategische Weg. SAP positioniert CTMS als Ziel. Empfehle HOLD: keine neuen Projekte auf CTS+ für CPI. Confidence: Medium.",
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "CTS+ (Enhanced Change and Transport System) ist der ABAP-basierte SAP-Transportmechanismus, der in hybriden Szenarien auch für CPI-Artefakte genutzt werden kann. CTS+ ist eine Übergangslösung für Organisationen, die noch kein CTMS aufgebaut haben oder CPI-Transport in bestehende ABAP-TMS-Prozesse integrieren müssen.",
          "whyRing": "ASSESS für Übergangsszenario; nicht als strategische Lösung. Strategisches Ziel ist CTMS + Content Agent. CTS+ nur akzeptabel wenn kurzfristige Brückenlösung mit klarem Migrationspfad zu CTMS.",
          "risks": [
            "CTS+ ist für Cloud-Artefakte nicht optimal – Einschränkungen bei Artefakttypen und Granularität. Abhängigkeit vom ABAP-TMS-Stack für Cloud-Transporte ist architektonisch problematisch. SAP empfiehlt CTMS als Zielzustand."
          ],
          "do": [
            "CTS+ nur als Übergangslösung mit explizitem Migrationspfad zu CTMS akzeptieren. Zeitplan für CTMS-Migration von Beginn festlegen."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Für alle Neuaufbauten: direkt CTMS nutzen."
          ]
        },
        "en": {
          "intro": "CTS+ (Enhanced Change and Transport System) is the ABAP-based SAP transport mechanism that can also be used for CPI artifacts in hybrid scenarios. CTS+ is a transitional solution for organizations that have not yet set up CTMS or need to integrate CPI transport into existing ABAP TMS processes.",
          "whyRing": "ASSESS for transition scenario; not as a strategic solution. Strategic target is CTMS + Content Agent. CTS+ only acceptable as a short-term bridge with a clear migration path to CTMS.",
          "risks": [
            "CTS+ is not optimal for cloud artifacts – limitations on artifact types and granularity. Dependency on ABAP TMS stack for cloud transports is architecturally problematic. SAP recommends CTMS as the target state."
          ],
          "do": [
            "Accept CTS+ only as a transitional solution with an explicit migration path to CTMS. Define timeline for CTMS migration from the start."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "For all new builds: use CTMS directly."
          ]
        }
      }
    },
    {
      "label": "SAP Cloud ALM integration & exception monitoring",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/cloud-alm"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-alm",
        "https://help.sap.com/docs/cloud-alm/applicationhelp/integration-and-exception-monitoring",
        "https://support.sap.com/en/alm/sap-cloud-alm.html"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SAP Cloud ALM (Application Lifecycle Management) ist die Cloud-native Nachfolgeplattform für SAP Solution Manager. Im Integrationskontext bietet Cloud ALM zentrale Exception-Überwachung für CPI-Fehler, Alerting, Health-Monitoring für BTP-Services und Integration mit ITSM-Systemen (ServiceNow). Cloud ALM ist in der BTP-Entitlements für S/4HANA-Kunden oft bereits inklusive.",
          "whyRing": "ADOPT: Cloud ALM ist für SAP-Kunden oft kostenfrei verfügbar und bietet Out-of-the-box-Integration mit CPI-Monitoring. Kein Grund, es nicht zu nutzen wenn S/4HANA im Einsatz.",
          "risks": [
            "Cloud ALM deckt primär SAP-Applikationen ab – Non-SAP-Systeme erfordern zusätzliche Observability-Tools. Alert-Konfiguration ohne Schwellwert-Kalibrierung führt zu Alert-Fatigue. ITSM-Integration (ServiceNow) erfordert initiale Einrichtung und Prozessdefinition."
          ],
          "do": [
            "CPI Exception Monitoring in Cloud ALM von Beginn aktivieren. Alert-Schwellwerte und Eskalationspfade klar definieren (wer bekommt welchen Alert?). ITSM-Integration (ServiceNow, Jira) für automatische Ticket-Erstellung bei kritischen Fehlern. Cloud ALM als primäres Ops-Dashboard für SAP-Integrations-Betrieb nutzen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn kein SAP BTP / S/4HANA im Stack. Wenn vollständige Observability-Plattform (Dynatrace, Datadog) bereits vorhanden – Cloud ALM komplementär, nicht konkurrierend nutzen."
          ]
        },
        "en": {
          "intro": "SAP Cloud ALM (Application Lifecycle Management) is the cloud-native successor platform for SAP Solution Manager. In the integration context, Cloud ALM provides centralized exception monitoring for CPI errors, alerting, health monitoring for BTP services, and integration with ITSM systems (ServiceNow). Cloud ALM is often included in BTP entitlements for S/4HANA customers.",
          "whyRing": "ADOPT: Cloud ALM is often available at no additional cost for SAP customers and offers out-of-the-box integration with CPI monitoring. No reason not to use it when S/4HANA is in scope.",
          "risks": [
            "Cloud ALM primarily covers SAP applications – non-SAP systems require additional observability tools. Alert configuration without threshold calibration leads to alert fatigue. ITSM integration (ServiceNow) requires initial setup and process definition."
          ],
          "do": [
            "Activate CPI Exception Monitoring in Cloud ALM from day one. Clearly define alert thresholds and escalation paths (who receives which alert?). ITSM integration (ServiceNow, Jira) for automatic ticket creation on critical errors. Use Cloud ALM as the primary ops dashboard for SAP integration operations."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When no SAP BTP / S/4HANA is in the stack. When a complete observability platform (Dynatrace, Datadog) is already in place – use Cloud ALM as a complement, not a competitor."
          ]
        }
      }
    },
    {
      "label": "Cloud Integration central monitoring & alerting",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/integration-suite"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/operate-and-monitor-integrations",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/managing-alerts"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Das integrierte CPI-Monitoring (Message Monitor, Operations Cockpit) in der SAP Integration Suite ermöglicht die Überwachung von IFlow-Ausführungen, Message-Status, Payload-Inspektion und die Konfiguration von Alert-Regeln. Ergänzt durch SAP Cloud ALM für übergreifende Exception-Überwachung und ITSM-Integration.",
          "whyRing": "ADOPT: Grundlegende Betriebsfähigkeit einer CPI-Landschaft ist ohne konfiguriertes Monitoring nicht herstellbar. Das CPI-eigene Monitoring ist die erste Verteidigungslinie.",
          "risks": [
            "Default-Monitoring ohne Alert-Regeln erkennt Fehler erst reaktiv. Payload-Logging mit sensiblen Daten in CPI-Traces ist DSGVO-relevant. Monitoring-Zugriffsrechte ohne RBAC-Kontrolle sind ein Sicherheitsrisiko."
          ],
          "do": [
            "Alert-Regeln für alle produktiven IFlows definieren (Fehlerrate, Laufzeit-Überschreitung). Payload-Logging gezielt und zeitlich begrenzt aktivieren (nie permanent für sensible Daten). Monitoring-Berechtigungen nach Least-Privilege vergeben (Ops ≠ Developer ≠ Admin). Retention-Policy für Message-Logs festlegen (Standard 30 Tage – je nach Compliance prüfen)."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Keine Ausnahme – obligatorisch für alle produktiven CPI-Landschaften."
          ]
        },
        "en": {
          "intro": "The built-in CPI monitoring (Message Monitor, Operations Cockpit) in the SAP Integration Suite enables monitoring of IFlow executions, message status, payload inspection, and alert rule configuration. Complemented by SAP Cloud ALM for cross-system exception monitoring and ITSM integration.",
          "whyRing": "ADOPT: Basic operational capability of a CPI landscape cannot be established without configured monitoring. CPI's own monitoring is the first line of defense.",
          "risks": [
            "Default monitoring without alert rules only detects errors reactively. Payload logging with sensitive data in CPI traces is GDPR-relevant. Monitoring access rights without RBAC control are a security risk."
          ],
          "do": [
            "Define alert rules for all production IFlows (error rate, runtime threshold exceeded). Activate payload logging selectively and time-limited (never permanently for sensitive data). Grant monitoring permissions by least-privilege (Ops ≠ Developer ≠ Admin). Define retention policy for message logs (default 30 days – review per compliance requirements)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "No exception – mandatory for all production CPI landscapes."
          ]
        }
      }
    },
    {
      "label": "Integration Assessment (ISA-M)",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/integration-assessment"
      ],
      "references": [
        "https://help.sap.com/docs/integration-assessment",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/integration-assessment",
        "https://www.sap.com/documents/2019/11/edd1fde9-7a7d-0010-87a3-c30de2ffd8ff.html"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Das SAP Integration Solution Advisory Methodology (ISA-M) Framework und das zugehörige BTP-Tool 'Integration Assessment' unterstützen die systematische Analyse von Integrationsanforderungen und die Ableitung geeigneter Integrationstechnologien. Es kategorisiert Integrationsszenarien nach Use-Case-Pattern (A2A, B2B, B2G, API) und empfiehlt passende SAP-Technologien.",
          "whyRing": "ADOPT: ISA-M ist der SAP-empfohlene Ansatz für Integrations-Strategie und Technologieauswahl. Das BTP-Tool schafft Transparenz über die eigene Integrationslandschaft und verhindert willkürliche Technologieentscheidungen.",
          "risks": [
            "ISA-M-Ergebnisse sind Empfehlungen, keine Garantien – kritisches Hinterfragen bleibt nötig. Ohne aktuelle Datenpflege veraltet das Integrations-Inventar schnell. ISA-M deckt Non-SAP-Technologien nicht vollständig ab."
          ],
          "do": [
            "ISA-M Assessment zu Beginn jedes größeren Integrationsprojekts durchführen. Integrations-Inventar im BTP-Tool pflegen und halbjährlich aktualisieren. ISA-M-Ergebnisse als Grundlage für Architecture Review Board-Entscheidungen nutzen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Nicht für einzelne kleine Integrationsaufgaben ohne strategische Bedeutung. Wenn kein SAP-Stack vorhanden (ISA-M ist SAP-zentriert)."
          ]
        },
        "en": {
          "intro": "The SAP Integration Solution Advisory Methodology (ISA-M) framework and associated BTP tool 'Integration Assessment' support systematic analysis of integration requirements and derivation of suitable integration technologies. It categorizes integration scenarios by use-case pattern (A2A, B2B, B2G, API) and recommends fitting SAP technologies.",
          "whyRing": "ADOPT: ISA-M is SAP's recommended approach for integration strategy and technology selection. The BTP tool creates transparency about the integration landscape and prevents arbitrary technology decisions.",
          "risks": [
            "ISA-M results are recommendations, not guarantees – critical review remains necessary. Without active data maintenance, the integration inventory becomes outdated quickly. ISA-M does not fully cover non-SAP technologies."
          ],
          "do": [
            "Run ISA-M assessment at the start of every major integration project. Maintain and update integration inventory in the BTP tool semi-annually. Use ISA-M results as the basis for Architecture Review Board decisions."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "Not for individual small integration tasks without strategic relevance. When no SAP stack is present (ISA-M is SAP-centric)."
          ]
        }
      }
    },
    {
      "label": "Postman/Bruno API test collections",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://docs.usebruno.com/"
      ],
      "references": [
        "https://learning.postman.com/docs/getting-started/introduction/",
        "https://docs.usebruno.com/",
        "https://www.npmjs.com/package/newman"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Postman und Bruno sind API-Testing-Werkzeuge, mit denen HTTP/REST- und OData-APIs explorativ getestet, automatisierte Tests geschrieben und in CI/CD-Pipelines integriert werden können. Bruno speichert Collections als Git-native JSON-Dateien (versionierbar). Im SAP-Kontext: Testen von S/4HANA OData-APIs, CPI-IFlow-Endpunkten, APIM-Proxies.",
          "whyRing": "ADOPT: Standard-Werkzeug in modernen Integrationsprojekten. Bruno als Open-Source-Alternative mit Git-nativer Speicherung ist für Teams mit Code-Review-Prozessen vorzuziehen. Postman für größere Teams mit Collaboration-Anforderungen weiterhin valide.",
          "risks": [
            "Postman-Collections mit gespeicherten Credentials in geteilten Workspaces sind Sicherheitsrisiko. Tests ohne Assertions prüfen nur Erreichbarkeit – kein fachlicher Mehrwert. Postman-Pricing-Änderungen (2023) haben einige Teams zu Bruno migriert."
          ],
          "do": [
            "Collections für alle produktiven APIs im Versionskontrollsystem halten (Git). Assertions für alle relevanten Response-Felder, HTTP-Status-Codes und Fehlerszenarien definieren. Collections in CI/CD-Pipeline via Newman (Postman) oder Bruno CLI ausführen. Keine echten Produktiv-Credentials in Collections speichern – Umgebungsvariablen nutzen."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Keine Ausnahme – API-Testing ist Pflicht für alle produktiven Schnittstellen."
          ]
        },
        "en": {
          "intro": "Postman and Bruno are API testing tools for exploratory testing of HTTP/REST and OData APIs, writing automated tests, and integrating into CI/CD pipelines. Bruno stores collections as Git-native JSON files (versionable). In the SAP context: testing S/4HANA OData APIs, CPI IFlow endpoints, APIM proxies.",
          "whyRing": "ADOPT: Standard tool in modern integration projects. Bruno as an open-source alternative with Git-native storage is preferable for teams with code review processes. Postman remains valid for larger teams with collaboration requirements.",
          "risks": [
            "Postman collections with saved credentials in shared workspaces are a security risk. Tests without assertions only check reachability – no functional value. Postman pricing changes (2023) drove some teams to migrate to Bruno."
          ],
          "do": [
            "Keep collections for all production APIs in version control (Git). Define assertions for all relevant response fields, HTTP status codes, and error scenarios. Run collections in CI/CD pipeline via Newman (Postman) or Bruno CLI. Do not store real production credentials in collections – use environment variables."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "No exception – API testing is mandatory for all production interfaces."
          ]
        }
      }
    },
    {
      "label": "SAP Application Interface Framework (AIF)",
      "ring": "ASSESS",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/SAP_APPLICATION_INTERFACE_FRAMEWORK"
      ],
      "references": [
        "https://help.sap.com/docs/SAP_APPLICATION_INTERFACE_FRAMEWORK",
        "https://help.sap.com/docs/application-interface-framework"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Das SAP Application Interface Framework (AIF) ist ein ABAP-basiertes Framework zur Überwachung, Fehlerbehandlung und Wiederverarbeitung von IDoc-, BAPI- und RFC-basierten Integrationsnachrichten direkt im SAP-ABAP-Stack. Es bietet eine einheitliche Fehler-cockpit-Oberfläche für Endbenutzer und Fachbereiche ohne technisches SAP-Wissen.",
          "whyRing": "ASSESS: AIF ist in ECC/S/4HANA On-Premise-Landschaften mit hohem IDoc-Volumen nützlich. Im Kontext moderner BTP-Integrationen ist AIF eine Ergänzung, kein strategisches Tool. Relevanz sinkt mit Migration von IDocs zu OData/Events.",
          "risks": [
            "AIF bindet Monitoring und Fehlerbehandlung an den ABAP-Stack – nicht kompatibel mit Cloud-only-Szenarien. Wartungsaufwand steigt mit SAP-Upgrade-Zyklen. Keine Unterstützung für BTP/CPI-basierte Integrationen."
          ],
          "do": [
            "AIF nur in ECC/S/4HANA On-Premise für IDoc/BAPI-Monitoring einsetzen. Migrationspfad zu CPI-basiertem Monitoring in Roadmap einplanen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Für alle Cloud/BTP-Integrationsszenarien. Wenn Migration von IDocs zu OData/Events bereits begonnen."
          ]
        },
        "en": {
          "intro": "The SAP Application Interface Framework (AIF) is an ABAP-based framework for monitoring, error handling, and reprocessing of IDoc, BAPI, and RFC-based integration messages directly in the SAP ABAP stack. It provides a unified error cockpit for end users and business departments without requiring technical SAP knowledge.",
          "whyRing": "ASSESS: AIF is useful in ECC/S/4HANA on-premise landscapes with high IDoc volume. In the context of modern BTP integrations, AIF is a complement, not a strategic tool. Relevance declines as IDocs migrate to OData/Events.",
          "risks": [
            "AIF binds monitoring and error handling to the ABAP stack – not compatible with cloud-only scenarios. Maintenance effort increases with SAP upgrade cycles. No support for BTP/CPI-based integrations."
          ],
          "do": [
            "Use AIF only in ECC/S/4HANA on-premise for IDoc/BAPI monitoring. Include migration path to CPI-based monitoring in the roadmap."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "For all cloud/BTP integration scenarios. When migration of IDocs to OData/Events has already begun."
          ]
        }
      }
    },
    {
      "label": "Manual prod changes",
      "ring": "HOLD",
      "quadrant": "Tooling & Ops",
      "sources": [
        "https://help.sap.com/docs/transport-management-service"
      ],
      "references": [
        "https://help.sap.com/docs/transport-management-service",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/content-transport"
      ],
      "ringChangeSuggestion": "HOLD",
      "changeRationale": "Compliance-Verstoß; Fehlende Revisionssicherheit und hohes Rollback-Risiko.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Manuelle Änderungen an produktiven Integrationsartefakten (IFlows, API-Proxies, Konfigurationen) direkt im Produktiv-System ohne strukturierten Transport- und Genehmigungsprozess. Dies umfasst direktes Deployment über den CPI Design-Editor in Produktion, manuelle Konfigurationsänderungen im APIM-Produktiv-Gateway oder unkontrollierte Credential-Änderungen.",
          "whyRing": "HOLD ohne Ausnahme. Manuelle Produktiv-Änderungen sind ein fundamentales Risiko für Stabilität, Auditierbarkeit und Compliance. Jede Ausnahme benötigt dokumentierten Genehmigungsprozess und sofortige Post-Change-Dokumentation.",
          "risks": [
            "Keine Reproduzierbarkeit und kein Rollback möglich ohne Transport-Pipeline. SOX/ITGC-Compliance-Verstöße durch fehlenden Audit-Trail. Drift zwischen DEV/QA und PROD – Fehler treten erst in Produktion auf. Wissenssilos: nur eine Person kennt die Produktiv-Konfiguration."
          ],
          "do": [
            "Notfall-Prozess dokumentieren: wer darf im Notfall eingreifen, welche Genehmigung ist nötig, wie wird nachträglich dokumentiert. Jede Ausnahme im Change-Management-System (ServiceNow, Jira) erfassen."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Generell vermeiden. Nur als dokumentierter Notfall-Prozess."
          ]
        },
        "en": {
          "intro": "Manual changes to production integration artifacts (IFlows, API proxies, configurations) directly in the production system without a structured transport and approval process. This includes direct deployment via the CPI Design editor in production, manual configuration changes in the APIM production gateway, or uncontrolled credential changes.",
          "whyRing": "HOLD without exception. Manual production changes are a fundamental risk to stability, auditability, and compliance. Every exception requires a documented approval process and immediate post-change documentation.",
          "risks": [
            "No reproducibility or rollback possible without a transport pipeline. SOX/ITGC compliance violations through missing audit trail. Drift between DEV/QA and PROD – errors only emerge in production. Knowledge silos: only one person knows the production configuration."
          ],
          "do": [
            "Document emergency process: who may intervene in emergencies, what approval is required, how to document retroactively. Record every exception in the change management system (ServiceNow, Jira)."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses."
          ],
          "whenNotToUse": [
            "Avoid generally. Only as a documented emergency process."
          ]
        }
      }
    },
    {
      "label": "API-first",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://spec.openapis.org/oas/latest.html"
      ],
      "references": [
        "https://spec.openapis.org/oas/latest.html",
        "https://api.sap.com",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "API-first bedeutet: der API-Kontrakt (OpenAPI-Spezifikation) wird vor der Implementierung definiert und ist unveränderliche Grundlage für Producer und Consumer. Im SAP-Kontext: S/4HANA OData v4-APIs und BTP-CPI-Endpunkte werden als API-Produkte konzipiert, nicht als technische Integration-Pipelines. API-first ermöglicht parallele Entwicklung durch Consumer und Producer auf Basis von Mocks.",
          "whyRing": "ADOPT: API-first ist das Fundament moderner Integrationsarchitektur. Ohne definierten Kontrakt entstehen fragile Point-to-Point-Verbindungen und hohe Änderungskosten.",
          "risks": [
            "API-first erfordert Disziplin: Kontrakt-Änderungen müssen über Governance-Prozess laufen. Ohne Schema-Versionierung entstehen Breaking Changes ohne Vorwarnung. Mock-basierte Entwicklung kann von echtem System-Verhalten abweichen."
          ],
          "do": [
            "OpenAPI 3.x-Spec als erstes Artefakt vor jeder Implementierung erstellen. API-Design-Review durch Architecture Board vor Freigabe der Spec. Consumer-Mocks aus der Spec generieren (Prism, WireMock). API-Versioning-Strategie (/v1/, /v2/) vorab verbindlich festlegen."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Nicht für rein technische Interna (z. B. interne CPI-zu-CPI-Kommunikation via JMS) ohne externe Consumer."
          ]
        },
        "en": {
          "intro": "API-first means: the API contract (OpenAPI specification) is defined before implementation and serves as the immutable foundation for both producers and consumers. In the SAP context: S/4HANA OData v4 APIs and BTP CPI endpoints are conceived as API products, not as technical integration pipelines. API-first enables parallel development by consumers and producers based on mocks.",
          "whyRing": "ADOPT: API-first is the foundation of modern integration architecture. Without a defined contract, fragile point-to-point connections and high change costs emerge.",
          "risks": [
            "API-first requires discipline: contract changes must go through a governance process. Without schema versioning, breaking changes occur without warning. Mock-based development can diverge from actual system behavior."
          ],
          "do": [
            "Create OpenAPI 3.x spec as the first artifact before any implementation. API design review by the Architecture Board before approving the spec. Generate consumer mocks from the spec (Prism, WireMock). Define API versioning strategy (/v1/, /v2/) upfront as a binding standard."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "Not for purely technical internals (e.g., internal CPI-to-CPI communication via JMS) without external consumers."
          ]
        }
      }
    },
    {
      "label": "API-led connectivity (system/process/experience)",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://www.enterpriseintegrationpatterns.com"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management",
        "https://api.sap.com"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "API-led Connectivity (ursprünglich von MuleSoft geprägt) strukturiert APIs in drei Schichten: System APIs (direkte Backend-Anbindung, z. B. S/4HANA OData), Process APIs (Orchestrierung, Business-Logik-Komposition) und Experience APIs (consumer-spezifische Aggregation für Mobile, Portal, Partner). Das Schichtenmodell erhöht Wiederverwendbarkeit und reduziert n×m-Integrationsbeziehungen. Im SAP/CPI-Kontext als Architekturleitprinzip anwendbar.",
          "whyRing": "ADOPT als Architekturprinzip – technologieunabhängig umsetzbar mit CPI, APIM, Kyma. Das Schichtenmodell ist praxisbewährt und reduziert Integrationskosten signifikant.",
          "risks": [
            "Zu starres Schichtenmodell kann Over-Engineering erzeugen für einfache A2A-Szenarien. Process-API-Schicht kann zum Engpass werden wenn nicht skalierbar. Ohne klare Ownership pro API-Schicht entstehen Governance-Lücken."
          ],
          "do": [
            "System APIs nah an der Backend-Quelle halten (S/4HANA OData v4 direkt). Process APIs in CPI oder Kyma orchestrieren. Experience APIs im APIM-Gateway als API-Produkte veröffentlichen. Ownership je Schicht klar definieren (Team-Verantwortung)."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Für einfache 1:1-A2A-Szenarien ohne Wiederverwendungsanforderung – Overengineering. Wenn Team-Kapazität für 3-Schicht-Governance nicht vorhanden."
          ]
        },
        "en": {
          "intro": "API-led Connectivity (originally coined by MuleSoft) structures APIs into three layers: System APIs (direct backend connectivity, e.g., S/4HANA OData), Process APIs (orchestration, business logic composition), and Experience APIs (consumer-specific aggregation for mobile, portal, partner). The layered model increases reusability and reduces n×m integration relationships. Applicable as an architecture guiding principle in SAP/CPI contexts.",
          "whyRing": "ADOPT as an architecture principle – implementable technology-agnostically with CPI, APIM, Kyma. The layered model is proven in practice and significantly reduces integration costs.",
          "risks": [
            "Overly rigid layered model can cause over-engineering for simple A2A scenarios. Process API layer can become a bottleneck if not scalable. Without clear ownership per API layer, governance gaps emerge."
          ],
          "do": [
            "Keep system APIs close to the backend source (S/4HANA OData v4 directly). Orchestrate process APIs in CPI or Kyma. Publish experience APIs in APIM gateway as API products. Define ownership per layer clearly (team responsibility)."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "For simple 1:1 A2A scenarios without reuse requirements – overengineering. When team capacity for 3-layer governance is not available."
          ]
        }
      }
    },
    {
      "label": "Contract-first with OpenAPI",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://spec.openapis.org/oas/latest.html"
      ],
      "references": [
        "https://spec.openapis.org/oas/latest.html",
        "https://api.sap.com",
        "https://stoplight.io/open-source/spectral",
        "https://www.rfc-editor.org/rfc/rfc7807"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Contract-first mit OpenAPI 3.x bedeutet: die REST-API-Spezifikation ist das primäre Entwurfsartefakt, aus dem Code (Stubs, Mocks, Client-SDKs) generiert wird – nicht umgekehrt. OpenAPI 3.x ist der Industriestandard für REST-API-Beschreibung, unterstützt von SAP API Business Hub, APIM und allen gängigen API-Test-Tools.",
          "whyRing": "ADOPT: OpenAPI 3.x ist der de-facto Standard. SAP publiziert alle S/4HANA-APIs im Business Accelerator Hub als OpenAPI-Specs. Keine Alternative für REST-APIs akzeptabel.",
          "risks": [
            "Spec-Drift: Implementierung weicht von Spec ab ohne Erkennung durch automatische Validierung. Veraltete Specs ohne Versionierung führen zu Konsumenten-Verwirrung. Schema-Qualität: oberflächliche Specs ohne Beispiele, Fehlermodelle und description-Felder bieten keinen echten Mehrwert."
          ],
          "do": [
            "OpenAPI 3.1 als Zielversion verwenden (JSON Schema full support). Spec versionieren und in Git versionskontrolliert halten. Spectral-Linter für API-Design-Guidelines als Pflicht-CI-Schritt einbinden. Fehlermodell (RFC 7807 Problem Details) in allen Fehlerfällen der Spec dokumentieren."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Nicht für Event-basierte APIs → AsyncAPI nutzen. Nicht für ABAP-RFC/IDoc-Schnittstellen (kein REST-Kontext)."
          ]
        },
        "en": {
          "intro": "Contract-first with OpenAPI 3.x means: the REST API specification is the primary design artifact from which code (stubs, mocks, client SDKs) is generated – not the reverse. OpenAPI 3.x is the industry standard for REST API description, supported by the SAP API Business Hub, APIM, and all common API test tools.",
          "whyRing": "ADOPT: OpenAPI 3.x is the de-facto standard. SAP publishes all S/4HANA APIs in the Business Accelerator Hub as OpenAPI specs. No alternative for REST APIs is acceptable.",
          "risks": [
            "Spec drift: implementation diverges from spec without detection via automated validation. Outdated specs without versioning lead to consumer confusion. Schema quality: superficial specs without examples, error models, and description fields offer no real value."
          ],
          "do": [
            "Use OpenAPI 3.1 as the target version (full JSON Schema support). Version the spec and keep it in Git version control. Include Spectral linter for API design guidelines as a mandatory CI step. Document the error model (RFC 7807 Problem Details) for all error cases in the spec."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "Not for event-based APIs → use AsyncAPI. Not for ABAP RFC/IDoc interfaces (no REST context)."
          ]
        }
      }
    },
    {
      "label": "AsyncAPI contract-first",
      "ring": "TRIAL",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://www.asyncapi.com/docs"
      ],
      "references": [
        "https://www.asyncapi.com/docs",
        "https://github.com/asyncapi/spec",
        "https://studio.asyncapi.com/",
        "https://help.sap.com/docs/event-mesh"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "AsyncAPI ist der Industriestandard für die Beschreibung von Event- und Message-APIs (Publish/Subscribe, Queues, Streams). Er ist das asynchrone Äquivalent zu OpenAPI und beschreibt Channels, Messages, Schemas und Server-Verbindungen. Im SAP-Kontext: Spezifikation von S/4HANA Business Events, Event-Mesh-Topics und CPI-Async-Integrationen. AsyncAPI 3.0 (2023 released) bringt deutliche Verbesserungen gegenüber 2.x.",
          "whyRing": "TRIAL korrekt: AsyncAPI 3.x ist technisch reif, aber im SAP-Ökosystem noch wenig verbreitet. Jetzt einführen bevor Event-Proliferation ohne Schema-Governance entsteht. Mittelfristig ADOPT-Kandidat.",
          "risks": [
            "Tooling für AsyncAPI weniger reif als für OpenAPI (weniger Code-Generatoren, Linter). Migration von AsyncAPI 2.x auf 3.x ist aufwändig (Breaking Changes in Spec-Struktur). Ohne AsyncAPI-Governance-Prozess entstehen inkompatible Event-Schemas zwischen Teams."
          ],
          "do": [
            "Für jeden Event-Topic/Channel eine AsyncAPI-Spec erstellen und in Git versionieren. AsyncAPI Studio oder AsyncAPI CLI für Spec-Validierung nutzen. S/4HANA Business Events mit AsyncAPI beschreiben und in Developer Portal publizieren. Schema-Kompatibilitätsregeln (Backward, Forward, Full) pro Topic definieren."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen.",
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Für synchrone REST-APIs → OpenAPI. Wenn noch keine Event-basierte Architektur im Einsatz."
          ]
        },
        "en": {
          "intro": "AsyncAPI is the industry standard for describing event and message APIs (publish/subscribe, queues, streams). It is the asynchronous equivalent of OpenAPI and describes channels, messages, schemas, and server connections. In the SAP context: specification of S/4HANA Business Events, Event Mesh topics, and CPI async integrations. AsyncAPI 3.0 (released 2023) brings significant improvements over 2.x.",
          "whyRing": "TRIAL is correct: AsyncAPI 3.x is technically mature but still uncommon in the SAP ecosystem. Introduce now before event proliferation without schema governance occurs. Medium-term ADOPT candidate.",
          "risks": [
            "Tooling for AsyncAPI is less mature than for OpenAPI (fewer code generators, linters). Migration from AsyncAPI 2.x to 3.x is effort-intensive (breaking changes in spec structure). Without AsyncAPI governance process, incompatible event schemas emerge between teams."
          ],
          "do": [
            "Create an AsyncAPI spec for every event topic/channel and version it in Git. Use AsyncAPI Studio or AsyncAPI CLI for spec validation. Describe S/4HANA Business Events with AsyncAPI and publish in developer portal. Define schema compatibility rules (Backward, Forward, Full) per topic."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency.",
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "For synchronous REST APIs → OpenAPI. When no event-based architecture is yet in use."
          ]
        }
      }
    },
    {
      "label": "Event-driven integration",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://help.sap.com/docs/s4hana-cloud/sap-s-4hana-cloud/business-events"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/event-driven-architecture",
        "https://help.sap.com/docs/s4hana-cloud/sap-s-4hana-cloud/business-events",
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/",
        "https://cloudevents.io/"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "S/4HANA Business Events sind in Cloud-Releases stabil und breit verfügbar. EDA ist strategische SAP-Architekturrichtung. Empfehle ADOPT für Teams mit etablierter EDA-Governance; TRIAL für Teams ohne Schema-Registry und DLQ-Strategie. Confidence: High.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Event-driven Integration (EDA) ersetzt synchrone Punkt-zu-Punkt-Aufrufe durch asynchrone Ereignisnachrichten: Producer publizieren ein Ereignis (z. B. 'SalesOrder.Created') in einen Broker (SAP Event Mesh, AEM, Kafka), Consumer reagieren unabhängig und zeitversetzt. S/4HANA Cloud emittiert native Business Events für über 100 Objekte. EDA entkoppelt Systeme strukturell und erhöht Resilienz bei Teilausfällen.",
          "whyRing": "TRIAL korrekt: EDA ist strategische Architekturrichtung für lose gekoppelte Landschaften, erfordert aber organisatorische Reife (Schema-Governance, Idempotenz-Konzepte, DLQ-Strategien). Technologie ist stabil; Herausforderung liegt in Betrieb und Governance.",
          "risks": [
            "Eventual Consistency schwer vermittelbar und in Fachprozessen nicht immer akzeptabel. Fehlende Idempotenz-Implementierung führt zu duplizierten Geschäftsvorfällen. Event-Proliferation ohne Schema-Governance erzeugt inkompatible Formate. Debugging verteilter asynchroner Flows erheblich komplexer als synchrone APIs."
          ],
          "do": [
            "EDA nur dort einsetzen wo asynchrone Verarbeitung fachlich akzeptabel ist. Für jeden Event-Typ: Schema (AsyncAPI), Owner, Consumer-Liste und Idempotenz-Schlüssel definieren. CloudEvents-Envelope als Standard-Hüllformat verwenden. DLQ + Reprocessing-Runbook vor Go-live verpflichtend. S/4HANA Business Events über BTP Event-Enablement konfigurieren (nicht RFC-Polling)."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Wenn synchrone Antwort zwingend erforderlich (z. B. Echtzeit-Verfügbarkeitsabfrage). Wenn Team noch kein Idempotenz- und DLQ-Konzept implementiert hat – erst Grundlagen schaffen."
          ]
        },
        "en": {
          "intro": "Event-driven Integration (EDA) replaces synchronous point-to-point calls with asynchronous event messages: producers publish an event (e.g., 'SalesOrder.Created') to a broker (SAP Event Mesh, AEM, Kafka), consumers react independently and asynchronously. S/4HANA Cloud emits native Business Events for over 100 objects. EDA structurally decouples systems and increases resilience during partial outages.",
          "whyRing": "TRIAL is correct: EDA is the strategic architecture direction for loosely coupled landscapes but requires organizational maturity (schema governance, idempotency concepts, DLQ strategies). Technology is stable; the challenge lies in operations and governance.",
          "risks": [
            "Eventual consistency hard to communicate and not always acceptable in business processes. Missing idempotency implementation leads to duplicated business transactions. Event proliferation without schema governance creates incompatible formats. Debugging distributed asynchronous flows significantly more complex than synchronous APIs."
          ],
          "do": [
            "Use EDA only where asynchronous processing is business-acceptable. For each event type: define schema (AsyncAPI), owner, consumer list, and idempotency key. Use CloudEvents envelope as standard wrapper format. DLQ + reprocessing runbook mandatory before go-live. Configure S/4HANA Business Events via BTP Event-Enablement (not RFC polling)."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "When synchronous response is mandatory (e.g., real-time availability query). When the team has not yet implemented idempotency and DLQ concepts – establish fundamentals first."
          ]
        }
      }
    },
    {
      "label": "Side-by-side extensibility",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://help.sap.com/docs/abap-cloud/abap-extensibility/side-by-side-extensibility"
      ],
      "references": [
        "https://help.sap.com/docs/btp/sap-business-technology-platform/extending-sap-solutions-using-automated-configurations",
        "https://help.sap.com/docs/abap-cloud/abap-extensibility/side-by-side-extensibility",
        "https://api.sap.com",
        "https://help.sap.com/docs/btp/sap-business-technology-platform/kyma-environment"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Side-by-side Extensibility ist das SAP-strategische Erweiterungsmodell: Custom-Anwendungen und -Logik werden außerhalb des SAP-Kernsystems (S/4HANA) auf BTP entwickelt und über stabile APIs (OData v4, Events) angebunden. Das Kernsystem bleibt 'clean core' – kein ABAP-Inplace-Modifikationen. Erweiterungen laufen auf Kyma, BTP Foundry oder SAP Build Apps.",
          "whyRing": "ADOPT: Clean-Core-Strategie ist SAP-Pflichtrichtung für S/4HANA Cloud. Side-by-side ist das technische Umsetzungsmodell dafür. Jede ABAP-Inplace-Erweiterung, die als Side-by-side umsetzbar ist, muss dort realisiert werden.",
          "risks": [
            "Latenz bei synchronen API-Calls zwischen BTP-Extension und S/4HANA-Kern – Netzwerkhops über SAP Cloud Connector erhöhen Response-Time. Erweiterungen können komplexer sein als ABAP-Äquivalente (mehr Technologien, mehr Teams). OData-v4-API-Coverage von S/4HANA noch nicht vollständig – manche Objekte nur über ältere APIs."
          ],
          "do": [
            "Clean Core als nicht verhandelbare Leitlinie etablieren: jede neue Anforderung muss zuerst auf Side-by-side-Umsetzbarkeit geprüft werden. Kyma oder BTP Foundry als Laufzeit für Custom Extensions wählen. Stabile OData v4-APIs aus dem SAP Business Accelerator Hub als Integration Foundation nutzen. Event-basierte Trigger (Business Events) statt Polling-basierter Anbindung bevorzugen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Latenzanforderungen durch Netzwerkhop nicht erfüllbar und ABAP-BADI die einzige Option. Wenn Objekt kein stabiles OData v4-API hat (→ SAP-Roadmap prüfen und eskalieren)."
          ]
        },
        "en": {
          "intro": "Side-by-side extensibility is SAP's strategic extension model: custom applications and logic are developed outside the SAP core system (S/4HANA) on BTP and connected via stable APIs (OData v4, Events). The core system stays 'clean core' – no ABAP in-place modifications. Extensions run on Kyma, BTP Foundry, or SAP Build Apps.",
          "whyRing": "ADOPT: Clean core strategy is SAP's mandatory direction for S/4HANA Cloud. Side-by-side is the technical implementation model for this. Every ABAP in-place extension that can be implemented as side-by-side must be realized there.",
          "risks": [
            "Latency in synchronous API calls between BTP extension and S/4HANA core – network hops via SAP Cloud Connector increase response time. Extensions can be more complex than ABAP equivalents (more technologies, more teams). OData v4 API coverage of S/4HANA is not yet complete – some objects only via older APIs."
          ],
          "do": [
            "Establish clean core as a non-negotiable guiding principle: every new requirement must first be assessed for side-by-side feasibility. Choose Kyma or BTP Foundry as runtime for custom extensions. Use stable OData v4 APIs from the SAP Business Accelerator Hub as integration foundation. Prefer event-based triggers (Business Events) over polling-based connectivity."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When latency requirements cannot be met via network hop and ABAP BADI is the only option. When the object has no stable OData v4 API (→ check SAP roadmap and escalate)."
          ]
        }
      }
    },
    {
      "label": "Circuit breaker",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://martinfowler.com/bliki/CircuitBreaker.html"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/",
        "https://martinfowler.com/bliki/CircuitBreaker.html",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/define-error-handling",
        "https://resilience4j.readme.io/docs/circuitbreaker"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Der Circuit Breaker verhindert kaskadierende Ausfälle in verteilten Integrationslandschaften: Wenn ein nachgelagertes System (z. B. externes ERP, Dritt-API) wiederholt fehlschlägt, öffnet der Circuit Breaker und blockiert weitere Anfragen für eine konfigurierbare Zeit (Open-Zustand), bevor er halb-offene Testanfragen zulässt. In CPI kann dies über Groovy-Skripte, JMS-Queue-Decoupling oder externe Resilience4j-Bibliotheken implementiert werden.",
          "whyRing": "ADOPT: Pflicht-Pattern für alle synchronen Integrationen gegen externe oder instabile Systeme. Ohne Circuit Breaker erzeugt ein einziger nachgelagerter Ausfall Thread-Pool-Erschöpfung und Kaskadenausfälle im gesamten Integrationssystem.",
          "risks": [
            "Ohne Circuit Breaker: ein ausgefallenes Drittsystem lähmt alle abhängigen IFlows. Falsch konfigurierte Timeouts (zu lang) maskieren Circuit-Breaker-Sinn. Fehlende Monitoring-Integration: Circuit-Breaker-Zustand nicht in Alerting sichtbar."
          ],
          "do": [
            "Timeout + Retry-Limits in jedem synchronen CPI-Adapter konfigurieren (HTTP-Adapter: Connection Timeout, Response Timeout). JMS-Queue als asynchroner Entkopplungspuffer vor instabilen Downstream-Systemen nutzen. Circuit-Breaker-Zustand als Custom-Alert in SAP Cloud ALM oder Monitoring-Dashboard aufnehmen. Fallback-Antwort definieren: Was soll im Open-Zustand zurückgegeben werden?"
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Bei idempotenten, rein asynchronen Integrationen mit DLQ – der Broker übernimmt die Entkopplung. Nicht als Ersatz für SLA-Vereinbarungen mit Downstream-Systemen."
          ]
        },
        "en": {
          "intro": "The Circuit Breaker prevents cascading failures in distributed integration landscapes: when a downstream system (e.g., external ERP, third-party API) repeatedly fails, the circuit breaker opens and blocks further requests for a configurable period (open state), before allowing half-open test requests. In CPI this can be implemented via Groovy scripts, JMS queue decoupling, or external Resilience4j libraries.",
          "whyRing": "ADOPT: Mandatory pattern for all synchronous integrations against external or unstable systems. Without a circuit breaker, a single downstream outage causes thread pool exhaustion and cascading failures across the entire integration system.",
          "risks": [
            "Without circuit breaker: one failed third-party system cripples all dependent IFlows. Incorrectly configured timeouts (too long) undermine the circuit breaker's purpose. Missing monitoring integration: circuit breaker state not visible in alerting."
          ],
          "do": [
            "Configure timeout + retry limits in every synchronous CPI adapter (HTTP adapter: connection timeout, response timeout). Use JMS queue as an asynchronous decoupling buffer before unstable downstream systems. Include circuit breaker state as a custom alert in SAP Cloud ALM or monitoring dashboard. Define fallback response: what should be returned in the open state?"
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "For idempotent, purely asynchronous integrations with DLQ – the broker handles decoupling. Not as a substitute for SLA agreements with downstream systems."
          ]
        }
      }
    },
    {
      "label": "Idempotent consumer pattern",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/idempotent-process-call"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/IdempotentReceiver.html",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/idempotent-process-call",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/define-idempotent-process-call"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Der Idempotent Consumer stellt sicher, dass eine Nachricht, die mehrfach zugestellt wird (At-least-once-Semantik in Event Mesh, JMS), denselben Effekt hat wie eine einmalige Zustellung. Implementiert über einen Idempotenz-Schlüssel (Message-ID, Business-Key), der in einem persistenten Store (DB, CPI-Idempotency-Repository, Redis) als 'bereits verarbeitet' markiert wird. Ohne Idempotenz erzeugt Retry-Logik doppelte Buchungen, Bestellungen oder Statusänderungen.",
          "whyRing": "ADOPT ohne Ausnahme: Pflicht-Pattern für jeden asynchronen Consumer und für alle synchronen Operationen die nicht natürlich idempotent sind (PUT/DELETE idempotent, POST nicht). Fehlende Idempotenz ist der häufigste Produktionsfehler in EDA-Implementierungen.",
          "risks": [
            "Idempotenz-Store-Wachstum ohne Retention-Policy füllt Speicher. Race Conditions bei parallelen Nachrichten mit gleichem Key erfordern Locking-Mechanismus. Business-Key-Wahl falsch: zu generisch (Kollisionen) oder zu spezifisch (doppelte Verarbeitung nicht erkannt)."
          ],
          "do": [
            "Idempotenz-Schlüssel immer aus unveränderlichen Business-Feldern ableiten (Bestell-ID + Positions-ID, nicht Timestamp). CPI Idempotent Process Call oder externe Redis/DB-basierte Lösung für Duplicate-Detection nutzen. Retention-Policy für Idempotenz-Store definieren (z. B. 30 Tage). Idempotenz-Verletzungen im Monitoring sichtbar machen (Alert bei Duplicate-Erkennung)."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Operationen von Natur aus idempotent sind (GET, PUT mit vollständiger Ressource). Bei exakter Einmal-Zustellung (Exactly-once – nur in Kafka Transactions / SAP AEM möglich)."
          ]
        },
        "en": {
          "intro": "The Idempotent Consumer ensures that a message delivered multiple times (at-least-once semantics in Event Mesh, JMS) has the same effect as a single delivery. Implemented via an idempotency key (message ID, business key) marked as 'already processed' in a persistent store (DB, CPI Idempotency Repository, Redis). Without idempotency, retry logic creates duplicate bookings, orders, or status changes.",
          "whyRing": "ADOPT without exception: mandatory pattern for every asynchronous consumer and for all synchronous operations that are not naturally idempotent (PUT/DELETE idempotent, POST not). Missing idempotency is the most common production failure in EDA implementations.",
          "risks": [
            "Idempotency store growth without retention policy fills storage. Race conditions with parallel messages sharing the same key require a locking mechanism. Wrong business key choice: too generic (collisions) or too specific (duplicate processing not detected)."
          ],
          "do": [
            "Always derive idempotency key from immutable business fields (order ID + line item ID, not timestamp). Use CPI Idempotent Process Call or external Redis/DB-based solution for duplicate detection. Define retention policy for idempotency store (e.g., 30 days). Make idempotency violations visible in monitoring (alert on duplicate detection)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When operations are naturally idempotent (GET, PUT with complete resource). With exactly-once delivery (only available in Kafka Transactions / SAP AEM)."
          ]
        }
      }
    },
    {
      "label": "Retry/backoff + dead-letter handling",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/DeadLetterChannel.html"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/DeadLetterChannel.html",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/define-error-handling",
        "https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Retry mit exponentiellem Backoff und Dead Letter Queue (DLQ) ist das Resilience-Grundmuster für asynchrone Integrationen: Transiente Fehler (Netzwerk-Timeout, temporäre Systemunavailability) werden durch Retries mit wachsenden Wartezeiten (1s → 2s → 4s → ..., max. N Versuche) überbrückt. Dauerhaft nicht verarbeitbare Nachrichten landen in der DLQ, wo sie manuell oder automatisiert repariert und neu eingespielt werden können.",
          "whyRing": "ADOPT ohne Ausnahme: jede asynchrone Integration ohne Retry/DLQ-Konzept verliert Nachrichten bei Fehlern. Dieses Pattern ist die Mindestanforderung an betriebstaugliche EDA.",
          "risks": [
            "Retry-Sturm: gleichzeitige Retries vieler Consumer überlasten bereits gestresstes System. DLQ ohne Monitoring und Owner ist ein stilles Datenleck – Nachrichten verschwinden unbemerkt. Fehlende Reprocessing-Runbooks: DLQ-Nachrichten bleiben liegen weil niemand weiß wie vorzugehen."
          ],
          "do": [
            "Exponentiellen Backoff mit Jitter implementieren (verhindert Retry-Stürme). Max. Retry-Anzahl konfigurieren (3–5 Versuche typisch). DLQ für jeden Consumer-Topic definieren und in SAP Cloud ALM alerten. Reprocessing-Runbook dokumentieren: wer, wie, wann DLQ-Nachrichten repariert. DLQ-Nachrichten mit Original-Kontext (Timestamp, Fehlergrund, Retry-Anzahl) anreichern."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Nicht für Szenarien mit Exactly-once-Anforderung ohne entsprechenden Broker-Support. Bei synchronen REST-APIs: Retry nur für idempotente Methoden (GET, PUT)."
          ]
        },
        "en": {
          "intro": "Retry with exponential backoff and Dead Letter Queue (DLQ) is the foundational resilience pattern for asynchronous integrations: transient errors (network timeout, temporary system unavailability) are bridged by retries with increasing wait times (1s → 2s → 4s → ..., max. N attempts). Persistently unprocessable messages land in the DLQ, where they can be repaired and reinjected manually or automatically.",
          "whyRing": "ADOPT without exception: every asynchronous integration without a retry/DLQ concept loses messages on errors. This pattern is the minimum requirement for operationally sound EDA.",
          "risks": [
            "Retry storm: simultaneous retries from many consumers overwhelm an already stressed system. DLQ without monitoring and owner is a silent data leak – messages disappear unnoticed. Missing reprocessing runbooks: DLQ messages remain stranded because no one knows the procedure."
          ],
          "do": [
            "Implement exponential backoff with jitter (prevents retry storms). Configure max retry count (3–5 attempts typical). Define DLQ for each consumer topic and alert in SAP Cloud ALM. Document reprocessing runbook: who, how, when to repair DLQ messages. Enrich DLQ messages with original context (timestamp, error reason, retry count)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "Not for exactly-once scenarios without corresponding broker support. For synchronous REST APIs: retry only for idempotent methods (GET, PUT)."
          ]
        }
      }
    },
    {
      "label": "Outbox pattern",
      "ring": "TRIAL",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://cap.cloud.sap/docs/node.js/outbox"
      ],
      "references": [
        "https://microservices.io/patterns/data/transactional-outbox.html",
        "https://cap.cloud.sap/docs/node.js/outbox",
        "https://cap.cloud.sap/docs/guides/messaging/"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Das Transactional Outbox Pattern löst das Dual-Write-Problem: Wenn eine Anwendung gleichzeitig in die DB schreiben UND ein Event publizieren muss, können beide Operationen nicht atomar erfolgen. Die Outbox-Tabelle wird in derselben DB-Transaktion wie die Hauptoperation geschrieben; ein separater Outbox-Processor (Change Data Capture oder Polling) liest die Outbox und publiziert Events in den Broker. Im SAP/BTP-Kontext relevant für CAP-Anwendungen mit Custom Events.",
          "whyRing": "TRIAL korrekt: Das Pattern ist konzeptionell etabliert, aber in SAP CAP/BTP-Umgebungen noch nicht standardmäßig durch SAP-Tools abgedeckt. CAP bietet seit neueren Versionen rudimentäre Outbox-Unterstützung. Für Custom-Implementierungen Evaluierung empfohlen.",
          "risks": [
            "Outbox-Processor als zusätzliche Komponente erhöht Betriebskomplexität. CDC (Debezium, SAP HANA CDC) erfordert DB-Berechtigungen und spezifisches Setup. Polling-basierter Outbox-Processor erzeugt Latenz und DB-Last."
          ],
          "do": [
            "CAP-native Outbox-Support (cds.outboxed) nutzen wenn verfügbar. Outbox-Tabelle mit Status (PENDING, SENT, FAILED) und Retry-Counter designen. Outbox-Processor in Kyma oder BTP Job als separater Service. Retention-Policy für gesendete Outbox-Einträge definieren."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn der Event-Broker Exactly-once-Transaktionen mit DB-Integration unterstützt (Kafka Connect + Transactional Outbox Plugin). Wenn System kein Transaktions-DB hat (reine API-Integration ohne Persistenz)."
          ]
        },
        "en": {
          "intro": "The Transactional Outbox Pattern solves the dual-write problem: when an application must simultaneously write to the DB AND publish an event, both operations cannot be atomic. An outbox table is written in the same DB transaction as the main operation; a separate outbox processor (Change Data Capture or polling) reads the outbox and publishes events to the broker. Relevant in SAP/BTP context for CAP applications with custom events.",
          "whyRing": "TRIAL is correct: the pattern is conceptually established but not yet covered by SAP tools as a standard in SAP CAP/BTP environments. CAP offers rudimentary outbox support in recent versions. Evaluation recommended for custom implementations.",
          "risks": [
            "Outbox processor as an additional component increases operational complexity. CDC (Debezium, SAP HANA CDC) requires DB permissions and specific setup. Polling-based outbox processor creates latency and DB load."
          ],
          "do": [
            "Use CAP-native outbox support (cds.outboxed) when available. Design outbox table with status (PENDING, SENT, FAILED) and retry counter. Outbox processor in Kyma or BTP Job as a separate service. Define retention policy for sent outbox entries."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When the event broker supports exactly-once transactions with DB integration (Kafka Connect + Transactional Outbox Plugin). When system has no transactional DB (pure API integration without persistence)."
          ]
        }
      }
    },
    {
      "label": "Strangler pattern for legacy replacement",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-assessment"
      ],
      "references": [
        "https://martinfowler.com/bliki/StranglerFigApplication.html",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-from-sap-process-integration-and-process-orchestration",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-assessment"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Das Strangler Fig Pattern ermöglicht inkrementelle Migration von Legacy-Integrationssystemen (SAP PI/PO, proprietäre Middleware) zu modernen Plattformen (CPI), ohne Big-Bang-Ablösung. Neue Schnittstellen werden direkt in CPI implementiert; bestehende PI/PO-Interfaces bleiben aktiv und werden schrittweise – priorisiert nach Business Value und Risiko – migriert. PI/PO und CPI laufen parallel bis zur vollständigen Migration.",
          "whyRing": "ADOPT ohne Alternative: Big-Bang-Migrationen von PI/PO zu CPI scheitern regelmäßig. Strangler ist der einzig vertretbare Ansatz für Produktivlandschaften mit 100+ Interfaces. Praxisbewährt in großen SAP-Migrations-Projekten.",
          "risks": [
            "Parallel-Betrieb von PI/PO und CPI erhöht Betriebskosten (zwei Plattformen). Migrations-Backlog ohne Priorisierung und Kapazitätsplanung wächst unkontrolliert. Doppelte Monitoring-Infrastruktur erhöht Komplexität. PI/PO-EOM 2027 erzeugt Zeitdruck – Strangler-Tempo muss geplant sein."
          ],
          "do": [
            "SAP Migration Assessment Tool für vollständiges Interface-Inventar und Komplexitäts-Score nutzen. Priorisierung nach Matrix: Business-Impact × Migrations-Aufwand (Quick-Wins zuerst). Klare Migrations-KPIs definieren: Interfaces/Quartal, % migrated, Ziel-Datum PI/PO-Abschaltung. Neue Interfaces ausnahmslos in CPI – kein neues PI/PO-Interface mehr."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Legacy-System bereits abgeschaltet ist oder nur 1–3 Interfaces hat (→ direkte Migration effizienter). Nicht als Rechtfertigung für unbegrenzte PI/PO-Verlängerung."
          ]
        },
        "en": {
          "intro": "The Strangler Fig Pattern enables incremental migration of legacy integration systems (SAP PI/PO, proprietary middleware) to modern platforms (CPI) without big-bang replacement. New interfaces are implemented directly in CPI; existing PI/PO interfaces remain active and are migrated step-by-step – prioritized by business value and risk. PI/PO and CPI run in parallel until migration is complete.",
          "whyRing": "ADOPT without alternative: big-bang migrations from PI/PO to CPI regularly fail. Strangler is the only defensible approach for production landscapes with 100+ interfaces. Proven in practice in large SAP migration projects.",
          "risks": [
            "Parallel operation of PI/PO and CPI increases operating costs (two platforms). Migration backlog without prioritization and capacity planning grows uncontrolled. Dual monitoring infrastructure increases complexity. PI/PO EOM 2027 creates time pressure – strangler pace must be planned."
          ],
          "do": [
            "Use SAP Migration Assessment Tool for complete interface inventory and complexity scoring. Prioritize via matrix: business impact × migration effort (quick wins first). Define clear migration KPIs: interfaces/quarter, % migrated, target PI/PO shutdown date. All new interfaces exclusively in CPI – no new PI/PO interface ever again."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When legacy system is already decommissioned or has only 1–3 interfaces (→ direct migration more efficient). Not as justification for unlimited PI/PO extension."
          ]
        }
      }
    },
    {
      "label": "CloudEvents envelope",
      "ring": "ADOPT",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md"
      ],
      "references": [
        "https://cloudevents.io/",
        "https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md",
        "https://help.sap.com/docs/event-mesh",
        "https://www.cncf.io/projects/cloudevents/"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "CloudEvents 1.0 ist CNCF Graduated, von SAP nativ unterstützt und klar besser als proprietäre Formate. Empfehle ADOPT als verbindliches Event-Envelope-Format. Confidence: High.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "CloudEvents ist ein CNCF-Standard (v1.0, Graduated) zur einheitlichen Beschreibung von Ereignisdaten über Systemgrenzen hinweg. Das Envelope-Format definiert Pflichtattribute (id, source, specversion, type) und optionale Metadaten (datacontenttype, subject, time). SAP Event Mesh und S/4HANA Business Events unterstützen CloudEvents nativ. CloudEvents verhindert proprietäre Event-Formate und ermöglicht Broker-Interoperabilität.",
          "whyRing": "TRIAL mit klarer ADOPT-Empfehlung: CloudEvents 1.0 ist CNCF Graduated und in SAP-Produkten nativ unterstützt. Als Standard-Event-Hüllformat sollte es für alle neuen Events verpflichtend werden. Wechsel von TRIAL zu ADOPT sobald im Unternehmen als Standard eingeführt.",
          "risks": [
            "Event-Payload-Schema (im 'data'-Feld) ist nicht durch CloudEvents selbst standardisiert – AsyncAPI für Schema-Governance weiterhin notwendig. Legacy-Producer (ältere S/4HANA-Releases) erzeugen noch kein CloudEvents-Format – Adapter/Transformer in CPI notwendig."
          ],
          "do": [
            "CloudEvents 1.0-Format als verbindlichen Standard für alle neuen Events definieren. type-Attribut als sprechenden Bezeichner nutzen: 'com.sap.s4hana.businesspartner.v1.changed'. source-Attribut eindeutig und nachverfolgbar: 'urn:sap:s4hana:PROD:BP-module'. CloudEvents + AsyncAPI kombinieren: CloudEvents für Envelope, AsyncAPI für Schema-Dokumentation."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Wenn Broker kein CloudEvents unterstützt und Transformation zu aufwändig. Für Legacy-Producer die kein CloudEvents emittieren können – Adapter in CPI."
          ]
        },
        "en": {
          "intro": "CloudEvents is a CNCF standard (v1.0, Graduated) for uniformly describing event data across system boundaries. The envelope format defines mandatory attributes (id, source, specversion, type) and optional metadata (datacontenttype, subject, time). SAP Event Mesh and S/4HANA Business Events natively support CloudEvents. CloudEvents prevents proprietary event formats and enables broker interoperability.",
          "whyRing": "TRIAL with clear ADOPT recommendation: CloudEvents 1.0 is CNCF Graduated and natively supported in SAP products. As the standard event envelope format, it should become mandatory for all new events. Move from TRIAL to ADOPT once established as an enterprise standard.",
          "risks": [
            "Event payload schema (in the 'data' field) is not standardized by CloudEvents itself – AsyncAPI for schema governance is still necessary. Legacy producers (older S/4HANA releases) do not yet emit CloudEvents format – adapter/transformer in CPI required."
          ],
          "do": [
            "Define CloudEvents 1.0 format as the binding standard for all new events. Use type attribute as a meaningful identifier: 'com.sap.s4hana.businesspartner.v1.changed'. Source attribute unique and traceable: 'urn:sap:s4hana:PROD:BP-module'. Combine CloudEvents + AsyncAPI: CloudEvents for envelope, AsyncAPI for schema documentation."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "When broker does not support CloudEvents and transformation is too costly. For legacy producers that cannot emit CloudEvents – use adapter in CPI."
          ]
        }
      }
    },
    {
      "label": "Choreography over orchestration (selective)",
      "ring": "TRIAL",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://microservices.io/patterns/data/saga.html"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/",
        "https://microservices.io/patterns/data/saga.html",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/event-driven-architecture"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Choreographie-basierte Integration: Systeme reagieren eigenständig auf Events aus dem Broker, ohne zentralen Orchestrator (CPI-IFlow als Koordinator). Jeder Service kennt nur seinen eigenen Schritt und die Events, auf die er reagiert. Gegensatz zur Orchestrierung, bei der ein zentraler IFlow den gesamten Prozessablauf steuert. Choreographie eignet sich für lose gekoppelte, domänenübergreifende Szenarien mit klar definierten Events.",
          "whyRing": "TRIAL korrekt: Choreographie reduziert zentrale Kopplung und Single-Points-of-Failure, aber erhöht Debugging-Komplexität erheblich (verteilte Prozess-Sichtbarkeit fehlt). Selektiv einsetzen: nicht als Ablösung für alle Orchestrierungen.",
          "risks": [
            "Fehlende Prozess-Sichtbarkeit: Gesamtzustand eines verteilten Prozesses nicht direkt einsehbar. Choreographie-Schleifen (A triggert B triggert A) schwer zu erkennen und zu stoppen. Fehlende zentrale Fehlerbehandlung: jeder Consumer muss seine eigene DLQ-Strategie haben."
          ],
          "do": [
            "Choreographie nur für klar abgegrenzte, domänenübergreifende Szenarien nutzen (z. B. Order-to-Cash über Domänengrenzen hinweg). Process Mining oder Event-Tracing für Prozess-Sichtbarkeit einsetzen. Jeder choreographierte Service muss vollständig idempotent und mit DLQ ausgestattet sein. Klare Event-Taxonomie und -Ownership als Voraussetzung."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Für Prozesse mit strengen SLA-Anforderungen und Kompensationslogik (→ Saga mit Orchestrierung). Wenn zentrale Prozess-Sichtbarkeit für Compliance oder Audit erforderlich."
          ]
        },
        "en": {
          "intro": "Choreography-based integration: systems independently react to events from the broker, without a central orchestrator (CPI IFlow as coordinator). Each service knows only its own step and the events it reacts to. Contrast with orchestration, where a central IFlow controls the entire process flow. Choreography suits loosely coupled, cross-domain scenarios with well-defined events.",
          "whyRing": "TRIAL is correct: choreography reduces central coupling and single points of failure but significantly increases debugging complexity (distributed process visibility is lacking). Use selectively: not as a replacement for all orchestrations.",
          "risks": [
            "Missing process visibility: overall state of a distributed process not directly visible. Choreography loops (A triggers B triggers A) hard to detect and stop. Missing central error handling: every consumer must have its own DLQ strategy."
          ],
          "do": [
            "Use choreography only for clearly bounded, cross-domain scenarios (e.g., order-to-cash across domain boundaries). Use process mining or event tracing for process visibility. Each choreographed service must be fully idempotent and equipped with DLQ. Clear event taxonomy and ownership as prerequisite."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "For processes with strict SLA requirements and compensation logic (→ Saga with orchestration). When central process visibility is required for compliance or audit."
          ]
        }
      }
    },
    {
      "label": "Canonical model (lightweight)",
      "ring": "ASSESS",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/CanonicalDataModel.html"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/CanonicalDataModel.html",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/cloud-integration",
        "https://json-schema.org/"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Ein Canonical Data Model (CDM) definiert ein herstellerneutrales, intermediäres Datenmodell zwischen Quell- und Zielsystemen. Statt n×m direkten Mappings (jedes System zu jedem anderen) mappen alle Systeme auf das Canonical Model (n+n Mappings). 'Lightweight' betont pragmatisches Vorgehen: nur die tatsächlich gemeinsam genutzten Felder werden kanonisiert, nicht ein vollständiges Enterprise Data Dictionary.",
          "whyRing": "ASSESS korrekt: CDM ist wertvoll ab ~5+ Systemen mit überlappenden Datenobjekten (Business Partner, Product, Order). Initialer Aufwand hoch; Return on Investment erst nach mehreren Integrationen spürbar. Nicht übertreiben: lightweight ist Pflicht.",
          "risks": [
            "Over-Engineering: zu umfassendes CDM wird nie fertig und bremst Integrationsprojekte. CDM-Pflege ohne Owner-Struktur degeneriert schnell zu 'niemand ist zuständig'. Canonical-Mapping als Bottleneck wenn Modell nicht flexibel für Erweiterungen."
          ],
          "do": [
            "Zeitlich begrenzten PoC mit klarer Go/No-Go-Entscheidung durchführen.",
            "Klare Success-Kriterien und Betriebsmetriken vor dem Rollout festlegen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn weniger als 3–4 Systeme integriert werden. Wenn alle Systeme dasselbe Datenmodell nutzen (z. B. reine SAP-Landschaft mit OData v4)."
          ]
        },
        "en": {
          "intro": "A Canonical Data Model (CDM) defines a vendor-neutral, intermediate data model between source and target systems. Instead of n×m direct mappings (every system to every other), all systems map to the canonical model (n+n mappings). 'Lightweight' emphasizes a pragmatic approach: only genuinely shared fields are canonicalized, not a complete enterprise data dictionary.",
          "whyRing": "ASSESS is correct: CDM is valuable from ~5+ systems with overlapping data objects (business partner, product, order). Initial effort is high; return on investment only noticeable after several integrations. Do not over-engineer: lightweight is mandatory.",
          "risks": [
            "Falsch eingesetzte Architektur-Pattern erhöhen Komplexität ohne proportionalen Nutzen."
          ],
          "do": [
            "Zeitlich begrenzten PoC mit klarer Go/No-Go-Entscheidung durchführen.",
            "Define clear success criteria and operating metrics before rollout."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When fewer than 3–4 systems are integrated. When all systems use the same data model (e.g., pure SAP landscape with OData v4)."
          ]
        }
      }
    },
    {
      "label": "CQRS (selective)",
      "ring": "ASSESS",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://martinfowler.com/bliki/CQRS.html"
      ],
      "references": [
        "https://martinfowler.com/bliki/CQRS.html",
        "https://www.enterpriseintegrationpatterns.com",
        "https://help.sap.com/docs/hana-cloud"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "Command Query Responsibility Segregation (CQRS) trennt Lese- und Schreiboperationen auf getrennte Modelle/Datenspeicher. Im SAP-Integrationskontext: S/4HANA als Write-System (Transaktionen über OData v4), ein separates Read-Modell (SAP HANA Cloud, ElasticSearch) für komplexe Abfragen und Reporting. Relevant wenn Query-Last das transaktionale System belastet oder Aggregationen über mehrere SAP-Systeme erforderlich.",
          "whyRing": "ASSESS korrekt: CQRS ist mächtiges Pattern, erzeugt aber erhebliche Komplexität (Eventual Consistency zwischen Write- und Read-Modell, Synchronisierungs-Pipeline). Nur einsetzen wenn Leselast oder Abfragekomplexität nachweisbar das Schreibsystem belastet.",
          "risks": [
            "Eventual Consistency: Read-Modell hinkt Write-Modell nach (Sekunden bis Minuten). Synchronisierungs-Pipeline (CPI-Event-Flow oder CDC) als zusätzlicher Fehlerbereich. Entwicklungs- und Testkomplexität steigt erheblich."
          ],
          "do": [
            "Konkreten Leistungsnachweis führen bevor CQRS eingeführt wird (messbare Query-Last). Change Events (S/4HANA Business Events) als Synchronisierungstrigger nutzen. Read-Modell in SAP HANA Cloud oder Datenpipeline (BTP Data Intelligence) aufbauen. Klare Inkonsistenz-Fenster und deren Akzeptabilität mit Fachbereich abstimmen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Leselast das Schreibsystem nicht nachweisbar belastet. Wenn starke Konsistenz (Strong Consistency) zwischen Lesen und Schreiben erforderlich."
          ]
        },
        "en": {
          "intro": "Command Query Responsibility Segregation (CQRS) separates read and write operations onto separate models/data stores. In the SAP integration context: S/4HANA as write system (transactions via OData v4), a separate read model (SAP HANA Cloud, ElasticSearch) for complex queries and reporting. Relevant when query load burdens the transactional system or aggregations across multiple SAP systems are required.",
          "whyRing": "ASSESS is correct: CQRS is a powerful pattern but creates significant complexity (eventual consistency between write and read model, synchronization pipeline). Use only when read load or query complexity demonstrably burdens the write system.",
          "risks": [
            "Eventual consistency: read model lags behind write model (seconds to minutes). Synchronization pipeline (CPI event flow or CDC) as an additional failure domain. Development and test complexity increases significantly."
          ],
          "do": [
            "Provide concrete performance evidence before introducing CQRS (measurable query load). Use change events (S/4HANA Business Events) as synchronization triggers. Build read model in SAP HANA Cloud or data pipeline (BTP Data Intelligence). Agree on clear inconsistency windows and their acceptability with business stakeholders."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When read load does not demonstrably burden the write system. When strong consistency between reads and writes is required."
          ]
        }
      }
    },
    {
      "label": "Direct point-to-point connections",
      "ring": "HOLD",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://www.enterpriseintegrationpatterns.com"
      ],
      "references": [
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageBus.html",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Direkte Punkt-zu-Punkt-Verbindungen verbinden zwei Systeme ohne zwischengeschaltete Integrationsplattform: System A ruft direkt System B's API auf, ohne CPI-IFlow, ohne API-Gateway, ohne zentrale Logging-Schicht. Historisch üblich (ABAP-HTTP-Destination direkt auf externe REST-API), erzeugt aber unkontrollierbare n×m-Beziehungen und Betriebsblindheit.",
          "whyRing": "HOLD ohne Ausnahme für Neuvorhaben. Bestehende P2P-Verbindungen inventarisieren und mit Exit-Strategie versehen. Jede direkte Verbindung ist ein unkontrollierbares Governance- und Sicherheitsrisiko.",
          "risks": [
            "Keine zentrale Observability: Fehler in P2P-Verbindungen nicht in Monitoring sichtbar. Keine Credential-Governance: Secrets direkt in ABAP-RFC-Destinations oder Code. Keine Versionierung: Breaking Changes in Zielsystem brechen ohne Vorwarnung. Unkontrolliertes Wachstum: nach 50 Systemen niemand mehr den Überblick."
          ],
          "do": [
            "Alle bestehenden P2P-Verbindungen inventarisieren (Werkzeug: SAP LeanIX, eigenes Register). Migrationspfad zu CPI/APIM für jede Verbindung definieren. Ausnahmen (z. B. S/4HANA interne ABAP-Aufrufe) explizit dokumentieren und genehmigen."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generell für alle Neuvorhaben. Bestand aktiv abbauen."
          ]
        },
        "en": {
          "intro": "Direct point-to-point connections link two systems without an intermediate integration platform: System A calls System B's API directly, without a CPI IFlow, without an API gateway, without a central logging layer. Historically common (ABAP HTTP destination directly to external REST API), but creates uncontrollable n×m relationships and operational blindness.",
          "whyRing": "HOLD without exception for new initiatives. Inventory existing P2P connections and define exit strategies. Every direct connection is an uncontrollable governance and security risk.",
          "risks": [
            "No central observability: failures in P2P connections not visible in monitoring. No credential governance: secrets directly in ABAP RFC destinations or code. No versioning: breaking changes in target system break without warning. Uncontrolled growth: after 50 systems, nobody has an overview."
          ],
          "do": [
            "Inventory all existing P2P connections (tooling: SAP LeanIX, own register). Define migration path to CPI/APIM for each connection. Explicitly document and approve exceptions (e.g., S/4HANA internal ABAP calls)."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generally for all new initiatives. Actively reduce existing inventory."
          ]
        }
      }
    },
    {
      "label": "File-drop integrations (shared folder/manual polling)",
      "ring": "HOLD",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/sftp-adapter"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/sftp-adapter",
        "https://www.enterpriseintegrationpatterns.com/patterns/messaging/",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/cloud-integration"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "File-drop-Integrationen: System A legt Dateien (CSV, XML, IDoc-Flat) in einem Shared-Folder oder SFTP-Server ab; System B pollt regelmäßig und verarbeitet neue Dateien. Historisch weit verbreitet (SAP PI-File-Adapter), aber ohne Transaktionssicherheit, Zustellquittung oder Fehler-Rückmeldung. Typische Legacy-Praxis in B2B und On-Premise-Landschaften.",
          "whyRing": "HOLD für Neuvorhaben ohne Ausnahme. Bestehende File-drop-Integrationen auf SFTP/MFT oder direkte API-Integration migrieren. Wenn File-Transfer zwingend notwendig: Managed File Transfer (MFT) mit Quittierung und Audit-Trail als Ablösung.",
          "risks": [
            "Keine Transaktionssicherheit: Datei halb geschrieben, Consumer liest unvollständige Daten. Kein Fehler-Feedback: Producer weiß nicht ob Consumer Datei verarbeitet hat. Kein Audit-Trail: wer hat welche Datei wann abgelegt und verarbeitet? Sicherheitsrisiko: Shared Folder ohne Zugriffsrechtekontrolle."
          ],
          "do": [
            "Bestehende File-drop-Integrationen inventarisieren und priorisiert migrieren. Wenn File-Transfer unvermeidbar: GoAnywhere, Axway MFT oder CPI-SFTP-Adapter mit Quittierung und Audit-Log. Klare Datei-Namenskonvention und Archivierungsstrategie für Migration definieren."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses.",
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generell für alle Neuvorhaben. Bestand migrieren."
          ]
        },
        "en": {
          "intro": "File-drop integrations: System A places files (CSV, XML, IDoc-flat) in a shared folder or SFTP server; System B polls regularly and processes new files. Historically widespread (SAP PI file adapter), but without transactional integrity, delivery acknowledgment, or error feedback. Typical legacy practice in B2B and on-premise landscapes.",
          "whyRing": "HOLD for new initiatives without exception. Migrate existing file-drop integrations to SFTP/MFT or direct API integration. When file transfer is unavoidable: Managed File Transfer (MFT) with acknowledgment and audit trail as replacement.",
          "risks": [
            "No transactional integrity: file half-written, consumer reads incomplete data. No error feedback: producer does not know if consumer processed the file. No audit trail: who placed which file when and when was it processed? Security risk: shared folder without access control."
          ],
          "do": [
            "Inventory existing file-drop integrations and migrate with priority. When file transfer is unavoidable: GoAnywhere, Axway MFT, or CPI SFTP adapter with acknowledgment and audit log. Define clear file naming convention and archiving strategy for migration."
          ],
          "dont": [
            "Keine direkten Prod-Änderungen außerhalb des definierten Change-Prozesses.",
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generally for all new initiatives. Migrate existing inventory."
          ]
        }
      }
    },
    {
      "label": "Direct DB integration",
      "ring": "HOLD",
      "quadrant": "Patterns & Architecture",
      "sources": [
        "https://api.sap.com"
      ],
      "references": [
        "https://help.sap.com/docs/abap-platform/abap-platform-integration/api-business-hub",
        "https://api.sap.com",
        "https://help.sap.com/docs/s4hana-cloud/sap-s-4hana-cloud/apis"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Direkte Datenbankintegration: externes System oder Middleware liest/schreibt direkt in die SAP-HANA- oder ERP-Datenbank, ohne den Anwendungsserver und dessen Business-Logik, Berechtigungsprüfungen und Transaktionssteuerung zu nutzen. Typisch in Legacy-Setups: direkter JDBC-Zugriff auf S/4HANA-HANA-Mandant.",
          "whyRing": "HOLD ohne jede Ausnahme. Direkter DB-Zugriff auf SAP-Systeme ist nicht supportiert, verletzt SAP-Lizenz- und Support-Bedingungen, umgeht Berechtigungs- und Mandantenschutz und erzeugt kritische Datenkonsistenz-Risiken.",
          "risks": [
            "SAP-Support-Ausschluss: Schäden durch direkten DB-Zugriff sind nicht durch SAP abgedeckt. Datenkonsistenz: Schreiben ohne ABAP-Application-Server umgeht Locking, Triggering und Business-Logik. Security: DB-Credentials mit breitem Zugriff massives Sicherheitsrisiko. Lizenzverstoß: direkte DB-Zugriffe können SAP-Lizenzvereinbarungen verletzen."
          ],
          "do": [
            "Bestehende direkte DB-Zugiffe sofort inventarisieren und Migrationsplan erstellen. Ablösung durch offizielle OData v4-APIs oder RFC/BAPI (als Übergangslösung). Security-Audit für alle Systeme mit direktem DB-Zugriff durchführen."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Immer und ohne Ausnahme vermeiden."
          ]
        },
        "en": {
          "intro": "Direct database integration: an external system or middleware reads/writes directly into the SAP HANA or ERP database, bypassing the application server and its business logic, authorization checks, and transaction control. Typical in legacy setups: direct JDBC access to S/4HANA HANA client.",
          "whyRing": "HOLD without any exception. Direct DB access to SAP systems is not supported, violates SAP license and support terms, bypasses authorization and client protection, and creates critical data consistency risks.",
          "risks": [
            "SAP support exclusion: damage caused by direct DB access is not covered by SAP. Data consistency: writing without ABAP application server bypasses locking, triggering, and business logic. Security: DB credentials with broad access are a massive security risk. License violation: direct DB access may violate SAP license agreements."
          ],
          "do": [
            "Immediately inventory existing direct DB accesses and create migration plan. Replace with official OData v4 APIs or RFC/BAPI (as interim solution). Conduct security audit for all systems with direct DB access."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Always avoid without exception."
          ]
        }
      }
    },
    {
      "label": "REST/JSON",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://www.rfc-editor.org/rfc/rfc7807"
      ],
      "references": [
        "https://spec.openapis.org/oas/latest.html",
        "https://www.rfc-editor.org/rfc/rfc7807",
        "https://api.sap.com",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Es geht um die Definition und den stabilen Betrieb von Schnittstellenverträgen inklusive Security, Versionierung und Governance. Dieser Punkt beeinflusst direkt Entkopplung, Änderbarkeit und Integrationskosten über Teamgrenzen hinweg.",
          "whyRing": "ADOPT ohne Einschränkung. REST/JSON hat SOAP, RFC und proprietäre Formate als Standard für neue Systemintegrationen vollständig abgelöst. Kein Neuvorhaben ohne REST/JSON-Grundlage.",
          "risks": [
            "REST ist kein Protokoll – Interoperabilität entsteht erst durch konsistente API-Design-Guidelines (HTTP-Verben korrekt, HTTP-Status-Codes semantisch korrekt, Ressourcenmodellierung). Fehlende Fehlermodelle (HTTP 200 für jeden Fehler) verursachen Consumer-Integrationsfehler. Kein natives Schema-Enforcement ohne OpenAPI-Validierung."
          ],
          "do": [
            "HTTP-Semantik korrekt nutzen: GET (idempotent, cacheable), POST (create), PUT (replace), PATCH (partial update), DELETE (remove). HTTP-Status-Codes semantisch korrekt: 200 OK, 201 Created, 400 Bad Request, 401 Unauthorized, 404 Not Found, 500 Internal Server Error. Fehlermodell nach RFC 7807 (Problem Details for HTTP APIs) standardisieren. OpenAPI 3.1-Spec als Kontrakt-Grundlage für alle REST-APIs."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Wenn Streaming-Protokolle benötigt (→ WebSocket, SSE). Wenn hochvolumige interne Service-zu-Service-Kommunikation mit sehr niedrigen Latenzanforderungen (→ gRPC). Wenn Event-basierte asynchrone Kommunikation (→ AMQP/Event Mesh)."
          ]
        },
        "en": {
          "intro": "This item is about defining and running stable interface contracts, including security, versioning, and governance. It directly impacts decoupling, changeability, and integration cost across teams.",
          "whyRing": "ADOPT without restriction. REST/JSON has fully replaced SOAP, RFC, and proprietary formats as the standard for new system integrations. No new initiative without a REST/JSON foundation.",
          "risks": [
            "REST is not a protocol – interoperability only emerges through consistent API design guidelines (correct HTTP verbs, semantically correct HTTP status codes, resource modeling). Missing error models (HTTP 200 for every error) cause consumer integration failures. No native schema enforcement without OpenAPI validation."
          ],
          "do": [
            "Use HTTP semantics correctly: GET (idempotent, cacheable), POST (create), PUT (replace), PATCH (partial update), DELETE (remove). HTTP status codes semantically correct: 200 OK, 201 Created, 400 Bad Request, 401 Unauthorized, 404 Not Found, 500 Internal Server Error. Standardize error model per RFC 7807 (Problem Details for HTTP APIs). OpenAPI 3.1 spec as contract foundation for all REST APIs."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "When streaming protocols are needed (→ WebSocket, SSE). When high-volume internal service-to-service communication requires very low latency (→ gRPC). When event-based asynchronous communication is needed (→ AMQP/Event Mesh)."
          ]
        }
      }
    },
    {
      "label": "OData v4",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://docs.oasis-open.org/odata/odata/v4.01/odata-v4.01-part1-protocol.html"
      ],
      "references": [
        "https://docs.oasis-open.org/odata/odata/v4.01/odata-v4.01-part1-protocol.html",
        "https://api.sap.com",
        "https://help.sap.com/docs/abap-cloud/abap-restful-application-programming/odata-v4-service-exposure",
        "https://sap.github.io/cloud-sdk/docs/java/features/odata/overview"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "OData v4 (Open Data Protocol) ist ein OASIS-Standard für REST-basierte Datenzugriff-APIs mit standardisierten Query-Optionen ($filter, $select, $expand, $orderby, $top, $skip), Batch-Requests, Entity-Relationship-Modell und Metadaten-Dokument (EDMX). SAP nutzt OData v4 als primäres API-Protokoll für S/4HANA Cloud, SAP Fiori und alle neuen SAP-Produktlinien. OData v2 ist Legacy.",
          "whyRing": "ADOPT: OData v4 ist das strategische SAP-API-Protokoll für alle Datenzugriffe auf S/4HANA Cloud. OData v2 aktiv durch v4 ersetzen. Kein Neueinsatz von OData v2 oder RFC für Szenarien die OData v4 abdeckt.",
          "risks": [
            "OData v4-Coverage in S/4HANA noch nicht für alle Geschäftsobjekte vollständig – Prüfung im SAP Business Accelerator Hub vor Projektstart notwendig. Komplexe $expand-Ketten können hohe DB-Last erzeugen – serverseitiges Paging ist Pflicht. EDMX-Metadaten-Dokument muss bei API-Änderungen aktuell gehalten werden."
          ],
          "do": [
            "Vor Projektstart prüfen ob OData v4-API auf api.sap.com verfügbar. Serverseitiges Paging ($top, $skip, $skiptoken) für alle Collections aktivieren – kein Full-Table-Scan. OData-Client-Libraries nutzen (SAP Cloud SDK, Microsoft OData Client) statt manuellem HTTP. OpenAPI-Spec aus EDMX generieren für nicht-OData-affine Consumer."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Wenn kein Entity-Relationship-basiertes Datenmodell vorhanden (→ REST/JSON). Für Event-basierte Kommunikation (→ AMQP / Event Mesh)."
          ]
        },
        "en": {
          "intro": "OData v4 (Open Data Protocol) is an OASIS standard for REST-based data access APIs with standardized query options ($filter, $select, $expand, $orderby, $top, $skip), batch requests, entity-relationship model, and metadata document (EDMX). SAP uses OData v4 as the primary API protocol for S/4HANA Cloud, SAP Fiori, and all new SAP product lines. OData v2 is legacy.",
          "whyRing": "ADOPT: OData v4 is the strategic SAP API protocol for all data access on S/4HANA Cloud. Actively replace OData v2 with v4. No new use of OData v2 or RFC for scenarios covered by OData v4.",
          "risks": [
            "OData v4 coverage in S/4HANA not yet complete for all business objects – check in SAP Business Accelerator Hub before project start. Complex $expand chains can create high DB load – server-side paging is mandatory. EDMX metadata document must be kept current when APIs change."
          ],
          "do": [
            "Before project start, verify OData v4 API availability at api.sap.com. Activate server-side paging ($top, $skip, $skiptoken) for all collections – no full table scan. Use OData client libraries (SAP Cloud SDK, Microsoft OData Client) instead of manual HTTP. Generate OpenAPI spec from EDMX for non-OData-affine consumers."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "When no entity-relationship-based data model exists (→ REST/JSON). For event-based communication (→ AMQP / Event Mesh)."
          ]
        }
      }
    },
    {
      "label": "OAuth2 / OIDC",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/identity-authentication"
      ],
      "references": [
        "https://oauth.net/2/",
        "https://openid.net/developers/how-connect-works/",
        "https://help.sap.com/docs/identity-authentication",
        "https://help.sap.com/docs/btp/sap-business-technology-platform/security-and-authorizations",
        "https://www.rfc-editor.org/rfc/rfc6749"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "OAuth 2.0 (RFC 6749) ist das Industriestandard-Framework für delegierte API-Autorisierung; OpenID Connect (OIDC) ist die Authentifizierungsschicht auf OAuth 2.0. Im SAP-Kontext: SAP IAS (Identity Authentication Service) als Identity Provider, BTP XSUAA als Authorization Server, OAuth2-Client-Credentials-Flow für M2M-Integrationen und Authorization-Code-Flow für User-delegierte Zugriffe. OAuth2/OIDC ersetzt Basic Auth und proprietäre Token-Systeme vollständig.",
          "whyRing": "ADOPT ohne Ausnahme. Basic Authentication für API-Zugriffe ist ein Sicherheitsrisiko und in modernen Landschaften nicht akzeptabel. OAuth2/OIDC ist Standard.",
          "risks": [
            "Falsche Flow-Wahl: Client-Credentials statt Authorization-Code für User-Context (Sicherheitsrisiko). Token-Ablauf ohne automatische Rotation führt zu Integrationsausfällen. XSUAA-Scope-Konfiguration ohne Least-Privilege-Prinzip gibt zu breite Zugriffsrechte. Fehlende Token-Validierung auf Resource-Server-Seite (Signatur, Audience, Expiry)."
          ],
          "do": [
            "Client-Credentials-Flow für M2M (System-zu-System) ohne User-Kontext. Authorization-Code-Flow (PKCE) für User-delegierte Szenarien. Token-Refresh automatisieren – kein manuelles Token-Management. Scopes nach Least-Privilege konfigurieren: nur notwendige API-Berechtigungen. JWT-Signatur und Audience im Resource Server validieren (nicht nur Existenz prüfen). SAP IAS als zentralen IdP für alle BTP-Dienste konfigurieren."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Nicht für interne Service-zu-Service-Kommunikation innerhalb eines vertrauenswürdigen Kyma-Clusters (→ mTLS/Service Mesh). Nicht als Ersatz für App-Autorisierung (ABAP-Rollen, RBAC)."
          ]
        },
        "en": {
          "intro": "OAuth 2.0 (RFC 6749) is the industry-standard framework for delegated API authorization; OpenID Connect (OIDC) is the authentication layer on top of OAuth 2.0. In the SAP context: SAP IAS (Identity Authentication Service) as identity provider, BTP XSUAA as authorization server, OAuth2 Client Credentials Flow for M2M integrations, and Authorization Code Flow for user-delegated access. OAuth2/OIDC fully replaces Basic Auth and proprietary token systems.",
          "whyRing": "ADOPT without exception. Basic Authentication for API access is a security risk and not acceptable in modern landscapes. OAuth2/OIDC is the standard.",
          "risks": [
            "Wrong flow selection: Client Credentials instead of Authorization Code for user context (security risk). Token expiry without automatic rotation leads to integration outages. XSUAA scope configuration without least-privilege gives overly broad access rights. Missing token validation on the resource server side (signature, audience, expiry)."
          ],
          "do": [
            "Client Credentials Flow for M2M (system-to-system) without user context. Authorization Code Flow (PKCE) for user-delegated scenarios. Automate token refresh – no manual token management. Configure scopes with least-privilege: only required API permissions. Validate JWT signature and audience in the resource server (not just existence). Configure SAP IAS as the central IdP for all BTP services."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "Not for internal service-to-service communication within a trusted Kyma cluster (→ mTLS/Service Mesh). Not as a replacement for app-level authorization (ABAP roles, RBAC)."
          ]
        }
      }
    },
    {
      "label": "OpenAPI 3.x",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://spec.openapis.org/oas/latest.html"
      ],
      "references": [
        "https://spec.openapis.org/oas/latest.html",
        "https://stoplight.io/open-source/spectral",
        "https://api.sap.com",
        "https://json-schema.org/"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "OpenAPI 3.x (ehemals Swagger) ist der OASIS-Standard zur maschinenlesbaren Beschreibung von REST-APIs: Endpunkte, HTTP-Methoden, Request/Response-Schemas, Authentifizierungsverfahren, Beispiele und Fehlercodes. OpenAPI 3.1 (2021) ist vollständig JSON-Schema-kompatibel. SAP Business Accelerator Hub publiziert alle S/4HANA-APIs als OpenAPI-Spezifikationen. OpenAPI ist Voraussetzung für APIM, Contract Testing, Code-Generierung und Developer Portal.",
          "whyRing": "ADOPT: Kein REST-API ohne OpenAPI 3.x-Spezifikation ist im APIM/Developer-Portal publizierbar oder testbar. Pflicht-Artefakt für jede Schnittstelle.",
          "risks": [
            "Spec-Drift: Implementierung weicht von Spec ab ohne automatische Erkennung. Oberflächliche Specs ohne Beispiele, Fehlermodelle und Schemavalidierung liefern keinen Mehrwert. Veraltete OpenAPI 2.0 (Swagger)-Specs im Umlauf neben aktuellen 3.x-Specs."
          ],
          "do": [
            "OpenAPI 3.1 als Zielversion für alle neuen APIs. Spectral-Linter mit eigenen API-Design-Regeln als CI-Pflichtschritt. Beispiele (examples/example) für alle Request- und Response-Schemata. Fehlercodes dokumentieren: welche HTTP-Status-Codes werden in welchen Situationen zurückgegeben. Spec in Git versionieren (Semantic Versioning für die API-Version im Info-Block)."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Nicht für Event/Message-APIs – AsyncAPI ist der korrekte Standard. Nicht für ABAP/RFC-Schnittstellen ohne REST-Wrapper."
          ]
        },
        "en": {
          "intro": "OpenAPI 3.x (formerly Swagger) is the OASIS standard for machine-readable REST API description: endpoints, HTTP methods, request/response schemas, authentication schemes, examples, and error codes. OpenAPI 3.1 (2021) is fully JSON Schema compatible. The SAP Business Accelerator Hub publishes all S/4HANA APIs as OpenAPI specifications. OpenAPI is a prerequisite for APIM, contract testing, code generation, and developer portal.",
          "whyRing": "ADOPT: No REST API without an OpenAPI 3.x specification can be published in APIM/Developer Portal or tested. Mandatory artifact for every interface.",
          "risks": [
            "Spec drift: implementation diverges from spec without automatic detection. Superficial specs without examples, error models, and schema validation provide no value. Outdated OpenAPI 2.0 (Swagger) specs in circulation alongside current 3.x specs."
          ],
          "do": [
            "OpenAPI 3.1 as target version for all new APIs. Spectral linter with custom API design rules as mandatory CI step. Examples (examples/example) for all request and response schemas. Document error codes: which HTTP status codes are returned in which situations. Version spec in Git (semantic versioning for API version in the Info block)."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "Not for event/message APIs – AsyncAPI is the correct standard. Not for ABAP/RFC interfaces without a REST wrapper."
          ]
        }
      }
    },
    {
      "label": "Webhooks with signature verification",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/https-sender-adapter"
      ],
      "references": [
        "https://docs.github.com/en/webhooks/using-webhooks/validating-webhook-deliveries",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/https-sender-adapter",
        "https://www.rfc-editor.org/rfc/rfc2104"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Webhooks sind HTTP-Callbacks: Producer-System sendet beim Eintreten eines Ereignisses aktiv eine HTTP-POST-Nachricht an einen vorkonfigurierten Consumer-Endpunkt. Signaturverifikation (HMAC-SHA256, oft im Header X-Webhook-Signature) stellt sicher, dass die Nachricht tatsächlich vom erwarteten Producer stammt und nicht manipuliert wurde. Verwendet von externen SaaS-Systemen (GitHub, Salesforce, SAP Commerce) zur Event-Benachrichtigung ohne Event-Broker.",
          "whyRing": "ADOPT: Webhooks mit Signaturverifikation sind Industriestandard für externe SaaS-Integration. Ohne Signaturverifikation ist jeder Webhook-Endpunkt ein offenes Angriffsziel (Replay-Attacks, Spoofing).",
          "risks": [
            "Kein Replay-Schutz ohne Timestamp-Validierung (Nachricht aus der Vergangenheit re-deliverbar). Webhook-Endpunkt muss öffentlich erreichbar sein – Exposure-Risiko. Fehlende Idempotenz: Webhook-Delivery ist in der Regel at-least-once."
          ],
          "do": [
            "HMAC-SHA256-Signatur im Header validieren bevor Payload verarbeitet wird. Timestamp aus Webhook-Header prüfen: Nachrichten älter als 5 Minuten ablehnen (Replay-Schutz). Webhook-Endpunkt in CPI als HTTP-Sender-Adapter mit Signaturvalidierung im Groovy-Script. Idempotenz-Key aus Webhook-ID oder Event-ID ableiten. Webhook-Endpunkt-URL nicht öffentlich dokumentieren (Security-through-Obscurity als zweite Schicht)."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Producer keinen Webhook-Mechanismus bietet (→ Polling oder Event-Mesh-basiert). Für interne SAP-zu-SAP-Kommunikation (→ Event Mesh / CPI direkt)."
          ]
        },
        "en": {
          "intro": "Webhooks are HTTP callbacks: the producer system actively sends an HTTP POST message to a pre-configured consumer endpoint when an event occurs. Signature verification (HMAC-SHA256, often in the X-Webhook-Signature header) ensures the message actually comes from the expected producer and has not been tampered with. Used by external SaaS systems (GitHub, Salesforce, SAP Commerce) for event notification without an event broker.",
          "whyRing": "ADOPT: Webhooks with signature verification are an industry standard for external SaaS integration. Without signature verification, every webhook endpoint is an open attack target (replay attacks, spoofing).",
          "risks": [
            "No replay protection without timestamp validation (past message can be re-delivered). Webhook endpoint must be publicly reachable – exposure risk. Missing idempotency: webhook delivery is typically at-least-once."
          ],
          "do": [
            "Validate HMAC-SHA256 signature in the header before processing payload. Check timestamp from webhook header: reject messages older than 5 minutes (replay protection). Webhook endpoint in CPI as HTTP sender adapter with signature validation in Groovy script. Derive idempotency key from webhook ID or event ID. Do not publicly document webhook endpoint URL (security through obscurity as second layer)."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When producer does not offer a webhook mechanism (→ polling or event-mesh-based). For internal SAP-to-SAP communication (→ Event Mesh / CPI directly)."
          ]
        }
      }
    },
    {
      "label": "AsyncAPI",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://www.asyncapi.com/docs"
      ],
      "references": [
        "https://www.asyncapi.com/docs",
        "https://github.com/asyncapi/spec",
        "https://studio.asyncapi.com/",
        "https://help.sap.com/docs/event-mesh"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "AsyncAPI 3.x ist CNCF-backed, technisch reif und de-facto Standard für Event-API-Governance. Da Event-Mesh-Adoption wächst, muss AsyncAPI-Spec-Pflicht jetzt eingeführt werden. Empfehle ADOPT als obligatorisches Governance-Artefakt für alle Events. Confidence: Medium.",
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "AsyncAPI ist der CNCF-/Linux-Foundation-Standard zur Beschreibung von Event- und Message-APIs (Publish/Subscribe, Queues, Streams). AsyncAPI 3.0 (2023) definiert Channels, Operations (send/receive), Messages und Server-Verbindungen. Es ist das asynchrone Äquivalent zu OpenAPI und deckt AMQP, Kafka, MQTT, WebSocket und HTTP-Callbacks ab. Im SAP-Kontext: Spezifikation von S/4HANA Business Events und SAP Event Mesh-Topics.",
          "whyRing": "TRIAL korrekt: AsyncAPI 3.x ist technisch reif und industriell im Wachstum. Im SAP-Ökosystem noch wenig verbreitet. Jetzt Governance einführen, bevor Event-Proliferation einsetzt. Mittelfristig ADOPT-Kandidat.",
          "risks": [
            "Tooling-Reife geringer als bei OpenAPI (weniger Code-Generatoren, Linter-Unterstützung). Migration von AsyncAPI 2.x auf 3.0 erfordert Spec-Anpassungen (Breaking Changes in Spec-Struktur). Ohne Governance-Prozess entstehen inkompatible Schemas zwischen Producer- und Consumer-Teams."
          ],
          "do": [
            "Für jeden Event-Topic eine AsyncAPI 3.x-Spec erstellen und in Git versionieren. AsyncAPI Studio oder CLI für Spec-Validierung und Preview. S/4HANA Business Events mit AsyncAPI dokumentieren und im Developer Portal veröffentlichen. Schema-Kompatibilitätsstrategie (Backward-compatible-only) pro Topic festlegen."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen.",
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Für synchrone REST-APIs → OpenAPI 3.x. Wenn noch kein Event-basiertes System im Einsatz."
          ]
        },
        "en": {
          "intro": "AsyncAPI is the CNCF/Linux Foundation standard for describing event and message APIs (publish/subscribe, queues, streams). AsyncAPI 3.0 (2023) defines channels, operations (send/receive), messages, and server connections. It is the asynchronous equivalent of OpenAPI and covers AMQP, Kafka, MQTT, WebSocket, and HTTP callbacks. In the SAP context: specification of S/4HANA Business Events and SAP Event Mesh topics.",
          "whyRing": "TRIAL is correct: AsyncAPI 3.x is technically mature and growing industrially. Still uncommon in the SAP ecosystem. Introduce governance now before event proliferation sets in. Medium-term ADOPT candidate.",
          "risks": [
            "Tooling maturity lower than OpenAPI (fewer code generators, linter support). Migration from AsyncAPI 2.x to 3.0 requires spec adjustments (breaking changes in spec structure). Without governance process, incompatible schemas emerge between producer and consumer teams."
          ],
          "do": [
            "Create an AsyncAPI 3.x spec for every event topic and version it in Git. Use AsyncAPI Studio or CLI for spec validation and preview. Document S/4HANA Business Events with AsyncAPI and publish in developer portal. Define schema compatibility strategy (backward-compatible-only) per topic."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency.",
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "For synchronous REST APIs → OpenAPI 3.x. When no event-based system is yet in use."
          ]
        }
      }
    },
    {
      "label": "CloudEvents",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md"
      ],
      "references": [
        "https://cloudevents.io/",
        "https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md",
        "https://www.cncf.io/projects/cloudevents/",
        "https://github.com/cloudevents/sdk-javascript"
      ],
      "ringChangeSuggestion": "ADOPT",
      "changeRationale": "CNCF Graduated, in SAP Event Mesh nativ unterstützt, klarer Mehrwert gegenüber proprietären Formaten. Empfehle ADOPT als Envelope-Pflichtstandard. Confidence: High.",
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "CloudEvents 1.0 ist ein CNCF Graduated-Standard für ein einheitliches Event-Daten-Hüllformat. Es definiert Pflichtattribute (id, source, specversion, type) und optionale Attribute (datacontenttype, subject, time, dataschema) als JSON-, AVRO- oder Protobuf-Envelope. SAP Event Mesh und S/4HANA Business Events unterstützen CloudEvents nativ. CloudEvents macht Events Broker-agnostisch und ermöglicht einheitliches Routing und Filtering.",
          "whyRing": "TRIAL mit starker ADOPT-Tendenz: CloudEvents 1.0 ist ein stabiler, breiter Industriestandard und in SAP-Produkten native verfügbar. Einführung als Pflicht-Envelope-Format für alle neuen Events wird empfohlen.",
          "risks": [
            "CloudEvents standardisiert nur das Envelope – Payload-Schema (im 'data'-Feld) muss separat via AsyncAPI oder JSON Schema spezifiziert werden. Legacy-Systeme emittieren noch kein CloudEvents-Format: CPI-Adapter als Transformer nötig."
          ],
          "do": [
            "Einsatz auf wenige, geeignete Domänen begrenzen und Ergebnisse systematisch auswerten.",
            "Schemas versionieren und Consumer-Kompatibilität explizit testen.",
            "Retry/Backoff, DLQ und Reprocessing-Runbook definieren."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Wenn Broker kein CloudEvents unterstützt und Transformation unverhältnismäßig aufwändig."
          ]
        },
        "en": {
          "intro": "CloudEvents 1.0 is a CNCF Graduated standard for a unified event data envelope format. It defines mandatory attributes (id, source, specversion, type) and optional attributes (datacontenttype, subject, time, dataschema) as JSON, AVRO, or Protobuf envelope. SAP Event Mesh and S/4HANA Business Events natively support CloudEvents. CloudEvents makes events broker-agnostic and enables unified routing and filtering.",
          "whyRing": "TRIAL with strong ADOPT tendency: CloudEvents 1.0 is a stable, broad industry standard and natively available in SAP products. Introduction as mandatory envelope format for all new events is recommended.",
          "risks": [
            "CloudEvents only standardizes the envelope – payload schema (in the 'data' field) must be specified separately via AsyncAPI or JSON Schema. Legacy systems do not yet emit CloudEvents format: CPI adapter as transformer required."
          ],
          "do": [
            "Einsatz auf wenige, geeignete Domänen begrenzen und Ergebnisse systematisch auswerten.",
            "Schemas versionieren und Consumer-Kompatibilität explizit testen.",
            "Retry/Backoff, DLQ und Reprocessing-Runbook definieren."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "When broker does not support CloudEvents and transformation is disproportionately costly."
          ]
        }
      }
    },
    {
      "label": "AMQP 1.0",
      "ring": "ADOPT",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://www.amqp.org/resources/specifications"
      ],
      "references": [
        "https://www.amqp.org/resources/specifications",
        "https://help.sap.com/docs/event-mesh",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/amqp-sender-adapter"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "AMQP 1.0 (Advanced Message Queuing Protocol) ist ein OASIS-Standard für nachrichtenbasierte Middleware-Kommunikation. Er definiert Wire-Level-Protokoll, Framing, Session-Management und Delivery-Guarantees (at-most-once, at-least-once, exactly-once auf Session-Ebene). SAP Event Mesh, SAP Advanced Event Mesh und Azure Service Bus unterstützen AMQP 1.0 nativ. Im CPI: AMQP-Adapter für Event-Mesh-Kommunikation.",
          "whyRing": "ADOPT: AMQP 1.0 ist der primäre Protokollstandard für SAP Event Mesh und AEM. Für alle produktiven EDA-Integrationen auf SAP-Broker-Basis ist AMQP 1.0 die erste Wahl gegenüber REST-basiertem Messaging (geringere Overhead, bessere Delivery-Garantien).",
          "risks": [
            "AMQP-Client-Bibliotheken je Programmiersprache unterschiedlich reif. Verbindungsmanagement (Reconnect, Heartbeat) muss explizit konfiguriert werden. AMQP 1.0 ist nicht kompatibel mit AMQP 0-9-1 (RabbitMQ) – Verwechslung häufig."
          ],
          "do": [
            "AMQP 1.0 für alle produktiven SAP-Event-Mesh-Consumer und -Producer nutzen. AMQP-Verbindungs-Heartbeat und Auto-Reconnect konfigurieren. SAP CPI AMQP-Adapter für Event-Mesh-Consumer-Integration nutzen. AMQP 1.0 klar von AMQP 0-9-1 abgrenzen in Architektur-Dokumentation."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Wenn Broker kein AMQP 1.0 unterstützt (→ REST-basiertes Messaging). Für IoT/OT-Szenarien mit ressourcenbeschränkten Devices (→ MQTT)."
          ]
        },
        "en": {
          "intro": "AMQP 1.0 (Advanced Message Queuing Protocol) is an OASIS standard for message-based middleware communication. It defines a wire-level protocol, framing, session management, and delivery guarantees (at-most-once, at-least-once, exactly-once at session level). SAP Event Mesh, SAP Advanced Event Mesh, and Azure Service Bus natively support AMQP 1.0. In CPI: AMQP adapter for Event Mesh communication.",
          "whyRing": "ADOPT: AMQP 1.0 is the primary protocol standard for SAP Event Mesh and AEM. For all production EDA integrations on SAP broker basis, AMQP 1.0 is the first choice over REST-based messaging (lower overhead, better delivery guarantees).",
          "risks": [
            "AMQP client library maturity varies by programming language. Connection management (reconnect, heartbeat) must be explicitly configured. AMQP 1.0 is not compatible with AMQP 0-9-1 (RabbitMQ) – frequent confusion."
          ],
          "do": [
            "Use AMQP 1.0 for all production SAP Event Mesh consumers and producers. Configure AMQP connection heartbeat and auto-reconnect. Use SAP CPI AMQP adapter for Event Mesh consumer integration. Clearly distinguish AMQP 1.0 from AMQP 0-9-1 in architecture documentation."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "When broker does not support AMQP 1.0 (→ REST-based messaging). For IoT/OT scenarios with resource-constrained devices (→ MQTT)."
          ]
        }
      }
    },
    {
      "label": "MQTT (IoT/OT scenarios)",
      "ring": "ASSESS",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html"
      ],
      "references": [
        "https://mqtt.org/",
        "https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html",
        "https://help.sap.com/docs/sap-advanced-event-mesh"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "MQTT (Message Queuing Telemetry Transport) ist ein leichtgewichtiges Pub/Sub-Protokoll für ressourcenbeschränkte Geräte und IoT/OT-Szenarien. MQTT 5.0 (2019) bringt verbesserte Fehlerbehandlung, Session-Expiry und Shared Subscriptions. SAP Advanced Event Mesh unterstützt MQTT 3.1 und 5.0 nativ. Relevant für SAP Asset Intelligence Network, SAP BTP IoT Service und OT-Anbindungen (Maschinenparks, Sensoren).",
          "whyRing": "ASSESS korrekt: MQTT ist im IoT/OT-Kontext der richtige Standard, aber im SAP-Enterprise-Integrationskontext ein Nischenprotokoll. Nur evaluieren wenn konkrete IoT/OT-Anbindungsanforderungen bestehen.",
          "risks": [
            "MQTT QoS-Level-Wahl (0/1/2) hat direkte Auswirkung auf Payload-Duplikate und Latenz. MQTT-Security: TLS + Client-Certificate-Auth oder Username/Password – kein ungesichertes MQTT. Skalierung zu sehr hohen Device-Zahlen erfordert Broker-Sizing-Expertise."
          ],
          "do": [
            "MQTT 5.0 gegenüber 3.1 bevorzugen (bessere Fehlerbehandlung, Shared Subscriptions). TLS und Mutual TLS (mTLS) für alle MQTT-Verbindungen erzwingen. QoS-Level bewusst wählen: QoS 1 für die meisten IoT-Szenarien ausreichend. MQTT-Broker (AEM) als Brücke zu AMQP/Event-Mesh für SAP-Backend-Anbindung nutzen."
          ],
          "dont": [
            "Keine Events ohne Ownership und fachliche Idempotenz veröffentlichen."
          ],
          "whenNotToUse": [
            "Für alle nicht-IoT/OT-Enterprise-Integrationen (→ AMQP 1.0 oder REST). Wenn Broker kein MQTT unterstützt."
          ]
        },
        "en": {
          "intro": "MQTT (Message Queuing Telemetry Transport) is a lightweight pub/sub protocol for resource-constrained devices and IoT/OT scenarios. MQTT 5.0 (2019) brings improved error handling, session expiry, and shared subscriptions. SAP Advanced Event Mesh natively supports MQTT 3.1 and 5.0. Relevant for SAP Asset Intelligence Network, SAP BTP IoT Service, and OT connectivity (machine parks, sensors).",
          "whyRing": "ASSESS is correct: MQTT is the right standard in IoT/OT context but a niche protocol in SAP enterprise integration. Evaluate only when concrete IoT/OT connectivity requirements exist.",
          "risks": [
            "MQTT QoS level choice (0/1/2) directly impacts payload duplicates and latency. MQTT security: TLS + client certificate auth or username/password – no unsecured MQTT. Scaling to very high device counts requires broker sizing expertise."
          ],
          "do": [
            "Prefer MQTT 5.0 over 3.1 (better error handling, shared subscriptions). Enforce TLS and mutual TLS (mTLS) for all MQTT connections. Choose QoS level consciously: QoS 1 sufficient for most IoT scenarios. Use MQTT broker (AEM) as bridge to AMQP/Event Mesh for SAP backend connectivity."
          ],
          "dont": [
            "Do not publish events without clear ownership and business idempotency."
          ],
          "whenNotToUse": [
            "For all non-IoT/OT enterprise integrations (→ AMQP 1.0 or REST). When broker does not support MQTT."
          ]
        }
      }
    },
    {
      "label": "GraphQL (BFF only)",
      "ring": "ASSESS",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://spec.graphql.org/"
      ],
      "references": [
        "https://graphql.org/learn/",
        "https://spec.graphql.org/",
        "https://www.graphql-java.com/documentation/",
        "https://the-guild.dev/graphql/mesh"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "GraphQL ist eine Abfragesprache für APIs, die Clients erlaubt, exakt die benötigten Datenfelder anzufragen (kein Over-/Under-Fetching). Im SAP-Kontext ausschließlich relevant als Backend-for-Frontend (BFF)-Aggregationsschicht: GraphQL aggregiert Daten aus mehreren SAP-APIs (OData v4, REST) für spezifische Frontend-Anforderungen. SAP bietet keine native First-Class-GraphQL-Unterstützung auf API-Ebene.",
          "whyRing": "ASSESS korrekt: GraphQL löst spezifische Frontend-Aggregationsprobleme, hat aber im SAP-Enterprise-Kontext keinen strategischen Mehrwert gegenüber OData v4. Nur evaluieren wenn konkreter BFF-Anwendungsfall nachweisbar vorhanden.",
          "risks": [
            "N+1-Query-Problem: unkontrollierte Resolver-Aufrufe können Backends überlasten. Keine native SAP-Unterstützung: eigener GraphQL-Server (Kyma) zu entwickeln und betreiben. Caching komplexer als bei REST (kein HTTP-Caching auf Request-Ebene möglich). Fehlende Tool-Reife für Enterprise-API-Governance (kein GraphQL-Äquivalent zu APIM-Policies)."
          ],
          "do": [
            "GraphQL nur als BFF für spezifische Frontend-Anforderungen mit mehreren Datenquellen. DataLoader-Pattern für Batching und N+1-Vermeidung implementieren. Tiefenbegrenzung und Komplexitätslimits für Queries konfigurieren. GraphQL-Server in Kyma als Container-Service betreiben."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Wenn OData v4 $select und $expand den Aggregationsbedarf abdeckt. Für Backend-zu-Backend-SAP-Integrationen. Wenn kein dediziertes Frontend-Team mit GraphQL-Bedarf vorhanden."
          ]
        },
        "en": {
          "intro": "GraphQL is a query language for APIs that allows clients to request exactly the data fields needed (no over-/under-fetching). In the SAP context, exclusively relevant as a Backend-for-Frontend (BFF) aggregation layer: GraphQL aggregates data from multiple SAP APIs (OData v4, REST) for specific frontend requirements. SAP offers no native first-class GraphQL support at the API level.",
          "whyRing": "ASSESS is correct: GraphQL solves specific frontend aggregation problems but has no strategic advantage over OData v4 in the SAP enterprise context. Evaluate only when a concrete BFF use case is demonstrably present.",
          "risks": [
            "N+1 query problem: uncontrolled resolver calls can overwhelm backends. No native SAP support: own GraphQL server (Kyma) must be developed and operated. Caching more complex than REST (no HTTP caching at request level). Immature tooling for enterprise API governance (no GraphQL equivalent to APIM policies)."
          ],
          "do": [
            "Use GraphQL only as BFF for specific frontend requirements with multiple data sources. Implement DataLoader pattern for batching and N+1 prevention. Configure depth limits and complexity limits for queries. Operate GraphQL server in Kyma as a container service."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "When OData v4 $select and $expand cover the aggregation need. For backend-to-backend SAP integrations. When no dedicated frontend team with a GraphQL need exists."
          ]
        }
      }
    },
    {
      "label": "gRPC (internal S2S)",
      "ring": "ASSESS",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://grpc.io/docs/"
      ],
      "references": [
        "https://grpc.io/docs/",
        "https://protobuf.dev/",
        "https://grpc.io/docs/guides/performance/"
      ],
      "confidence": "Medium",
      "i18n": {
        "de": {
          "intro": "gRPC ist ein von Google entwickeltes RPC-Framework auf Basis von HTTP/2 und Protocol Buffers (Protobuf) als IDL und Serialisierungsformat. Es bietet bidirektionales Streaming, starke Typisierung über Protobuf-Schema und sehr hohe Performance für interne Service-zu-Service-Kommunikation. Im SAP-BTP-Kontext: relevant für Kyma-basierte Microservices mit hohem internem Kommunikationsaufkommen.",
          "whyRing": "ASSESS korrekt: gRPC ist technisch leistungsfähig für interne Microservice-Kommunikation, aber im SAP-Integrationskontext ein Nischenanwendungsfall. REST/OData ist für SAP-Backend-Kommunikation die Standardwahl.",
          "risks": [
            "Protobuf-Schema-Evolution erfordert Disziplin (Field-Numbers dürfen nicht geändert werden). gRPC nicht nativ im Browser unterstützt (→ gRPC-Web als Workaround). Debugging komplexer als REST (kein curl, kein Postman nativ). Kein nativer SAP-Support für gRPC in CPI oder APIM."
          ],
          "do": [
            "gRPC nur für interne Kyma-Microservice-zu-Microservice-Kommunikation mit nachgewiesenen Latenz-/Throughput-Anforderungen. Protobuf-Schema in Git versionieren (Breaking-Change-Policy). gRPC-Reflection für Debugging aktivieren (Entwicklungsumgebungen). mTLS für alle gRPC-Verbindungen erzwingen."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen."
          ],
          "whenNotToUse": [
            "Für alle externen APIs (Consumer außerhalb des Kyma-Clusters). Für SAP-Backend-Kommunikation. Wenn REST-Latenz ausreichend ist."
          ]
        },
        "en": {
          "intro": "gRPC is a Google-developed RPC framework based on HTTP/2 and Protocol Buffers (Protobuf) as IDL and serialization format. It provides bidirectional streaming, strong typing via Protobuf schema, and very high performance for internal service-to-service communication. In the SAP BTP context: relevant for Kyma-based microservices with high internal communication volume.",
          "whyRing": "ASSESS is correct: gRPC is technically powerful for internal microservice communication but a niche use case in the SAP integration context. REST/OData is the default choice for SAP backend communication.",
          "risks": [
            "Protobuf schema evolution requires discipline (field numbers must not be changed). gRPC not natively supported in browsers (→ gRPC-Web as workaround). Debugging more complex than REST (no curl, no native Postman). No native SAP support for gRPC in CPI or APIM."
          ],
          "do": [
            "Use gRPC only for internal Kyma microservice-to-microservice communication with proven latency/throughput requirements. Version Protobuf schema in Git (breaking change policy). Enable gRPC reflection for debugging (development environments). Enforce mTLS for all gRPC connections."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path."
          ],
          "whenNotToUse": [
            "For all external APIs (consumers outside the Kyma cluster). For SAP backend communication. When REST latency is sufficient."
          ]
        }
      }
    },
    {
      "label": "AVRO + Schema Registry",
      "ring": "ASSESS",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://avro.apache.org/docs/current/"
      ],
      "references": [
        "https://avro.apache.org/docs/current/",
        "https://docs.confluent.io/platform/current/schema-registry/index.html",
        "https://help.sap.com/docs/sap-advanced-event-mesh"
      ],
      "confidence": "Low",
      "i18n": {
        "de": {
          "intro": "Apache AVRO ist ein kompaktes, binäres Datenserialisierungsformat mit Schema-basierter Typisierung und Schema-Evolution-Unterstützung. Eine Schema Registry (Confluent Schema Registry, AWS Glue Schema Registry) speichert AVRO-Schemas zentral und stellt Kompatibilitätsprüfungen bei Schema-Änderungen sicher. Im SAP-Kontext: relevant für hochvolumige Kafka/AEM-basierte Streaming-Szenarien wo JSON-Overhead signifikant ist.",
          "whyRing": "ASSESS korrekt: AVRO + Schema Registry ist mächtiges Tooling für Kafka/Streaming-Szenarien, aber im SAP-BTP-Event-Mesh-Kontext oft Overengineering. Nur evaluieren wenn JSON-Overhead messbar Performance-Problem erzeugt.",
          "risks": [
            "Schema Registry als zusätzliche Infrastruktur-Abhängigkeit. AVRO-Schema-Evolution streng: falsche Kompatibilitätsstrategie führt zu Produktionsausfällen. AVRO-Tooling im Java-Ökosystem reif; in anderen Sprachen lückenhafter."
          ],
          "do": [
            "Nur einführen wenn JSON-Performance nachweisbar unzureichend. Schema Registry als zentralen Schema-Store für alle AVRO-Schemas nutzen. Backward-Kompatibilitätsstrategie als Default: neue optionale Felder erlaubt, Felder nicht entfernen/umbenennen ohne Migration. AVRO und AsyncAPI kombinieren: AsyncAPI für Dokumentation, AVRO für Wire-Format."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn JSON-Performance ausreichend. Bei SAP Event Mesh Basis (kein nativer AVRO-Support). Wenn Team keine AVRO/Kafka-Expertise hat."
          ]
        },
        "en": {
          "intro": "Apache AVRO is a compact binary data serialization format with schema-based typing and schema evolution support. A schema registry (Confluent Schema Registry, AWS Glue Schema Registry) centrally stores AVRO schemas and ensures compatibility checks on schema changes. In the SAP context: relevant for high-volume Kafka/AEM-based streaming scenarios where JSON overhead is significant.",
          "whyRing": "ASSESS is correct: AVRO + Schema Registry is powerful tooling for Kafka/streaming scenarios but often overengineering in the SAP BTP Event Mesh context. Evaluate only when JSON overhead measurably creates a performance problem.",
          "risks": [
            "Schema registry as additional infrastructure dependency. AVRO schema evolution is strict: wrong compatibility strategy leads to production outages. AVRO tooling mature in the Java ecosystem; more gaps in other languages."
          ],
          "do": [
            "Only introduce when JSON performance is demonstrably insufficient. Use schema registry as central schema store for all AVRO schemas. Backward compatibility strategy as default: new optional fields allowed, do not remove/rename fields without migration. Combine AVRO and AsyncAPI: AsyncAPI for documentation, AVRO for wire format."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When JSON performance is sufficient. With SAP Event Mesh base (no native AVRO support). When team has no AVRO/Kafka expertise."
          ]
        }
      }
    },
    {
      "label": "EDI/AS2",
      "ring": "ASSESS",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/as2-adapter"
      ],
      "references": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/b2b-integration",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/as2-adapter",
        "https://www.rfc-editor.org/rfc/rfc4130"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Electronic Data Interchange (EDI) in Verbindung mit AS2 (Applicability Statement 2) ist der Industriestandard für B2B-Dokumentenaustausch: EDIFACT-, X12- oder ANSI-Nachrichten werden über HTTPS mit MDN-Quittierung (Message Disposition Notification) und S/MIME-Signierung/Verschlüsselung übertragen. Im SAP-Kontext: SAP Trading Partner Management und SAP Integration Advisor unterstützen AS2 nativ.",
          "whyRing": "ASSESS: EDI/AS2 ist in bestimmten Branchen (Retail, Automotive, Handel) Pflichtstandard für Handelspartner-Kommunikation. Kein strategischer Investitionsbereich, aber notwendig wo Handelspartner es fordern. SAP-native Unterstützung vorhanden.",
          "risks": [
            "Zertifikatspflege für AS2-Signing und -Encryption: Ablauf unterbricht gesamten B2B-Datenaustausch. MDN-Quittierungen müssen archiviert werden (rechtliche Anforderungen). AS2-Partnerprofile ohne zentrale Verwaltung (TPM) werden schnell unkontrollierbar."
          ],
          "do": [
            "AS2-Zertifikate (Signing, Encryption) mit 6-monatigem Vorlauf vor Ablauf erneuern. MDN-Quittierungen persistieren und archivieren (Audit-Trail). SAP Trading Partner Management für zentrales Partnerprofile-Management nutzen. SAP Integration Advisor für EDIFACT/X12-Mapping einsetzen."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Handelspartner moderne REST-APIs oder API-Portal anbieten. Für interne System-zu-System-Kommunikation."
          ]
        },
        "en": {
          "intro": "Electronic Data Interchange (EDI) combined with AS2 (Applicability Statement 2) is the industry standard for B2B document exchange: EDIFACT, X12, or ANSI messages are transmitted over HTTPS with MDN acknowledgment (Message Disposition Notification) and S/MIME signing/encryption. In the SAP context: SAP Trading Partner Management and SAP Integration Advisor natively support AS2.",
          "whyRing": "ASSESS: EDI/AS2 is a mandatory standard in certain industries (retail, automotive, trade) for trading partner communication. Not a strategic investment area, but necessary where trading partners require it. SAP-native support available.",
          "risks": [
            "Certificate maintenance for AS2 signing and encryption: expiry interrupts the entire B2B data exchange. MDN acknowledgments must be archived (legal requirements). AS2 partner profiles without central management (TPM) quickly become uncontrollable."
          ],
          "do": [
            "Renew AS2 certificates (signing, encryption) with 6-month lead time before expiry. Persist and archive MDN acknowledgments (audit trail). Use SAP Trading Partner Management for central partner profile management. Use SAP Integration Advisor for EDIFACT/X12 mapping."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When trading partners offer modern REST APIs or API portals. For internal system-to-system communication."
          ]
        }
      }
    },
    {
      "label": "RFC/BAPI (remote)",
      "ring": "HOLD",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/rfc-receiver-adapter"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/rfc-receiver-adapter",
        "https://help.sap.com/docs/connectivity/sap-btp-connectivity-cf/cloud-connector",
        "https://api.sap.com"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Remote Function Calls (RFC) und Business Application Programming Interfaces (BAPIs) sind das proprietäre SAP-Protokoll für synchrone Systemkommunikation über den ABAP-Application-Server. RFC/BAPI wird über JCo (Java Connector) oder den CPI-RFC-Adapter angesprochen. Es ist das Legacy-Integrationsprotokoll vor OData/REST und noch weit verbreitet in ECC-Landschaften und PI/PO-Migrationsszenarien.",
          "whyRing": "HOLD: Kein neues Interface auf Basis von RFC/BAPI entwickeln. RFC/BAPI nur als Übergangslösung akzeptieren wenn kein OData-v4-Äquivalent verfügbar. Migrationspfad zu OData v4 für jedes RFC/BAPI-Interface einplanen.",
          "risks": [
            "Stark proprietär: JCo-Connector und SAP-eigenes Binärprotokoll. Keine Standard-Security-Protokolle (kein OAuth2 – nur SAP-Logon-Tickets oder Basic Auth). Firewall-Traversal erfordert SAP Cloud Connector oder offene RFC-Ports. RFC-Funktionen nicht versioniert – Breaking Changes ohne Ankündigung möglich."
          ],
          "do": [
            "RFC/BAPI-Nutzung inventarisieren und für jede Schnittstelle OData-v4-Äquivalent prüfen. Wenn RFC unvermeidbar: über CPI-RFC-Adapter kapseln (kein direkter JCo außerhalb CPI). Secure RFC (SNC) für alle produktiven RFC-Verbindungen erzwingen. SAP Cloud Connector für alle RFC-Verbindungen aus Cloud-Systemen verwenden."
          ],
          "dont": [
            "Keine stillen Breaking Changes ohne Migrationspfad einführen.",
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generell für alle Neuvorhaben. Bestand mit Migrationsplan versehen."
          ]
        },
        "en": {
          "intro": "Remote Function Calls (RFC) and Business Application Programming Interfaces (BAPIs) are SAP's proprietary protocol for synchronous system communication over the ABAP application server. RFC/BAPI is accessed via JCo (Java Connector) or the CPI RFC adapter. It is the legacy integration protocol preceding OData/REST and still widely used in ECC landscapes and PI/PO migration scenarios.",
          "whyRing": "HOLD: Do not develop new interfaces based on RFC/BAPI. Accept RFC/BAPI only as a transitional solution when no OData v4 equivalent is available. Plan migration path to OData v4 for every RFC/BAPI interface.",
          "risks": [
            "Highly proprietary: JCo connector and SAP-proprietary binary protocol. No standard security protocols (no OAuth2 – only SAP logon tickets or Basic Auth). Firewall traversal requires SAP Cloud Connector or open RFC ports. RFC functions are not versioned – breaking changes without notice possible."
          ],
          "do": [
            "Inventory RFC/BAPI usage and check for OData v4 equivalent for each interface. When RFC is unavoidable: encapsulate via CPI RFC adapter (no direct JCo outside CPI). Enforce Secure RFC (SNC) for all production RFC connections. Use SAP Cloud Connector for all RFC connections from cloud systems."
          ],
          "dont": [
            "Do not introduce silent breaking changes without a migration path.",
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generally for all new initiatives. Provide existing inventory with migration plan."
          ]
        }
      }
    },
    {
      "label": "SFTP (MFT transition)",
      "ring": "ASSESS",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/sftp-adapter"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/sftp-adapter",
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/sftp-sender-adapter",
        "https://www.rfc-editor.org/rfc/rfc4253"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "SFTP (SSH File Transfer Protocol) ist ein verschlüsseltes Dateiübertragungsprotokoll für den kontrollierten Dateitransfer zwischen Systemen. Im SAP-Kontext: CPI bietet einen SFTP-Adapter für Sender (Polling) und Receiver (Put). SFTP ist die Übergangs-/Übergangstechnologie von unsicheren File-Drop-Szenarien zu geregeltem Managed File Transfer (MFT). Nicht als langfristige strategische Lösung positioniert.",
          "whyRing": "ASSESS: SFTP ist legitim als Übergangslösung und dort wo Handelspartner SFTP zwingend nutzen. Ziel ist Migration zu direkten API-Integrationen. SFTP mit CPI-SFTP-Adapter ist deutlich besser als ungesicherter File-Drop.",
          "risks": [
            "SSH-Key-Management: Ablauf oder Kompromittierung von Private Keys unterbricht Integration. Kein nativer Fehler-Feedback-Mechanismus: Consumer weiß nicht ob Datei korrekt verarbeitet. SFTP-Server als Single-Point-of-Failure wenn nicht HA-konfiguriert. Dateiformat-Änderungen ohne Schema-Vertrag schwer zu erkennen."
          ],
          "do": [
            "SFTP-Adapter in CPI mit SSH-Key-Auth (kein Passwort). SSH-Key-Rotation-Prozess und Ablauf-Monitoring definieren. Erfolgreiche Verarbeitungsbestätigung durch Umbenennung/Archivierung der Quelldatei. Klaren Migrationsplan zu API-basierter Integration definieren und kommunizieren."
          ],
          "dont": [
            "Keine Einführung ohne abgestimmtes Betriebs- und Ownership-Modell."
          ],
          "whenNotToUse": [
            "Wenn Handelspartner oder System eine REST-API bietet. Für interne System-zu-System-Kommunikation ohne externe Partner-Anforderung."
          ]
        },
        "en": {
          "intro": "SFTP (SSH File Transfer Protocol) is an encrypted file transfer protocol for controlled file transfer between systems. In the SAP context: CPI provides an SFTP adapter for sender (polling) and receiver (put). SFTP is the transition technology from insecure file-drop scenarios to regulated Managed File Transfer (MFT). Not positioned as a long-term strategic solution.",
          "whyRing": "ASSESS: SFTP is legitimate as a transitional solution and where trading partners mandatorily use SFTP. Goal is migration to direct API integrations. SFTP with CPI SFTP adapter is significantly better than unsecured file drop.",
          "risks": [
            "SSH key management: expiry or compromise of private keys interrupts integration. No native error feedback mechanism: consumer does not know if file was processed correctly. SFTP server as single point of failure if not HA-configured. File format changes without schema contract hard to detect."
          ],
          "do": [
            "SFTP adapter in CPI with SSH key auth (no password). Define SSH key rotation process and expiry monitoring. Confirm successful processing by renaming/archiving the source file. Define and communicate a clear migration plan to API-based integration."
          ],
          "dont": [
            "Do not adopt without an agreed operating model and ownership setup."
          ],
          "whenNotToUse": [
            "When trading partner or system offers a REST API. For internal system-to-system communication without external partner requirements."
          ]
        }
      }
    },
    {
      "label": "New SOAP/WSDL interfaces",
      "ring": "HOLD",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/soap-sender-adapter"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/soap-sender-adapter",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/api-management",
        "https://api.sap.com"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Neue Integrationsschnittstellen auf Basis von SOAP (Simple Object Access Protocol) und WSDL (Web Services Description Language) als Spezifikationsformat. SOAP verwendet XML-Envelope-Format und HTTP als Transport. In bestehenden SAP-Landschaften noch weit verbreitet (SAP Enterprise Services, PI/PO SOAP-Adapter), aber für Neuvorhaben vollständig durch REST/OData abgelöst.",
          "whyRing": "HOLD ohne Ausnahme für Neuvorhaben. SOAP/WSDL ist ein abgelöster Standard; Neuimplementierungen erhöhen Legacy-Schulden. Ausnahme: Fremdsysteme erzwingen SOAP – dann CPI-SOAP-Adapter als Wrapper akzeptieren.",
          "risks": [
            "SOAP-Tooling-Ökosystem schrumpft: wenige neue Entwickler mit SOAP-Expertise. XML-Overhead signifikant gegenüber JSON. WS-Security (UsernameToken, SAML) komplexer als OAuth",
            "WSDL-Generierung aus modernen Code-Frameworks zunehmend eingestellt."
          ],
          "do": [
            "Wenn SOAP zwingend (Fremdsystem): CPI-SOAP-Sender-Adapter als Eingangspunkt, intern weiter als REST/JSON verarbeiten. Bestehende SOAP-Interfaces inventarisieren und mit REST-Migrationsplan versehen."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generell für alle Neuvorhaben ohne externe SOAP-Pflicht."
          ]
        },
        "en": {
          "intro": "New integration interfaces based on SOAP (Simple Object Access Protocol) and WSDL (Web Services Description Language) as specification format. SOAP uses an XML envelope format and HTTP as transport. Still widely used in existing SAP landscapes (SAP Enterprise Services, PI/PO SOAP adapter) but fully replaced by REST/OData for new initiatives.",
          "whyRing": "HOLD without exception for new initiatives. SOAP/WSDL is a superseded standard; new implementations increase legacy debt. Exception: external systems mandate SOAP – then accept CPI SOAP adapter as wrapper.",
          "risks": [
            "SOAP tooling ecosystem is shrinking: few new developers with SOAP expertise. XML overhead significant compared to JSON. WS-Security (UsernameToken, SAML) more complex than OAuth",
            "WSDL generation from modern code frameworks increasingly discontinued."
          ],
          "do": [
            "When SOAP is mandatory (external system): CPI SOAP sender adapter as entry point, process internally as REST/JSON. Inventory existing SOAP interfaces and provide REST migration plan."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generally for all new initiatives without external SOAP mandate."
          ]
        }
      }
    },
    {
      "label": "SOAP (legacy)",
      "ring": "HOLD",
      "quadrant": "Protocols & APIs",
      "sources": [
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-from-sap-process-integration-and-process-orchestration"
      ],
      "references": [
        "https://help.sap.com/docs/cloud-integration/sap-cloud-integration/soap-sender-adapter",
        "https://help.sap.com/docs/integration-suite/sap-integration-suite/migration-from-sap-process-integration-and-process-orchestration",
        "https://api.sap.com"
      ],
      "confidence": "High",
      "i18n": {
        "de": {
          "intro": "Bestehende SOAP/Web-Service-Integrationen in produktiven SAP-Landschaften – typisch in ECC-, S/4HANA-On-Premise- und PI/PO-basierten Setups mit SAP Enterprise Services (ES Repository). Diese Interfaces sind operativ aktiv und müssen gewartet, aber nicht erweitert werden. Migration zu REST/OData ist das Ziel.",
          "whyRing": "HOLD: Bestehende SOAP-Interfaces im Betrieb halten, aber nicht erweitern. Jede neue Anforderung an bestehende SOAP-Interfaces als Trigger für Migration nutzen. SOAP-Interfaces mit PI/PO-Migration synchronisieren.",
          "risks": [
            "SOAP-Expertise am Markt schwindend – Betrieb und Fehleranalyse wird teurer. WS-Security-Zertifikatspflege: Ablauf unterbricht Integration ohne Vorwarnung. SOAP-Interfaces in PI/PO müssen vor PI/PO-EOM 2027 migriert sein."
          ],
          "do": [
            "SOAP-Interfaces inventarisieren und mit PI/PO-Migrations-Backlog verknüpfen. WS-Security-Zertifikate mit Ablauf-Monitoring ausstatten. Bei jeder neuen Anforderung an SOAP-Interface: zuerst OData-v4-Migration prüfen."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generell für alle Neuvorhaben. Bestand migrieren."
          ]
        },
        "en": {
          "intro": "Existing SOAP/web service integrations in productive SAP landscapes – typical in ECC, S/4HANA on-premise, and PI/PO-based setups with SAP Enterprise Services (ES Repository). These interfaces are operationally active and must be maintained but not extended. Migration to REST/OData is the goal.",
          "whyRing": "HOLD: Keep existing SOAP interfaces running but do not extend them. Use every new requirement on an existing SOAP interface as a trigger for migration. Synchronize SOAP interface migration with PI/PO migration.",
          "risks": [
            "SOAP expertise in the market is diminishing – operations and troubleshooting become more expensive. WS-Security certificate maintenance: expiry interrupts integration without warning. SOAP interfaces in PI/PO must be migrated before PI/PO EOM 2027."
          ],
          "do": [
            "Inventory SOAP interfaces and link to PI/PO migration backlog. Equip WS-Security certificates with expiry monitoring. For every new requirement on a SOAP interface: first evaluate OData v4 migration."
          ],
          "dont": [
            "Keine neuen Legacy-Kopplungen in Greenfield-Umfängen aufbauen."
          ],
          "whenNotToUse": [
            "Generally for all new initiatives. Migrate existing inventory."
          ]
        }
      }
    },
    {
      "label": "SAP BTP Private Link",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Sichere Anbindung von Hyperscaler-Ressourcen (Azure/AWS) ohne öffentliche IP-Adressen.",
          "whyRing": "Sicherheits-Grundlage für moderne Hybrid-Cloud Setups.",
          "risks": [
            "Datenabfluss über das öffentliche Internet; Compliance-Verletzung."
          ],
          "do": [
            "Private Endpoints auf Hyperscaler-Seite nutzen."
          ],
          "dont": [
            "Public IP Whitelisting als Dauerlösung."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Sichere Anbindung von Hyperscaler-Ressourcen (Azure/AWS) ohne öffentliche IP-Adressen.",
          "whyRing": "Sicherheits-Grundlage für moderne Hybrid-Cloud Setups.",
          "risks": [
            "Datenabfluss über das öffentliche Internet; Compliance-Verletzung."
          ],
          "do": [
            "Private Endpoints auf Hyperscaler-Seite nutzen."
          ],
          "dont": [
            "Public IP Whitelisting als Dauerlösung."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "SAP Build Code (CAP)",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Standard Programmiermodell für Cloud-native Extensions. Nachfolger von Java/Node Pro-Code.",
          "whyRing": "Zentraler Baustein für 'Clean Core' Strategie.",
          "risks": [
            "Wildwuchs an In-App Modifikationen."
          ],
          "do": [
            "Side-by-Side Extensions bevorzugen."
          ],
          "dont": [
            "Logik direkt im S/4 Kern belassen."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Standard Programmiermodell für Cloud-native Extensions. Nachfolger von Java/Node Pro-Code.",
          "whyRing": "Zentraler Baustein für 'Clean Core' Strategie.",
          "risks": [
            "Wildwuchs an In-App Modifikationen."
          ],
          "do": [
            "Side-by-Side Extensions bevorzugen."
          ],
          "dont": [
            "Logik direkt im S/4 Kern belassen."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "Principal Propagation",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "End-to-End Identitätsweitergabe vom User bis zum S/4-Backend.",
          "whyRing": "Essentiell für Auditability und Security.",
          "risks": [
            "Verlust des User-Kontextes im Backend (Technischer User)."
          ],
          "do": [
            "OAuth2SAMLBearer Flow nutzen."
          ],
          "dont": [
            "Static Credentials in Destinations."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "End-to-End Identitätsweitergabe vom User bis zum S/4-Backend.",
          "whyRing": "Essentiell für Auditability und Security.",
          "risks": [
            "Verlust des User-Kontextes im Backend (Technischer User)."
          ],
          "do": [
            "OAuth2SAMLBearer Flow nutzen."
          ],
          "dont": [
            "Static Credentials in Destinations."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "SAP Graph",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Unified API Entrypoint für das gesamte SAP Portfolio (One Domain Model).",
          "whyRing": "Reduziert Komplexität für Frontend-Entwickler.",
          "risks": [
            "Redundante Datenabfragen gegen verschiedene Backends."
          ],
          "do": [
            "OData v4 standardisieren."
          ],
          "dont": [
            "Proprietäre API-Mapping-Layer bauen."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Unified API Entrypoint für das gesamte SAP Portfolio (One Domain Model).",
          "whyRing": "Reduziert Komplexität für Frontend-Entwickler.",
          "risks": [
            "Redundante Datenabfragen gegen verschiedene Backends."
          ],
          "do": [
            "OData v4 standardisieren."
          ],
          "dont": [
            "Proprietäre API-Mapping-Layer bauen."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "Cloud Connector HA",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Hochverfügbarkeits-Setup für den SAP Cloud Connector (Master/Shadow).",
          "whyRing": "Vermeidung eines Single-Point-of-Failure.",
          "risks": [
            "Totalausfall der Hybrid-Integration bei Server-Reboot."
          ],
          "do": [
            "Shadow-Instanz in anderem Subnetz/Host betreiben."
          ],
          "dont": [
            "Single-Instance Betrieb für Prod."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Hochverfügbarkeits-Setup für den SAP Cloud Connector (Master/Shadow).",
          "whyRing": "Vermeidung eines Single-Point-of-Failure.",
          "risks": [
            "Totalausfall der Hybrid-Integration bei Server-Reboot."
          ],
          "do": [
            "Shadow-Instanz in anderem Subnetz/Host betreiben."
          ],
          "dont": [
            "Single-Instance Betrieb für Prod."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "OpenTelemetry (OTrEx)",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Standardisierung von Traces, Metrics und Logs zur Cloud-weiten Analyse.",
          "whyRing": "Modernes Monitoring über Systemgrenzen hinweg.",
          "risks": [
            "Blindflug bei verteilten Microservices."
          ],
          "do": [
            "W3C Trace Context nutzen."
          ],
          "dont": [
            "Proprietäre Log-Formate ohne Korrelation."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Standardisierung von Traces, Metrics und Logs zur Cloud-weiten Analyse.",
          "whyRing": "Modernes Monitoring über Systemgrenzen hinweg.",
          "risks": [
            "Blindflug bei verteilten Microservices."
          ],
          "do": [
            "W3C Trace Context nutzen."
          ],
          "dont": [
            "Proprietäre Log-Formate ohne Korrelation."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "BTP Resource Quota Monitoring",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Automatisiertes Monitoring der Subaccount-Limits (Memory, Services).",
          "whyRing": "Operational Excellence & Stabilität.",
          "risks": [
            "Plötzlicher Stop von Services bei Limit-Erreichung."
          ],
          "do": [
            "Alert Notification Service (ANS) anbinden."
          ],
          "dont": [
            "Limits manuell in der Cockpit-UI prüfen."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Automatisiertes Monitoring der Subaccount-Limits (Memory, Services).",
          "whyRing": "Operational Excellence & Stabilität.",
          "risks": [
            "Plötzlicher Stop von Services bei Limit-Erreichung."
          ],
          "do": [
            "Alert Notification Service (ANS) anbinden."
          ],
          "dont": [
            "Limits manuell in der Cockpit-UI prüfen."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "Semantic Versioning (SemVer)",
      "ring": "ADOPT",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Standard für die API-Versionierung (Major.Minor.Patch).",
          "whyRing": "Vermeidung von Breaking Changes.",
          "risks": [
            "Konsumenten-Apps stürzen nach Deployment ab."
          ],
          "do": [
            "Breaking Change = Neue Major Version."
          ],
          "dont": [
            "APIs ohne Versions-Header im URL-Pfad."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Standard für die API-Versionierung (Major.Minor.Patch).",
          "whyRing": "Vermeidung von Breaking Changes.",
          "risks": [
            "Konsumenten-Apps stürzen nach Deployment ab."
          ],
          "do": [
            "Breaking Change = Neue Major Version."
          ],
          "dont": [
            "APIs ohne Versions-Header im URL-Pfad."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    },
    {
      "label": "SAP Datasphere Integration",
      "ring": "ADOPT",
      "quadrant": "Platforms",
      "confidence": "High",
      "references": [
        "https://help.sap.com/docs/SAP_DATASPHERE",
        "https://help.sap.com/docs/integration-suite",
        "https://community.sap.com/t5/technology-blogs-by-sap/bg-p/technology-blogs-by-sapblog-board?labels=Datasphere",
        "https://launchpad.support.sap.com/#/notes/2890171",
        "https://launchpad.support.sap.com/#/notes/325574",
        "https://launchpad.support.sap.com/#/notes/3297105"
      ],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json",
        "user-research-refinement-2026-02-24"
      ],
      "i18n": {
        "de": {
          "intro": "SAP Datasphere ist SAPs strategische Business-Data-Fabric-Plattform für Datenintegration, semantische Modellierung und Analytics-Bereitstellung. Sie trennt Datenintegration (ETL, CDC, Föderation) klar von Prozessintegration (Integration Suite/CPI). Massendaten laufen über Replication/Data Flows und Federation, während transaktionale und eventgetriebene Prozessflüsse in CPI/Event Mesh bleiben.",
          "whyRing": "ADOPT, weil die Rollentrennung Architektur-Risiken reduziert: CPI bleibt für prozessnahe Integrationen, Datasphere übernimmt datenlastige Extraktion, Harmonisierung und analytische Bereitstellung. Das verhindert Überlast der Prozess-Middleware und folgt der SAP-Strategie für Daten- und Analytics-Szenarien.",
          "risks": [
            "Fehlgeleitete ETL-Last auf CPI führt zu Laufzeitproblemen und Timeout-Kaskaden bei großen Datenmengen.",
            "DP-Agent-On-Prem-Setups sind oft unterdimensioniert; produktive Last braucht i. d. R. deutlich mehr Ressourcen als Defaults.",
            "ODP/API-Richtlinien und Tool-Nutzung müssen sauber geklärt sein (insb. bei Dritttool-Zugriffen).",
            "Premium-Features (z. B. Outbound-Integration in externe Data Lakes) können zusätzliche Lizenzkosten erzeugen.",
            "Ohne Foundation-Space-Ansatz entstehen unnötige Doppelreplikationen und Quellsystemlast."
          ],
          "do": [
            "Federation-first anwenden und Replikation nur dort nutzen, wo Performance, Historisierung oder Entkopplung es erfordern.",
            "Massendaten über Replication Flow + CDC aufsetzen; CPI für transaktionale Prozessintegration reservieren.",
            "Einmal replizieren, mehrfach konsumieren: zentrale Foundation-Space-Modelle per Sharing bereitstellen.",
            "Filterung und Reduktion möglichst quellsystemnah (z. B. CDS-seitig) definieren.",
            "Task Chains + Alerting für Replikation/Transformation standardisieren und operativ überwachen.",
            "SAC/Analytics direkt an Datasphere anbinden statt Umwege über Prozessmiddleware."
          ],
          "dont": [
            "Keine Massendaten-Batch-ETL über CPI-IFlows routen.",
            "JDBC-Direktzugriff aus CPI nicht für analytische Reads/Massenabfragen missbrauchen.",
            "Keine ODP-Zugriffe mit nicht freigegebenen Drittansätzen aufbauen.",
            "Sensible Daten nicht ohne Space-Governance und Access-Control übergreifend teilen.",
            "DP-Agent nicht mit Default-Sizing produktiv betreiben."
          ],
          "whenNotToUse": [
            "Nicht als Ersatz für transaktionale Prozessintegration mit synchroner Rückkopplung verwenden (dafür Integration Suite/CPI).",
            "Nicht für sub-sekündiges Event-Streaming einsetzen (dafür Event Mesh/AEM).",
            "Nicht als generische A2A-Middleware für RFC/IDoc/SOAP-Prozessflüsse verwenden."
          ]
        },
        "en": {
          "intro": "SAP Datasphere is SAP’s strategic business data fabric platform for data integration, semantic modeling, and analytics delivery. It separates data integration (ETL, CDC, federation) from process integration (Integration Suite/CPI). High-volume data workloads belong in Datasphere flows, while transactional and event-driven process flows stay in CPI/Event Mesh.",
          "whyRing": "ADOPT because this role split reduces architecture risk: CPI remains focused on process integration while Datasphere handles data-heavy extraction, harmonization, and analytic consumption according to SAP’s strategic direction.",
          "risks": [
            "Routing ETL-heavy workloads through CPI can cause runtime pressure and timeout cascades.",
            "On-prem DP Agent setups are often undersized for production throughput.",
            "ODP/API policy constraints must be validated early, especially for third-party access models.",
            "Premium outbound scenarios may introduce additional licensing costs.",
            "Without a foundation-space model, duplicate replications can overload source systems."
          ],
          "do": [
            "Apply federation-first and replicate only where performance/history/offline needs justify it.",
            "Use Replication Flow + CDC for high-volume scenarios and keep CPI for process-centric integration.",
            "Replicate once, consume many times through shared foundation-space models.",
            "Push filtering and reduction as close to the source as possible.",
            "Operationalize task chains and alerting for replication/transformation reliability.",
            "Connect SAC/analytics directly to Datasphere where appropriate."
          ],
          "dont": [
            "Do not route mass-data ETL through CPI iFlows.",
            "Do not use CPI JDBC access for analytics-heavy reads.",
            "Do not implement non-compliant ODP access patterns.",
            "Do not share sensitive data across spaces without governance controls.",
            "Do not run production DP Agent setups with default sizing."
          ],
          "whenNotToUse": [
            "Do not use Datasphere as a replacement for transactional process integration with synchronous feedback.",
            "Do not use it for sub-second event streaming (use Event Mesh/AEM instead).",
            "Do not position it as a generic A2A middleware for RFC/IDoc/SOAP process flows."
          ]
        }
      }
    },
    {
      "label": "Consumer-Driven Contract Testing",
      "ring": "TRIAL",
      "quadrant": "Tooling & Ops",
      "references": [],
      "sources": [
        "docs/proposed-new-entries-review-round-1.json"
      ],
      "i18n": {
        "de": {
          "intro": "Testverfahren (z.B. mit Pact), bei dem Consumer die Erwartung an APIs definieren.",
          "whyRing": "Sicherung der API-Stabilität in agilen Umfeldern.",
          "risks": [
            "Unentdeckte Breaking Changes erst in Prod."
          ],
          "do": [
            "Pact-Dateien in CI/CD einbinden."
          ],
          "dont": [
            "Sich allein auf manuelle Integrationstests verlassen."
          ],
          "whenNotToUse": [
            "Nicht einführen, wenn Nutzen, Betriebsmodell und Verantwortlichkeiten unklar sind."
          ]
        },
        "en": {
          "intro": "Testverfahren (z.B. mit Pact), bei dem Consumer die Erwartung an APIs definieren.",
          "whyRing": "Sicherung der API-Stabilität in agilen Umfeldern.",
          "risks": [
            "Unentdeckte Breaking Changes erst in Prod."
          ],
          "do": [
            "Pact-Dateien in CI/CD einbinden."
          ],
          "dont": [
            "Sich allein auf manuelle Integrationstests verlassen."
          ],
          "whenNotToUse": [
            "Do not introduce this when value, operating model, and ownership are still unclear."
          ]
        }
      }
    }
  ],
  "importedFrom": {
    "source": "/home/christian/.openclaw/workspace/tmp/results/SAP_Integration_TechRadar_FINAL.xlsx",
    "updatedAt": "2026-02-22",
    "importBatch": "review-round-2"
  }
}
